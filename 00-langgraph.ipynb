{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph í•¸ì¦ˆì˜¨\n",
    "\n",
    "**ì°¸ê³ í•˜ë©´ ì¢‹ì€ ìë£Œ**\n",
    "\n",
    "- [LangChain í•œêµ­ì–´ íŠœí† ë¦¬ì–¼ğŸ‡°ğŸ‡·](https://wikidocs.net/book/14314)\n",
    "- [LangChain í•œêµ­ì–´ íŠœí† ë¦¬ì–¼ Github ì†ŒìŠ¤ì½”ë“œ](https://github.com/teddylee777/langchain-kr)\n",
    "- [í…Œë””ë…¸íŠ¸ YouTube](https://www.youtube.com/c/@teddynote)\n",
    "- [í…Œë””ë…¸íŠ¸ ë¸”ë¡œê·¸](https://teddylee777.github.io/)\n",
    "- [í…Œë””ë…¸íŠ¸ YouTube ë¡œ RAG ë°°ìš°ê¸°!](https://teddylee777.notion.site/YouTube-RAG-10a24f35d12980dc8478c750faa752a2?pvs=74)\n",
    "- [RAG ë¹„ë²•ë…¸íŠ¸](https://fastcampus.co.kr/data_online_teddy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. í™˜ê²½ ì„¤ì •\n",
    "\n",
    "**OpenAI API Key ì„¤ì •**\n",
    "- https://wikidocs.net/233342\n",
    "\n",
    "**ì›¹ ê²€ìƒ‰ì„ ìœ„í•œ API í‚¤ ë°œê¸‰ ì£¼ì†Œ**\n",
    "- https://app.tavily.com/\n",
    "\n",
    "íšŒì› ê°€ì… í›„ API Keyë¥¼ ë°œê¸‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì„¤ì¹˜ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langgraph\n",
      "  Downloading langgraph-0.3.2-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: langchain_openai in /Users/kb/repos/venv/lib/python3.12/site-packages (0.1.23)\n",
      "Collecting langchain_teddynote\n",
      "  Downloading langchain_teddynote-0.3.42-py3-none-any.whl.metadata (708 bytes)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.10.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.4 kB)\n",
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
      "Requirement already satisfied: langchain_community in /Users/kb/repos/venv/lib/python3.12/site-packages (0.2.15)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.1 in /Users/kb/repos/venv/lib/python3.12/site-packages (from langgraph) (0.2.37)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
      "  Downloading langgraph_checkpoint-2.0.16-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph)\n",
      "  Downloading langgraph_prebuilt-0.1.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
      "  Downloading langgraph_sdk-0.1.53-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /Users/kb/repos/venv/lib/python3.12/site-packages (from langchain_openai) (1.40.3)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/kb/repos/venv/lib/python3.12/site-packages (from langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: langchain in /Users/kb/repos/venv/lib/python3.12/site-packages (from langchain_teddynote) (0.2.15)\n",
      "Collecting kiwipiepy (from langchain_teddynote)\n",
      "  Downloading kiwipiepy-0.20.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.1 kB)\n",
      "Collecting rank-bm25 (from langchain_teddynote)\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pinecone-client[grpc] (from langchain_teddynote)\n",
      "  Downloading pinecone_client-6.0.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting pinecone-text (from langchain_teddynote)\n",
      "  Downloading pinecone_text-0.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting olefile (from langchain_teddynote)\n",
      "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting pdf2image (from langchain_teddynote)\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting anthropic (from langchain_teddynote)\n",
      "  Downloading anthropic-0.49.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting deepl (from langchain_teddynote)\n",
      "  Downloading deepl-1.21.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting feedparser (from langchain_teddynote)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting tavily-python (from langchain_teddynote)\n",
      "  Downloading tavily_python-0.5.1-py3-none-any.whl.metadata (91 kB)\n",
      "Collecting pandas (from langchain_teddynote)\n",
      "  Downloading pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Users/kb/repos/venv/lib/python3.12/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /Users/kb/repos/venv/lib/python3.12/site-packages (from faiss-cpu) (24.1)\n",
      "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in /Users/kb/repos/venv/lib/python3.12/site-packages (from pdfplumber) (10.4.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.1-py3-none-macosx_11_0_arm64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /Users/kb/repos/venv/lib/python3.12/site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six==20231228->pdfplumber)\n",
      "  Downloading cryptography-44.0.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/kb/repos/venv/lib/python3.12/site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/kb/repos/venv/lib/python3.12/site-packages (from langchain_community) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/kb/repos/venv/lib/python3.12/site-packages (from langchain_community) (3.10.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/kb/repos/venv/lib/python3.12/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /Users/kb/repos/venv/lib/python3.12/site-packages (from langchain_community) (0.1.98)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/kb/repos/venv/lib/python3.12/site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/kb/repos/venv/lib/python3.12/site-packages (from langchain_community) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/kb/repos/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/kb/repos/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/kb/repos/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/kb/repos/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/kb/repos/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/kb/repos/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/kb/repos/venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/kb/repos/venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /Users/kb/repos/venv/lib/python3.12/site-packages (from langchain->langchain_teddynote) (0.2.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/kb/repos/venv/lib/python3.12/site-packages (from langchain->langchain_teddynote) (2.8.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/kb/repos/venv/lib/python3.12/site-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/kb/repos/venv/lib/python3.12/site-packages (from langchain-core<0.4,>=0.1->langgraph) (4.12.2)\n",
      "Collecting langchain-core<0.4,>=0.1 (from langgraph)\n",
      "  Downloading langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting msgpack<2.0.0,>=1.1.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\n",
      "  Downloading msgpack-1.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.0 (from langchain_community)\n",
      "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /Users/kb/repos/venv/lib/python3.12/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.27.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /Users/kb/repos/venv/lib/python3.12/site-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/kb/repos/venv/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/kb/repos/venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/kb/repos/venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/kb/repos/venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /Users/kb/repos/venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/kb/repos/venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.66.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kb/repos/venv/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kb/repos/venv/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kb/repos/venv/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (2024.7.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/kb/repos/venv/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.7.24)\n",
      "Collecting sgmllib3k (from feedparser->langchain_teddynote)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting kiwipiepy_model<0.21,>=0.20 (from kiwipiepy->langchain_teddynote)\n",
      "  Downloading kiwipiepy_model-0.20.0.tar.gz (34.7 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m34.7/34.7 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /Users/kb/repos/venv/lib/python3.12/site-packages (from pandas->langchain_teddynote) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kb/repos/venv/lib/python3.12/site-packages (from pandas->langchain_teddynote) (2024.1)\n",
      "Collecting tzdata>=2022.7 (from pandas->langchain_teddynote)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting googleapis-common-protos>=1.66.0 (from pinecone-client[grpc]->langchain_teddynote)\n",
      "  Downloading googleapis_common_protos-1.68.0-py2.py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting grpcio>=1.59.0 (from pinecone-client[grpc]->langchain_teddynote)\n",
      "  Downloading grpcio-1.70.0-cp312-cp312-macosx_10_14_universal2.whl.metadata (3.9 kB)\n",
      "Collecting lz4>=3.1.3 (from pinecone-client[grpc]->langchain_teddynote)\n",
      "  Downloading lz4-4.4.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client[grpc]->langchain_teddynote)\n",
      "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting protobuf<6.0,>=5.29 (from pinecone-client[grpc]->langchain_teddynote)\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting protoc-gen-openapiv2<0.0.2,>=0.0.1 (from pinecone-client[grpc]->langchain_teddynote)\n",
      "  Downloading protoc_gen_openapiv2-0.0.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting mmh3<5.0.0,>=4.1.0 (from pinecone-text->langchain_teddynote)\n",
      "  Downloading mmh3-4.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting nltk<4.0.0,>=3.6.5 (from pinecone-text->langchain_teddynote)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /Users/kb/repos/venv/lib/python3.12/site-packages (from pinecone-text->langchain_teddynote) (1.0.1)\n",
      "Collecting types-requests<3.0.0,>=2.25.0 (from pinecone-text->langchain_teddynote)\n",
      "  Downloading types_requests-2.32.0.20250301-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting wget<4.0,>=3.2 (from pinecone-text->langchain_teddynote)\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /Users/kb/repos/venv/lib/python3.12/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/kb/repos/venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/kb/repos/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/kb/repos/venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n",
      "Collecting click (from nltk<4.0.0,>=3.6.5->pinecone-text->langchain_teddynote)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting joblib (from nltk<4.0.0,>=3.6.5->pinecone-text->langchain_teddynote)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/kb/repos/venv/lib/python3.12/site-packages (from pydantic<3,>=1->langchain->langchain_teddynote) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/kb/repos/venv/lib/python3.12/site-packages (from pydantic<3,>=1->langchain->langchain_teddynote) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kb/repos/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->langchain_teddynote) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/kb/repos/venv/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: pycparser in /Users/kb/repos/venv/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
      "Downloading langgraph-0.3.2-py3-none-any.whl (130 kB)\n",
      "Downloading langchain_teddynote-0.3.42-py3-none-any.whl (51 kB)\n",
      "Downloading faiss_cpu-1.10.0-cp312-cp312-macosx_11_0_arm64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
      "Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langgraph_checkpoint-2.0.16-py3-none-any.whl (38 kB)\n",
      "Downloading langchain_core-0.2.43-py3-none-any.whl (397 kB)\n",
      "Downloading langgraph_prebuilt-0.1.1-py3-none-any.whl (24 kB)\n",
      "Downloading langgraph_sdk-0.1.53-py3-none-any.whl (45 kB)\n",
      "Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
      "Downloading pypdfium2-4.30.1-py3-none-macosx_11_0_arm64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anthropic-0.49.0-py3-none-any.whl (243 kB)\n",
      "Downloading deepl-1.21.0-py3-none-any.whl (38 kB)\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Downloading kiwipiepy-0.20.3-cp312-cp312-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
      "Downloading pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl (11.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Downloading pinecone_text-0.9.0-py3-none-any.whl (23 kB)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Downloading tavily_python-0.5.1-py3-none-any.whl (43 kB)\n",
      "Downloading cryptography-44.0.1-cp39-abi3-macosx_10_9_universal2.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading googleapis_common_protos-1.68.0-py2.py3-none-any.whl (164 kB)\n",
      "Downloading grpcio-1.70.0-cp312-cp312-macosx_10_14_universal2.whl (11.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lz4-4.4.3-cp312-cp312-macosx_11_0_arm64.whl (189 kB)\n",
      "Downloading mmh3-4.1.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading msgpack-1.1.0-cp312-cp312-macosx_11_0_arm64.whl (82 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Downloading protobuf-5.29.3-cp38-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "Downloading protoc_gen_openapiv2-0.0.1-py3-none-any.whl (7.9 kB)\n",
      "Downloading types_requests-2.32.0.20250301-py3-none-any.whl (20 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Downloading pinecone_client-6.0.0-py3-none-any.whl (6.7 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Building wheels for collected packages: kiwipiepy_model, wget, sgmllib3k\n",
      "  Building wheel for kiwipiepy_model (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kiwipiepy_model: filename=kiwipiepy_model-0.20.0-py3-none-any.whl size=34818071 sha256=2fc3b731548385572fbdd44285346f42012bd259cf7585bf97d1da480226073d\n",
      "  Stored in directory: /Users/kb/Library/Caches/pip/wheels/f3/14/24/bd3e0c3aa06df12afcb35447b07840213644855333f33bfce5\n",
      "  Building wheel for wget (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9685 sha256=d143c62aac61acc49ca8de858db0b37280fd556946d1d72a35e6b8be70db855b\n",
      "  Stored in directory: /Users/kb/Library/Caches/pip/wheels/01/46/3b/e29ffbe4ebe614ff224bad40fc6a5773a67a163251585a13a9\n",
      "  Building wheel for sgmllib3k (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6090 sha256=7bb3cd9da8bf29a63852679a8be87dd6fd838fb9a8061fdbeb9a00d13b2f831e\n",
      "  Stored in directory: /Users/kb/Library/Caches/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
      "Successfully built kiwipiepy_model wget sgmllib3k\n",
      "Installing collected packages: wget, sgmllib3k, mmh3, kiwipiepy_model, tzdata, types-requests, rank-bm25, pypdfium2, protobuf, pinecone-plugin-interface, pdf2image, olefile, msgpack, lz4, kiwipiepy, joblib, grpcio, feedparser, faiss-cpu, click, pinecone-client, pandas, nltk, googleapis-common-protos, deepl, cryptography, tavily-python, protoc-gen-openapiv2, pinecone-text, pdfminer.six, langsmith, langgraph-sdk, anthropic, pdfplumber, langchain-core, langgraph-checkpoint, langgraph-prebuilt, langgraph, langchain_teddynote\n",
      "  Attempting uninstall: msgpack\n",
      "    Found existing installation: msgpack 1.0.8\n",
      "    Uninstalling msgpack-1.0.8:\n",
      "      Successfully uninstalled msgpack-1.0.8\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.1.98\n",
      "    Uninstalling langsmith-0.1.98:\n",
      "      Successfully uninstalled langsmith-0.1.98\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.2.37\n",
      "    Uninstalling langchain-core-0.2.37:\n",
      "      Successfully uninstalled langchain-core-0.2.37\n",
      "Successfully installed anthropic-0.49.0 click-8.1.8 cryptography-44.0.1 deepl-1.21.0 faiss-cpu-1.10.0 feedparser-6.0.11 googleapis-common-protos-1.68.0 grpcio-1.70.0 joblib-1.4.2 kiwipiepy-0.20.3 kiwipiepy_model-0.20.0 langchain-core-0.2.43 langchain_teddynote-0.3.42 langgraph-0.3.2 langgraph-checkpoint-2.0.16 langgraph-prebuilt-0.1.1 langgraph-sdk-0.1.53 langsmith-0.1.147 lz4-4.4.3 mmh3-4.1.0 msgpack-1.1.0 nltk-3.9.1 olefile-0.47 pandas-2.2.3 pdf2image-1.17.0 pdfminer.six-20231228 pdfplumber-0.11.5 pinecone-client-6.0.0 pinecone-plugin-interface-0.0.7 pinecone-text-0.9.0 protobuf-5.29.3 protoc-gen-openapiv2-0.0.1 pypdfium2-4.30.1 rank-bm25-0.2.2 sgmllib3k-1.0.0 tavily-python-0.5.1 types-requests-2.32.0.20250301 tzdata-2025.1 wget-3.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langgraph langchain_openai langchain_teddynote faiss-cpu pdfplumber langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì‹¤ìŠµìë£Œë¥¼ ë‹¤ìš´ë¡œë“œ ë°›ìŠµë‹ˆë‹¤**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: /Users/kb/repos/langcon-2025-handson\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(f\"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¡œë“œí•  .env íŒŒì¼ ê²½ë¡œ: /Users/kb/repos/langcon-2025-handson/rag/.env\n",
      ".env íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤!\n",
      "OpenAI API Key: sk-pr...YuMA\n",
      "OpenAI API Key ê¸¸ì´: 164ì\n",
      "Tavily API Key: tvly-de...IAFM\n",
      "Tavily API Key ê¸¸ì´: 41ì\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# .env íŒŒì¼ì˜ ì •í™•í•œ ê²½ë¡œ ì§€ì •\n",
    "env_path = os.path.join(os.getcwd(), 'rag', '.env')\n",
    "print(f\"ë¡œë“œí•  .env íŒŒì¼ ê²½ë¡œ: {env_path}\")\n",
    "\n",
    "# .env íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
    "if os.path.exists(env_path):\n",
    "    print(\".env íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤!\")\n",
    "    \n",
    "    # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "    load_dotenv(dotenv_path=env_path, override=True)\n",
    "    \n",
    "    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸\n",
    "    openai_key = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "    tavily_key = os.environ.get(\"TAVILY_API_KEY\", \"\")\n",
    "    \n",
    "    if openai_key:\n",
    "        print(f\"OpenAI API Key: {openai_key[:5]}...{openai_key[-4:] if len(openai_key) > 4 else ''}\")\n",
    "        print(f\"OpenAI API Key ê¸¸ì´: {len(openai_key)}ì\")\n",
    "    else:\n",
    "        print(\"OpenAI API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    if tavily_key:\n",
    "        print(f\"Tavily API Key: {tavily_key[:7]}...{tavily_key[-4:] if len(tavily_key) > 4 else ''}\")\n",
    "        print(f\"Tavily API Key ê¸¸ì´: {len(tavily_key)}ì\")\n",
    "    else:\n",
    "        print(\"Tavily API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(f\".env íŒŒì¼ì´ ì§€ì •í•œ ê²½ë¡œì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {env_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\"  # ë°œê¸‰ ë°›ì€ OpenAI API Key ì…ë ¥\n",
    "# os.environ[\"TAVILY_API_KEY\"] = \"\"  # ë°œê¸‰ ë°›ì€ Tavily API Key ì…ë ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ì„ íƒ ì‚¬í•­)\n",
    "\n",
    "LangSmith ì¶”ì ì„ ì›í•˜ëŠ” ê²½ìš° ì•„ë˜ LangSmith API Key ë¥¼ ë°œê¸‰ ë°›ì•„ ì…ë ¥í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "- ë§í¬: https://smith.langchain.com\n",
    "- íšŒì› ê°€ì… í›„ - ì„¤ì • - ìƒë‹¨ API Keys ì—ì„œ ë°œê¸‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"LANGSMITH_API_KEY\"] = \"\"  # ë°œê¸‰ ë°›ì€ LangSmith API Key ì…ë ¥\n",
    "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"  # ì¶”ì  ì„¤ì •\n",
    "# os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"  # ì¶”ì  ì—”ë“œí¬ì¸íŠ¸\n",
    "# os.environ[\"LANGSMITH_PROJECT\"] = \"LangGraph-Hands-On\"  # í”„ë¡œì íŠ¸ ì´ë¦„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. ê¸°ë³¸ ReAct Agent êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ëª¨ë¸ ì„¤ì •\n",
    "model = ChatOpenAI(model_name=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë„êµ¬ (Tools) ì„¤ì •\n",
    "\n",
    "ë„êµ¬(Tool)ëŠ” ì—ì´ì „íŠ¸, ì²´ì¸ ë˜ëŠ” LLMì´ ì™¸ë¶€ ì„¸ê³„ì™€ ìƒí˜¸ì‘ìš©í•˜ê¸° ìœ„í•œ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.\n",
    "\n",
    "LangChain ì—ì„œ ê¸°ë³¸ ì œê³µí•˜ëŠ” ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‰½ê²Œ ë„êµ¬ë¥¼ í™œìš©í•  ìˆ˜ ìˆìœ¼ë©°, ì‚¬ìš©ì ì •ì˜ ë„êµ¬(Custom Tool) ë¥¼ ì‰½ê²Œ êµ¬ì¶•í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "**LangChain ì— í†µí•©ëœ ë„êµ¬ ë¦¬ìŠ¤íŠ¸ëŠ” ì•„ë˜ ë§í¬ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.**\n",
    "\n",
    "ë­ì²´ì¸ì—ì„œ ì œê³µí•˜ëŠ” ì‚¬ì „ì— ì •ì˜ëœ ë„êµ¬(tool) ì™€ íˆ´í‚·(toolkit) ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "tool ì€ ë‹¨ì¼ ë„êµ¬ë¥¼ ì˜ë¯¸í•˜ë©°, toolkit ì€ ì—¬ëŸ¬ ë„êµ¬ë¥¼ ë¬¶ì–´ì„œ í•˜ë‚˜ì˜ ë„êµ¬ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ê´€ë ¨ ë„êµ¬ëŠ” ì•„ë˜ì˜ ë§í¬ì—ì„œ ì°¸ê³ í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "**ì°¸ê³ **\n",
    "- [LangChain Tools/Toolkits](https://python.langchain.com/docs/integrations/tools/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ê²€ìƒ‰ API ë„êµ¬**\n",
    "\n",
    "Tavily ê²€ìƒ‰ APIë¥¼ í™œìš©í•˜ì—¬ ê²€ìƒ‰ ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤. \n",
    "\n",
    "**API í‚¤ ë°œê¸‰ ì£¼ì†Œ**\n",
    "- https://app.tavily.com/\n",
    "\n",
    "ë°œê¸‰í•œ API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ì— ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "`.env` íŒŒì¼ì— ì•„ë˜ì™€ ê°™ì´ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "```\n",
    "TAVILY_API_KEY=tvly-abcdefghijklmnopqrstuvwxyz\n",
    "```\n",
    "\n",
    "**TavilySearch**\n",
    "\n",
    "**ì„¤ëª…**\n",
    "- Tavily ê²€ìƒ‰ APIë¥¼ ì¿¼ë¦¬í•˜ê³  JSON í˜•ì‹ì˜ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "- í¬ê´„ì ì´ê³  ì •í™•í•˜ë©° ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²°ê³¼ì— ìµœì í™”ëœ ê²€ìƒ‰ ì—”ì§„ì…ë‹ˆë‹¤.\n",
    "- í˜„ì¬ ì´ë²¤íŠ¸ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” ë§¤ê°œë³€ìˆ˜**\n",
    "- `max_results` (int): ë°˜í™˜í•  ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 5)\n",
    "- `search_depth` (str): ê²€ìƒ‰ ê¹Šì´ (\"basic\" ë˜ëŠ” \"advanced\")\n",
    "- `include_domains` (List[str]): ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨í•  ë„ë©”ì¸ ëª©ë¡\n",
    "- `exclude_domains` (List[str]): ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì œì™¸í•  ë„ë©”ì¸ ëª©ë¡\n",
    "- `include_answer` (bool): ì›ë³¸ ì¿¼ë¦¬ì— ëŒ€í•œ ì§§ì€ ë‹µë³€ í¬í•¨ ì—¬ë¶€\n",
    "- `include_raw_content` (bool): ê° ì‚¬ì´íŠ¸ì˜ ì •ì œëœ HTML ì½˜í…ì¸  í¬í•¨ ì—¬ë¶€\n",
    "- `include_images` (bool): ì¿¼ë¦¬ ê´€ë ¨ ì´ë¯¸ì§€ ëª©ë¡ í¬í•¨ ì—¬ë¶€\n",
    "\n",
    "**ë°˜í™˜ ê°’**\n",
    "- ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬í•¨í•˜ëŠ” JSON í˜•ì‹ì˜ ë¬¸ìì—´(url, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "\n",
    "# ì›¹ ê²€ìƒ‰ ë„êµ¬ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "web_search_tool = TavilySearch(\n",
    "    max_results=6,  # ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼\n",
    ")\n",
    "\n",
    "# ì›¹ ê²€ìƒ‰ ë„êµ¬ì˜ ì´ë¦„ê³¼ ì„¤ëª…ì„ ì„¤ì •í•©ë‹ˆë‹¤.  ë§¤ìš° ì¤‘ìš”í•¨. ì–´ë–¤ ë„êµ¬ë¥¼ ì“¸ì§€ AIê°€ ì•Œë ¤ì¤˜ì•¼í•¨. \n",
    "web_search_tool.name = \"web_search\"\n",
    "web_search_tool.description = \"Use this tool to search on the web\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PDFRetrievalChain`: PDF ë¬¸ì„œ ê¸°ë°˜ Naive RAG ì²´ì¸\n",
    "\n",
    "ë¬¸ì„œ ê¸°ë°˜ RAG ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ ì²´ì¸ì€ ì£¼ì–´ì§„ PDF ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” ë§¤ê°œë³€ìˆ˜**\n",
    "- `source_uri` (List[str]): PDF ë¬¸ì„œì˜ ê²½ë¡œ\n",
    "- `model_name` (str): ì‚¬ìš©í•  ëª¨ë¸ì˜ ì´ë¦„\n",
    "- `k` (int): ë°˜í™˜í•  ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.pdf import PDFRetrievalChain\n",
    "\n",
    "# PDF ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "pdf = PDFRetrievalChain(\n",
    "    [\"data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf\"], model_name=\"gpt-4o-mini\", k=6\n",
    ").create_chain()\n",
    "\n",
    "# retrieverì™€ chainì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "pdf_retriever = pdf.retriever\n",
    "pdf_chain = pdf.chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 1, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}, page_content='â–¹ ì‚¼ì„±ì „ì, ìì²´ ê°œë°œ ìƒì„± AI â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ ê³µê°œ Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·10\\nâ–¹ êµ¬ê¸€, ì•¤ìŠ¤ë¡œí”½ì— 20ì–µ ë‹¬ëŸ¬ íˆ¬ìë¡œ ìƒì„± AI í˜‘ë ¥ ê°•í™” Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·11\\nâ–¹ IDC, 2027ë…„ AI ì†Œí”„íŠ¸ì›¨ì–´ ë§¤ì¶œ 2,500ì–µ ë‹¬ëŸ¬ ëŒíŒŒ ì „ë§Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·12'),\n",
       " Document(metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}, page_content='SPRi AI Brief |\\n2023-12ì›”í˜¸\\nì‚¼ì„±ì „ì, ìì²´ ê°œë°œ ìƒì„± AI â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ ê³µê°œ\\nKEY Contents\\nn ì‚¼ì„±ì „ìê°€ ì˜¨ë””ë°”ì´ìŠ¤ì—ì„œ ì‘ë™ ê°€ëŠ¥í•˜ë©° ì–¸ì–´, ì½”ë“œ, ì´ë¯¸ì§€ì˜ 3ê°œ ëª¨ë¸ë¡œ êµ¬ì„±ëœ ìì²´ ê°œë°œ ìƒì„±\\nAI ëª¨ë¸ â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ë¥¼ ê³µê°œ\\nn ì‚¼ì„±ì „ìëŠ” ì‚¼ì„± ê°€ìš°ìŠ¤ë¥¼ ë‹¤ì–‘í•œ ì œí’ˆì— ë‹¨ê³„ì ìœ¼ë¡œ íƒ‘ì¬í•  ê³„íšìœ¼ë¡œ, ì˜¨ë””ë°”ì´ìŠ¤ ì‘ë™ì´ ê°€ëŠ¥í•œ\\nì‚¼ì„± ê°€ìš°ìŠ¤ëŠ” ì™¸ë¶€ë¡œ ì‚¬ìš©ì ì •ë³´ê°€ ìœ ì¶œë  ìœ„í—˜ì´ ì—†ë‹¤ëŠ” ì¥ì ì„ ë³´ìœ \\nÂ£ì–¸ì–´, ì½”ë“œ, ì´ë¯¸ì§€ì˜ 3ê°œ ëª¨ë¸ë¡œ êµ¬ì„±ëœ ì‚¼ì„± ê°€ìš°ìŠ¤, ì˜¨ë””ë°”ì´ìŠ¤ ì‘ë™ ì§€ì›'),\n",
       " Document(metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}, page_content='ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ ì ìš©í•œ êµ¬ê¸€ í”½ì…€(Pixel)ê³¼ ê²½ìŸí•  ê²ƒìœ¼ë¡œ ì˜ˆìƒ\\nâ˜ ì¶œì²˜ : ì‚¼ì„±ì „ì, â€˜ì‚¼ì„± AI í¬ëŸ¼â€™ì„œ ìì²´ ê°œë°œ ìƒì„±í˜• AI â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ ê³µê°œ, 2023.11.08.\\nì‚¼ì„±ì „ì, â€˜ì‚¼ì„± ê°œë°œì ì½˜í¼ëŸ°ìŠ¤ ì½”ë¦¬ì•„ 2023â€™ ê°œìµœ, 2023.11.14.\\nTechRepublic, Samsung Gauss: Samsung Research Reveals Generative AI, 2023.11.08.\\n10'),\n",
       " Document(metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}, page_content='Â£ì–¸ì–´, ì½”ë“œ, ì´ë¯¸ì§€ì˜ 3ê°œ ëª¨ë¸ë¡œ êµ¬ì„±ëœ ì‚¼ì„± ê°€ìš°ìŠ¤, ì˜¨ë””ë°”ì´ìŠ¤ ì‘ë™ ì§€ì›\\nn ì‚¼ì„±ì „ìê°€ 2023ë…„ 11ì›” 8ì¼ ì—´ë¦° â€˜ì‚¼ì„± AI í¬ëŸ¼ 2023â€™ í–‰ì‚¬ì—ì„œ ìì²´ ê°œë°œí•œ ìƒì„± AI ëª¨ë¸\\nâ€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ë¥¼ ìµœì´ˆ ê³µê°œ\\nâˆ™ ì •ê·œë¶„í¬ ì´ë¡ ì„ ì •ë¦½í•œ ì²œì¬ ìˆ˜í•™ì ê°€ìš°ìŠ¤(Gauss)ì˜ ì´ë¦„ì„ ë³¸ëœ¬ ì‚¼ì„± ê°€ìš°ìŠ¤ëŠ” ë‹¤ì–‘í•œ ìƒí™©ì—\\nìµœì í™”ëœ í¬ê¸°ì˜ ëª¨ë¸ ì„ íƒì´ ê°€ëŠ¥\\nâˆ™ ì‚¼ì„± ê°€ìš°ìŠ¤ëŠ” ë¼ì´ì„ ìŠ¤ë‚˜ ê°œì¸ì •ë³´ë¥¼ ì¹¨í•´í•˜ì§€ ì•ŠëŠ” ì•ˆì „í•œ ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµë˜ì—ˆìœ¼ë©°,\\nì˜¨ë””ë°”ì´ìŠ¤ì—ì„œ ì‘ë™í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ ì™¸ë¶€ë¡œ ì‚¬ìš©ìì˜ ì •ë³´ê°€ ìœ ì¶œë˜ì§€ ì•ŠëŠ” ì¥ì ì„ ë³´ìœ '),\n",
       " Document(metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 18, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}, page_content='<êµ¬ê¸€ ë”¥ë§ˆì¸ë“œì˜ ë²”ìš© AI ë¶„ë¥˜ í”„ë ˆì„ì›Œí¬>\\nì„±ëŠ¥ íŠ¹ìˆ˜ AI ì˜ˆì‹œ ë²”ìš© AI ì˜ˆì‹œ\\n0ë‹¨ê³„: AI ì•„ë‹˜ ê³„ì‚°ê¸° ì†Œí”„íŠ¸ì›¨ì–´, ì»´íŒŒì¼ëŸ¬ ì•„ë§ˆì¡´ ë©”ì»¤ë‹ˆì»¬ í„°í¬\\n1ë‹¨ê³„: ì‹ ì§„(ìˆ™ë ¨ë˜ì§€ ì•Šì€ ì¸ê°„) GOFAI(Good Old Fashioned Artificial Intelligence) ì±—GPT, ë°”ë“œ, ë¼ë§ˆ2\\nìŠ¤ë§ˆíŠ¸ ìŠ¤í”¼ì»¤(ì• í”Œ ì‹œë¦¬, ì•„ë§ˆì¡´ ì•Œë ‰ì‚¬, êµ¬ê¸€\\n2ë‹¨ê³„: ìœ ëŠ¥(ìˆ™ë ¨ëœ ì¸ê°„ì˜ 50% ì´ìƒ) ë¯¸ë‹¬ì„±\\nì–´ì‹œìŠ¤í„´íŠ¸), IBM ì™“ìŠ¨\\n3ë‹¨ê³„: ì „ë¬¸ê°€(ìˆ™ë ¨ëœ ì¸ê°„ì˜ 90% ì´ìƒ) ë¬¸ë²• êµì •ê¸°(ê·¸ë˜ë¨¸ë¦¬), ìƒì„± ì´ë¯¸ì§€ ëª¨ë¸(ë‹¬ë¦¬2) ë¯¸ë‹¬ì„±'),\n",
       " Document(metadata={'source': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'file_path': 'data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf', 'page': 12, 'total_pages': 23, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13462', 'Producer': 'Hancom PDF 1.3.0.542', 'CreationDate': \"D:20231208132838+09'00'\", 'ModDate': \"D:20231208132838+09'00'\", 'PDFVersion': '1.4'}, page_content='ì²˜ë¦¬ë¥¼ ì§€ì›\\nâˆ™ ì½”ë“œ ëª¨ë¸ ê¸°ë°˜ì˜ AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ â€˜ì½”ë“œì•„ì´(code.i)â€™ëŠ” ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ë¡œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ë©°\\nì‚¬ë‚´ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì— ìµœì í™”\\nâˆ™ ì´ë¯¸ì§€ ëª¨ë¸ì€ ì°½ì˜ì ì¸ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê³  ê¸°ì¡´ ì´ë¯¸ì§€ë¥¼ ì›í•˜ëŠ” ëŒ€ë¡œ ë°”ê¿€ ìˆ˜ ìˆë„ë¡ ì§€ì›í•˜ë©°\\nì €í•´ìƒë„ ì´ë¯¸ì§€ì˜ ê³ í•´ìƒë„ ì „í™˜ë„ ì§€ì›\\nn IT ì „ë¬¸ì§€ í…Œí¬ë¦¬í¼ë¸”ë¦­(TechRepublic)ì€ ì˜¨ë””ë°”ì´ìŠ¤ AIê°€ ì£¼ìš” ê¸°ìˆ  íŠ¸ë Œë“œë¡œ ë¶€ìƒí–ˆë‹¤ë©°,\\n2024ë…„ë¶€í„° ê°€ìš°ìŠ¤ë¥¼ íƒ‘ì¬í•œ ì‚¼ì„± ìŠ¤ë§ˆíŠ¸í°ì´ ë©”íƒ€ì˜ ë¼ë§ˆ(Llama)2ë¥¼ íƒ‘ì¬í•œ í€„ì»´ ê¸°ê¸° ë° êµ¬ê¸€')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: Connection error caused failure to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. Please confirm your internet connection. SSLError(MaxRetryError(\"HTTPSConnectionPool(host='api.smith.langchain.com', port=443): Max retries exceeded with url: /runs/multipart (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:2406)')))\"))\n",
      "Content-Length: 595009\n",
      "API Key: lsv2_**********..trace=c5a40433-51b2-455a-a7b2-c6039ece7580,id=5e10562a-cce0-42a2-994d-91dd3f2647cd; trace=c5a40433-51b2-455a-a7b2-c6039ece7580,id=66d03d07-c344-48c8-8c5e-8470ce15d84b; trace=c5a40433-51b2-455a-a7b2-c6039ece7580,id=a2cd8c94-b968-4d69-b09f-11d302acb302; trace=c5a40433-51b2-455a-a7b2-c6039ece7580,id=6f80fbec-2995-4fa0-b47a-feaaf0c07b05; trace=c5a40433-51b2-455a-a7b2-c6039ece7580,id=4d40003a-e9bb-416b-8e04-168c9a58d978; trace=c5a40433-51b2-455a-a7b2-c6039ece7580,id=ffb0f03a-135d-448e-bbcd-95accbdcf7f8; trace=c5a40433-51b2-455a-a7b2-c6039ece7580,id=c5a40433-51b2-455a-a7b2-c6039ece7580\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    }
   ],
   "source": [
    "# ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì…ë ¥í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "searched_docs = pdf_retriever.invoke(\"ì‚¼ì„±ì „ìê°€ ë§Œë“  ìƒì„±í˜• AI ì´ë¦„ì„ ì°¾ì•„ì¤˜\")\n",
    "searched_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì¶”ì **: https://smith.langchain.com/public/bdaa2410-0a6a-44c9-8e2b-c5d8628bf84e/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚¼ì„±ì „ìê°€ ë§Œë“  ìƒì„±í˜• AIì˜ ì´ë¦„ì€ 'ì‚¼ì„± ê°€ìš°ìŠ¤'ì…ë‹ˆë‹¤.\n",
      "\n",
      "**Source**\n",
      "- data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf (page 1)\n"
     ]
    }
   ],
   "source": [
    "answer = pdf_chain.invoke(\n",
    "    {\"question\": \"ì‚¼ì„±ì „ìê°€ ë§Œë“  ìƒì„±í˜• AI ì´ë¦„ì„ ì°¾ì•„ì¤˜\", \"context\": searched_docs}\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "\n",
    "# PDF ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ ë„êµ¬ ìƒì„±\n",
    "# ì—¬ê¸°ì„œë„ llm ì´ ë„êµ¬ë¥¼ ì°¾ê¸° ë•Œë¬¸ì— ì´ë¦„ì´ë‘ ë„êµ¬ ëª©ì ì„ ë°˜ë“œì‹œ ìƒì„¸íˆ ì ì–´ì¤˜ì•¼í•¨. \n",
    "retriever_tool = create_retriever_tool( \n",
    "    pdf_retriever,\n",
    "    \"pdf_retriever\",\n",
    "    \"Search and return information about SPRI AI Brief PDF file. It contains useful information on recent AI trends. The document is published on Dec 2023.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¹ ì‚¼ì„±ì „ì, ìì²´ ê°œë°œ ìƒì„± AI â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ ê³µê°œ Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·10\n",
      "â–¹ êµ¬ê¸€, ì•¤ìŠ¤ë¡œí”½ì— 20ì–µ ë‹¬ëŸ¬ íˆ¬ìë¡œ ìƒì„± AI í˜‘ë ¥ ê°•í™” Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·11\n",
      "â–¹ IDC, 2027ë…„ AI ì†Œí”„íŠ¸ì›¨ì–´ ë§¤ì¶œ 2,500ì–µ ë‹¬ëŸ¬ ëŒíŒŒ ì „ë§Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·12\n",
      "\n",
      "SPRi AI Brief |\n",
      "2023-12ì›”í˜¸\n",
      "ì‚¼ì„±ì „ì, ìì²´ ê°œë°œ ìƒì„± AI â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ ê³µê°œ\n",
      "KEY Contents\n",
      "n ì‚¼ì„±ì „ìê°€ ì˜¨ë””ë°”ì´ìŠ¤ì—ì„œ ì‘ë™ ê°€ëŠ¥í•˜ë©° ì–¸ì–´, ì½”ë“œ, ì´ë¯¸ì§€ì˜ 3ê°œ ëª¨ë¸ë¡œ êµ¬ì„±ëœ ìì²´ ê°œë°œ ìƒì„±\n",
      "AI ëª¨ë¸ â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ë¥¼ ê³µê°œ\n",
      "n ì‚¼ì„±ì „ìëŠ” ì‚¼ì„± ê°€ìš°ìŠ¤ë¥¼ ë‹¤ì–‘í•œ ì œí’ˆì— ë‹¨ê³„ì ìœ¼ë¡œ íƒ‘ì¬í•  ê³„íšìœ¼ë¡œ, ì˜¨ë””ë°”ì´ìŠ¤ ì‘ë™ì´ ê°€ëŠ¥í•œ\n",
      "ì‚¼ì„± ê°€ìš°ìŠ¤ëŠ” ì™¸ë¶€ë¡œ ì‚¬ìš©ì ì •ë³´ê°€ ìœ ì¶œë  ìœ„í—˜ì´ ì—†ë‹¤ëŠ” ì¥ì ì„ ë³´ìœ \n",
      "Â£ì–¸ì–´, ì½”ë“œ, ì´ë¯¸ì§€ì˜ 3ê°œ ëª¨ë¸ë¡œ êµ¬ì„±ëœ ì‚¼ì„± ê°€ìš°ìŠ¤, ì˜¨ë””ë°”ì´ìŠ¤ ì‘ë™ ì§€ì›\n",
      "\n",
      "ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ ì ìš©í•œ êµ¬ê¸€ í”½ì…€(Pixel)ê³¼ ê²½ìŸí•  ê²ƒìœ¼ë¡œ ì˜ˆìƒ\n",
      "â˜ ì¶œì²˜ : ì‚¼ì„±ì „ì, â€˜ì‚¼ì„± AI í¬ëŸ¼â€™ì„œ ìì²´ ê°œë°œ ìƒì„±í˜• AI â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ ê³µê°œ, 2023.11.08.\n",
      "ì‚¼ì„±ì „ì, â€˜ì‚¼ì„± ê°œë°œì ì½˜í¼ëŸ°ìŠ¤ ì½”ë¦¬ì•„ 2023â€™ ê°œìµœ, 2023.11.14.\n",
      "TechRepublic, Samsung Gauss: Samsung Research Reveals Generative AI, 2023.11.08.\n",
      "10\n",
      "\n",
      "Â£ì–¸ì–´, ì½”ë“œ, ì´ë¯¸ì§€ì˜ 3ê°œ ëª¨ë¸ë¡œ êµ¬ì„±ëœ ì‚¼ì„± ê°€ìš°ìŠ¤, ì˜¨ë””ë°”ì´ìŠ¤ ì‘ë™ ì§€ì›\n",
      "n ì‚¼ì„±ì „ìê°€ 2023ë…„ 11ì›” 8ì¼ ì—´ë¦° â€˜ì‚¼ì„± AI í¬ëŸ¼ 2023â€™ í–‰ì‚¬ì—ì„œ ìì²´ ê°œë°œí•œ ìƒì„± AI ëª¨ë¸\n",
      "â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ë¥¼ ìµœì´ˆ ê³µê°œ\n",
      "âˆ™ ì •ê·œë¶„í¬ ì´ë¡ ì„ ì •ë¦½í•œ ì²œì¬ ìˆ˜í•™ì ê°€ìš°ìŠ¤(Gauss)ì˜ ì´ë¦„ì„ ë³¸ëœ¬ ì‚¼ì„± ê°€ìš°ìŠ¤ëŠ” ë‹¤ì–‘í•œ ìƒí™©ì—\n",
      "ìµœì í™”ëœ í¬ê¸°ì˜ ëª¨ë¸ ì„ íƒì´ ê°€ëŠ¥\n",
      "âˆ™ ì‚¼ì„± ê°€ìš°ìŠ¤ëŠ” ë¼ì´ì„ ìŠ¤ë‚˜ ê°œì¸ì •ë³´ë¥¼ ì¹¨í•´í•˜ì§€ ì•ŠëŠ” ì•ˆì „í•œ ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµë˜ì—ˆìœ¼ë©°,\n",
      "ì˜¨ë””ë°”ì´ìŠ¤ì—ì„œ ì‘ë™í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ ì™¸ë¶€ë¡œ ì‚¬ìš©ìì˜ ì •ë³´ê°€ ìœ ì¶œë˜ì§€ ì•ŠëŠ” ì¥ì ì„ ë³´ìœ \n",
      "\n",
      "<êµ¬ê¸€ ë”¥ë§ˆì¸ë“œì˜ ë²”ìš© AI ë¶„ë¥˜ í”„ë ˆì„ì›Œí¬>\n",
      "ì„±ëŠ¥ íŠ¹ìˆ˜ AI ì˜ˆì‹œ ë²”ìš© AI ì˜ˆì‹œ\n",
      "0ë‹¨ê³„: AI ì•„ë‹˜ ê³„ì‚°ê¸° ì†Œí”„íŠ¸ì›¨ì–´, ì»´íŒŒì¼ëŸ¬ ì•„ë§ˆì¡´ ë©”ì»¤ë‹ˆì»¬ í„°í¬\n",
      "1ë‹¨ê³„: ì‹ ì§„(ìˆ™ë ¨ë˜ì§€ ì•Šì€ ì¸ê°„) GOFAI(Good Old Fashioned Artificial Intelligence) ì±—GPT, ë°”ë“œ, ë¼ë§ˆ2\n",
      "ìŠ¤ë§ˆíŠ¸ ìŠ¤í”¼ì»¤(ì• í”Œ ì‹œë¦¬, ì•„ë§ˆì¡´ ì•Œë ‰ì‚¬, êµ¬ê¸€\n",
      "2ë‹¨ê³„: ìœ ëŠ¥(ìˆ™ë ¨ëœ ì¸ê°„ì˜ 50% ì´ìƒ) ë¯¸ë‹¬ì„±\n",
      "ì–´ì‹œìŠ¤í„´íŠ¸), IBM ì™“ìŠ¨\n",
      "3ë‹¨ê³„: ì „ë¬¸ê°€(ìˆ™ë ¨ëœ ì¸ê°„ì˜ 90% ì´ìƒ) ë¬¸ë²• êµì •ê¸°(ê·¸ë˜ë¨¸ë¦¬), ìƒì„± ì´ë¯¸ì§€ ëª¨ë¸(ë‹¬ë¦¬2) ë¯¸ë‹¬ì„±\n",
      "\n",
      "ì²˜ë¦¬ë¥¼ ì§€ì›\n",
      "âˆ™ ì½”ë“œ ëª¨ë¸ ê¸°ë°˜ì˜ AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ â€˜ì½”ë“œì•„ì´(code.i)â€™ëŠ” ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ë¡œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ë©°\n",
      "ì‚¬ë‚´ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì— ìµœì í™”\n",
      "âˆ™ ì´ë¯¸ì§€ ëª¨ë¸ì€ ì°½ì˜ì ì¸ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê³  ê¸°ì¡´ ì´ë¯¸ì§€ë¥¼ ì›í•˜ëŠ” ëŒ€ë¡œ ë°”ê¿€ ìˆ˜ ìˆë„ë¡ ì§€ì›í•˜ë©°\n",
      "ì €í•´ìƒë„ ì´ë¯¸ì§€ì˜ ê³ í•´ìƒë„ ì „í™˜ë„ ì§€ì›\n",
      "n IT ì „ë¬¸ì§€ í…Œí¬ë¦¬í¼ë¸”ë¦­(TechRepublic)ì€ ì˜¨ë””ë°”ì´ìŠ¤ AIê°€ ì£¼ìš” ê¸°ìˆ  íŠ¸ë Œë“œë¡œ ë¶€ìƒí–ˆë‹¤ë©°,\n",
      "2024ë…„ë¶€í„° ê°€ìš°ìŠ¤ë¥¼ íƒ‘ì¬í•œ ì‚¼ì„± ìŠ¤ë§ˆíŠ¸í°ì´ ë©”íƒ€ì˜ ë¼ë§ˆ(Llama)2ë¥¼ íƒ‘ì¬í•œ í€„ì»´ ê¸°ê¸° ë° êµ¬ê¸€\n"
     ]
    }
   ],
   "source": [
    "result = retriever_tool.invoke(\"ì‚¼ì„±ì „ìê°€ ë§Œë“  ìƒì„±í˜• AI ì´ë¦„ì„ ì°¾ì•„ì¤˜\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë„êµ¬ ëª©ë¡ì„ ì •ì˜ í•©ë‹ˆë‹¤. ì´ëŠ” Agent ì—ê²Œ ì œê³µë  ë„êµ¬ ëª©ë¡ì…ë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TavilySearch(name='web_search', description='Use this tool to search on the web', client=<tavily.tavily.TavilyClient object at 0x10578b9e0>, max_results=6),\n",
       " Tool(name='pdf_retriever', description='Search and return information about SPRI AI Brief PDF file. It contains useful information on recent AI trends. The document is published on Dec 2023.', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x106acce00>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x145541cd0>, search_kwargs={'k': 6}), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x106accf40>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x145541cd0>, search_kwargs={'k': 6}), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë„êµ¬ ëª©ë¡ ì •ì˜\n",
    "tools = [web_search_tool, retriever_tool]\n",
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_react_agent`\n",
    "\n",
    "ReAct Agent ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ëŠ” ë„êµ¬ ëª©ë¡ì„ ì œê³µí•˜ê³ , ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `model`: ì‚¬ìš©í•  ëª¨ë¸\n",
    "- `tools`: ë„êµ¬ ëª©ë¡\n",
    "- `prompt`: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "simple_react_agent = create_react_agent(\n",
    "    model, tools, prompt=\"You are a helpful assistant. Answer in Korean.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ê·¸ë˜í”„ ì‹œê°í™”**\n",
    "\n",
    "`visualize_graph` í•¨ìˆ˜ëŠ” ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(simple_react_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê·¸ë˜í”„ ì‹¤í–‰\n",
    "\n",
    "- `config` íŒŒë¼ë¯¸í„°ëŠ” ê·¸ë˜í”„ ì‹¤í–‰ ì‹œ í•„ìš”í•œ ì„¤ì • ì •ë³´ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "    - `resursion_limit`: ê·¸ë˜í”„ ì‹¤í–‰ ì‹œ ì¬ê·€ ìµœëŒ€ íšŸìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "    - `thread_id`: ê·¸ë˜í”„ ì‹¤í–‰ ì‹œ ìŠ¤ë ˆë“œ ì•„ì´ë””ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "- `inputs`: ê·¸ë˜í”„ ì‹¤í–‰ ì‹œ í•„ìš”í•œ ì…ë ¥ ì •ë³´ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì°¸ê³ **\n",
    "\n",
    "- ë©”ì‹œì§€ ì¶œë ¥ ìŠ¤íŠ¸ë¦¬ë°ì€ [LangGraph ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì˜ ëª¨ë“  ê²ƒ](https://wikidocs.net/265770) ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mtools\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "â–¹ ì‚¼ì„±ì „ì, ìì²´ ê°œë°œ ìƒì„± AI â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ ê³µê°œ Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·10\n",
      "â–¹ êµ¬ê¸€, ì•¤ìŠ¤ë¡œí”½ì— 20ì–µ ë‹¬ëŸ¬ íˆ¬ìë¡œ ìƒì„± AI í˜‘ë ¥ ê°•í™” Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·11\n",
      "â–¹ IDC, 2027ë…„ AI ì†Œí”„íŠ¸ì›¨ì–´ ë§¤ì¶œ 2,500ì–µ ë‹¬ëŸ¬ ëŒíŒŒ ì „ë§Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·12\n",
      "\n",
      "SPRi AI Brief |\n",
      "2023-12ì›”í˜¸\n",
      "ì‚¼ì„±ì „ì, ìì²´ ê°œë°œ ìƒì„± AI â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ ê³µê°œ\n",
      "KEY Contents\n",
      "n ì‚¼ì„±ì „ìê°€ ì˜¨ë””ë°”ì´ìŠ¤ì—ì„œ ì‘ë™ ê°€ëŠ¥í•˜ë©° ì–¸ì–´, ì½”ë“œ, ì´ë¯¸ì§€ì˜ 3ê°œ ëª¨ë¸ë¡œ êµ¬ì„±ëœ ìì²´ ê°œë°œ ìƒì„±\n",
      "AI ëª¨ë¸ â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ë¥¼ ê³µê°œ\n",
      "n ì‚¼ì„±ì „ìëŠ” ì‚¼ì„± ê°€ìš°ìŠ¤ë¥¼ ë‹¤ì–‘í•œ ì œí’ˆì— ë‹¨ê³„ì ìœ¼ë¡œ íƒ‘ì¬í•  ê³„íšìœ¼ë¡œ, ì˜¨ë””ë°”ì´ìŠ¤ ì‘ë™ì´ ê°€ëŠ¥í•œ\n",
      "ì‚¼ì„± ê°€ìš°ìŠ¤ëŠ” ì™¸ë¶€ë¡œ ì‚¬ìš©ì ì •ë³´ê°€ ìœ ì¶œë  ìœ„í—˜ì´ ì—†ë‹¤ëŠ” ì¥ì ì„ ë³´ìœ \n",
      "Â£ì–¸ì–´, ì½”ë“œ, ì´ë¯¸ì§€ì˜ 3ê°œ ëª¨ë¸ë¡œ êµ¬ì„±ëœ ì‚¼ì„± ê°€ìš°ìŠ¤, ì˜¨ë””ë°”ì´ìŠ¤ ì‘ë™ ì§€ì›\n",
      "\n",
      "ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ ì ìš©í•œ êµ¬ê¸€ í”½ì…€(Pixel)ê³¼ ê²½ìŸí•  ê²ƒìœ¼ë¡œ ì˜ˆìƒ\n",
      "â˜ ì¶œì²˜ : ì‚¼ì„±ì „ì, â€˜ì‚¼ì„± AI í¬ëŸ¼â€™ì„œ ìì²´ ê°œë°œ ìƒì„±í˜• AI â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ ê³µê°œ, 2023.11.08.\n",
      "ì‚¼ì„±ì „ì, â€˜ì‚¼ì„± ê°œë°œì ì½˜í¼ëŸ°ìŠ¤ ì½”ë¦¬ì•„ 2023â€™ ê°œìµœ, 2023.11.14.\n",
      "TechRepublic, Samsung Gauss: Samsung Research Reveals Generative AI, 2023.11.08.\n",
      "10\n",
      "\n",
      "Â£ì–¸ì–´, ì½”ë“œ, ì´ë¯¸ì§€ì˜ 3ê°œ ëª¨ë¸ë¡œ êµ¬ì„±ëœ ì‚¼ì„± ê°€ìš°ìŠ¤, ì˜¨ë””ë°”ì´ìŠ¤ ì‘ë™ ì§€ì›\n",
      "n ì‚¼ì„±ì „ìê°€ 2023ë…„ 11ì›” 8ì¼ ì—´ë¦° â€˜ì‚¼ì„± AI í¬ëŸ¼ 2023â€™ í–‰ì‚¬ì—ì„œ ìì²´ ê°œë°œí•œ ìƒì„± AI ëª¨ë¸\n",
      "â€˜ì‚¼ì„± ê°€ìš°ìŠ¤â€™ë¥¼ ìµœì´ˆ ê³µê°œ\n",
      "âˆ™ ì •ê·œë¶„í¬ ì´ë¡ ì„ ì •ë¦½í•œ ì²œì¬ ìˆ˜í•™ì ê°€ìš°ìŠ¤(Gauss)ì˜ ì´ë¦„ì„ ë³¸ëœ¬ ì‚¼ì„± ê°€ìš°ìŠ¤ëŠ” ë‹¤ì–‘í•œ ìƒí™©ì—\n",
      "ìµœì í™”ëœ í¬ê¸°ì˜ ëª¨ë¸ ì„ íƒì´ ê°€ëŠ¥\n",
      "âˆ™ ì‚¼ì„± ê°€ìš°ìŠ¤ëŠ” ë¼ì´ì„ ìŠ¤ë‚˜ ê°œì¸ì •ë³´ë¥¼ ì¹¨í•´í•˜ì§€ ì•ŠëŠ” ì•ˆì „í•œ ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµë˜ì—ˆìœ¼ë©°,\n",
      "ì˜¨ë””ë°”ì´ìŠ¤ì—ì„œ ì‘ë™í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ ì™¸ë¶€ë¡œ ì‚¬ìš©ìì˜ ì •ë³´ê°€ ìœ ì¶œë˜ì§€ ì•ŠëŠ” ì¥ì ì„ ë³´ìœ \n",
      "\n",
      "ì˜¨ë””ë°”ì´ìŠ¤ì—ì„œ ì‘ë™í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ ì™¸ë¶€ë¡œ ì‚¬ìš©ìì˜ ì •ë³´ê°€ ìœ ì¶œë˜ì§€ ì•ŠëŠ” ì¥ì ì„ ë³´ìœ \n",
      "âˆ™ ì‚¼ì„±ì „ìëŠ” ì‚¼ì„± ê°€ìš°ìŠ¤ë¥¼ í™œìš©í•œ ì˜¨ë””ë°”ì´ìŠ¤ AI ê¸°ìˆ ë„ ì†Œê°œí–ˆìœ¼ë©°, ìƒì„± AI ëª¨ë¸ì„ ë‹¤ì–‘í•œ ì œí’ˆì—\n",
      "ë‹¨ê³„ì ìœ¼ë¡œ íƒ‘ì¬í•  ê³„íš\n",
      "n ì‚¼ì„± ê°€ìš°ìŠ¤ëŠ” â–³í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì–¸ì–´ëª¨ë¸ â–³ì½”ë“œë¥¼ ìƒì„±í•˜ëŠ” ì½”ë“œ ëª¨ë¸ â–³ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ”\n",
      "ì´ë¯¸ì§€ ëª¨ë¸ì˜ 3ê°œ ëª¨ë¸ë¡œ êµ¬ì„±\n",
      "âˆ™ ì–¸ì–´ ëª¨ë¸ì€ í´ë¼ìš°ë“œì™€ ì˜¨ë””ë°”ì´ìŠ¤ ëŒ€ìƒ ë‹¤ì–‘í•œ ëª¨ë¸ë¡œ êµ¬ì„±ë˜ë©°, ë©”ì¼ ì‘ì„±, ë¬¸ì„œ ìš”ì•½, ë²ˆì—­ ì—…ë¬´ì˜\n",
      "ì²˜ë¦¬ë¥¼ ì§€ì›\n",
      "\n",
      "ì²˜ë¦¬ë¥¼ ì§€ì›\n",
      "âˆ™ ì½”ë“œ ëª¨ë¸ ê¸°ë°˜ì˜ AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸ â€˜ì½”ë“œì•„ì´(code.i)â€™ëŠ” ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ë¡œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ë©°\n",
      "ì‚¬ë‚´ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì— ìµœì í™”\n",
      "âˆ™ ì´ë¯¸ì§€ ëª¨ë¸ì€ ì°½ì˜ì ì¸ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê³  ê¸°ì¡´ ì´ë¯¸ì§€ë¥¼ ì›í•˜ëŠ” ëŒ€ë¡œ ë°”ê¿€ ìˆ˜ ìˆë„ë¡ ì§€ì›í•˜ë©°\n",
      "ì €í•´ìƒë„ ì´ë¯¸ì§€ì˜ ê³ í•´ìƒë„ ì „í™˜ë„ ì§€ì›\n",
      "n IT ì „ë¬¸ì§€ í…Œí¬ë¦¬í¼ë¸”ë¦­(TechRepublic)ì€ ì˜¨ë””ë°”ì´ìŠ¤ AIê°€ ì£¼ìš” ê¸°ìˆ  íŠ¸ë Œë“œë¡œ ë¶€ìƒí–ˆë‹¤ë©°,\n",
      "2024ë…„ë¶€í„° ê°€ìš°ìŠ¤ë¥¼ íƒ‘ì¬í•œ ì‚¼ì„± ìŠ¤ë§ˆíŠ¸í°ì´ ë©”íƒ€ì˜ ë¼ë§ˆ(Llama)2ë¥¼ íƒ‘ì¬í•œ í€„ì»´ ê¸°ê¸° ë° êµ¬ê¸€\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±í˜• AIì˜ ì´ë¦„ì€ 'ì‚¼ì„± ê°€ìš°ìŠ¤'ì…ë‹ˆë‹¤. ì´ AI ëª¨ë¸ì€ ì–¸ì–´, ì½”ë“œ, ì´ë¯¸ì§€ì˜ 3ê°œ ëª¨ë¸ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ì˜¨ë””ë°”ì´ìŠ¤ì—ì„œ ì‘ë™ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. 'ì‚¼ì„± ê°€ìš°ìŠ¤'ëŠ” ì‚¬ìš©ìì˜ ì •ë³´ê°€ ì™¸ë¶€ë¡œ ìœ ì¶œë˜ì§€ ì•ŠëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.messages import stream_graph\n",
    "\n",
    "# Config ì„¤ì •\n",
    "## recursion_limit : ë…¸ë“œë¥¼ ë°˜ë³µíšŸìˆ˜ (ê²°ê³¼ ì‹¤íŒ¨ì‹œ ë¦¬ë°‹ 10íšŒ)\n",
    "## thread_id : ì±— room uid \n",
    "config = {\"configurable\": {\"resursion_limit\": 10, \"thread_id\": \"abc123\"}}\n",
    "\n",
    "# ì…ë ¥ ì„¤ì •\n",
    "inputs = {\n",
    "    \"messages\": [(\"human\", \"AI Brief ë¬¸ì„œì—ì„œ ì‚¼ì„±ì „ìê°€ ë§Œë“  ìƒì„±í˜• AI ì´ë¦„ì„ ì°¾ì•„ì¤˜\")]\n",
    "}\n",
    "\n",
    "# ê·¸ë˜í”„ ìŠ¤íŠ¸ë¦¼\n",
    "stream_graph(simple_react_agent, inputs, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¶”ì : https://smith.langchain.com/public/145d8012-9791-4320-a17d-b2ec048d0110/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì°¸ê³ **: `config` ëŠ” ì´ì „ì˜ ê°’ì„ ì¬í™œìš© í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mtools\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "[{\"title\": \"What Is Claude 3.7 Sonnet? - Built In\", \"url\": \"https://builtin.com/artificial-intelligence/claude-3-7-sonnet\", \"content\": \"Claude 3.7 Sonnet is an AI model developed by Anthropic. Claude 3.7 Sonnet is hitting the market at an especially competitive moment in the artificial intelligence industry, with companies like OpenAI, Google, xAI and DeepSeek all racing to develop AI models with advanced reasoning capabilities. Claude 3.7 Sonnet is Anthropicâ€™s first publicly available reasoning model. In â€œstandard mode,â€ the model essentially functions as an upgraded version of Anthropicâ€™s Claude 3.5 Sonnet, which excels at performing complex tasks that require rapid response, like knowledge retrieval, sales automation and computer programming. Anthropic says Claude 3.7 Sonnet is its best coding model to date â€” capable of spotting and fixing bugs, developing features, explaining technical concepts and suggesting improvements across different programming languages.\", \"score\": 0.9332426, \"raw_content\": \"What Is Claude 3.7 Sonnet? | Built In\\n \\n\\nCan't find your company? Create a company profile.\\nView All Jobs\\nFor Employers\\nJoin\\nLog In\\n\\nJobs\\nCompanies\\nRemote\\nArticles\\nSalaries\\nBest Places To Work\\nMy items\\n\\nArtificial Intelligence\\nAnthropicâ€™s Claude 3.7 Sonnet Combines Quick Responses and Advanced Reasoning\\nAnthropic is stepping into the ring with the likes of OpenAI, xAI, Google and DeepSeek, launching a new â€œhybridâ€ model in an effort to achieve the most advanced reasoning capabilities.\\n\\nWritten by Ellen Glover\\nPublished on Feb. 26, 2025\\n\\n\\nImage: Shutterstock\\nAnthropic has unveiled its latest AI model, Claude 3.7 Sonnet, calling it its â€œmost intelligentâ€ to date. Released in February 2025, the large language model has â€œhybrid reasoningâ€ capabilities, meaning it can think about its response â€” pausing to consider its answer before generating it â€” as well as deliver quick, real-time outputs when needed. Outperforming several of the industryâ€™s top models, Claude 3.7 Sonnet excels at coding, software engineering, instruction-following, multimodal understanding and agentic tasks.\\nWhat Is Claude 3.7 Sonnet?\\nClaude 3.7 Sonnet is an AI model developed by Anthropic. With so-called â€œhybrid reasoningâ€ capabilities, the model can switch between rapid responses and deep, reflective thinking within a single system.\\nClaude 3.7 Sonnet is hitting the market at an especially competitive moment in the artificial intelligence industry, with companies like OpenAI, Google, xAI and DeepSeek all racing to develop AI models with advanced reasoning capabilities. What sets Anthropicâ€™s new model apart is its ability to switch between rapid responses and deep, reflective thinking within a single system. Most competing models just focus on one or the other.\\nâ€œClaude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made visible to the user,â€ Anthropic explained in a blog post, claiming it creates a â€œmore seamless experienceâ€ for users. â€œJust as humans use a single brain for both quick responses and deep reflection, we believe reasoning should be an integrated capability of frontier models rather than a separate model entirely.â€\\nClaude 3.7 Sonnet can be accessed through the Claude chatbot across all subscription tiers (including the free tier), but its â€œextended thinkingâ€ mode is limited to Pro, Team and Enterprise subscribers. The model is also available through the Anthropic API, Amazon Bedrock and Google Cloudâ€™s Vertex AI platforms.\\nWhat Is Claude 3.7 Sonnet?\\nClaude 3.7 Sonnet is a foundation model designed to understand and generate human-like text. Capable of providing both quick, pattern-based outputs as well as more nuanced, thought-out answers, it performs especially well in tasks involving coding, instruction-following, multimodal understanding and agentic capabilities.\\nClaude 3.7 Sonnet was developed by Anthropic, an AI research and development startup founded in 2021 by former OpenAI executives. Known for its Claude chatbot and large suite of language models, the companyâ€™s stated goal is to responsibly advance the field of generative AI, with an emphasis on safety and ethics. To that end, Anthropic develops some of the most advanced AI products on the market â€” right alongside top competitors â€” but does not release them until they have sufficiently met its robust safety measures.\\nAnthropic says it conducted extensive testing, training and evaluation of Claude 3.7 Sonnet, working with â€œexternal expertsâ€ to ensure it meets all of its security, safety and reliability standards. The company also claims Claude 3.7 Sonnet can make more nuanced distinctions between harmful and benign prompts, so it will reject or defer questions less frequently than previous Claude models.\\nMore AI Model NewsWhat Is Nvidia Cosmos?\\nWhat Can Claude 3.7 Sonnet Do?\\nClaude 3.7 Sonnet can do much of what other comparable models can do: answer questions, brainstorm ideas, summarize content and generate new content â€” accepting both images and text as inputs. But it stands out from other Anthropic models in a few important ways.\\nReasoning\\nClaude 3.7 Sonnet is Anthropicâ€™s first publicly available reasoning model. In general, these kinds of models are designed to break down problems into smaller, more manageable steps, verifying facts along the way before generating a final answer. While they donâ€™t necessarily replicate human thinking or reasoning, their process is modeled after deduction, with the aim of providing more accurate and reliable responses.\\nFunctioning as both a typical large language model and reasoning model in one, Claude 3.7 Sonnet lets users pick whether they want a quick answer from the model or whether they want it to think longer before answering.\\n\\nIn â€œstandard mode,â€ the model essentially functions as an upgraded version of Anthropicâ€™s Claude 3.5 Sonnet, which excels at performing complex tasks that require rapid response, like knowledge retrieval, sales automation and computer programming.\\nWhen â€œextended thinking modeâ€ is turned on, the model creates â€œthinking content blocksâ€ where it visually displays its internal reasoning to the user, according to Anthropic. Those insights are then incorporated into its final response, improving the modelâ€™s performance in math, physics, instruction-following and coding, among other tasks.\\n\\nThrough Anthropicâ€™s API, users can control Claude 3.7 Sonnetâ€™s â€œthinkingâ€ budget, meaning they can set a limit on how long the model should reason before responding (with a maximum of 128,000 tokens). This essentially allows them to balance speed and cost with the quality of the answer. In both standard and extended thinking modes, Claude 3.7 Sonnet costs $3 per million input tokens and $15 per million output tokens, including those used for thinking.\\nCoding\\nAnthropic says Claude 3.7 Sonnet is its best coding model to date â€” capable of spotting and fixing bugs, developing features, explaining technical concepts and suggesting improvements across different programming languages. Its extended thinking mode is specifically optimized for powering AI agents that can handle more complex tasks and workflows, thus accelerating the building process across the entire software development lifecycle.\\nIn addition to Claude 3.7 Sonnet, Anthropic released a preview of its own agentic coding tool called Claude Code. Acting as an â€œactive collaborator,â€ the company says the tool can search and read code, edit files, write and run tests and use command tools â€” all the while keeping users â€œin the loop.â€\\nAnthropic claims Claude Code can complete tasks like test-driven development, debugging complex issues and large-scale refactoring â€” tasks that would typically take a human more than 45 minutes of manual work. In a video demonstration, the tool was able to analyze a project with a simple command like, â€œExplain this project structure.â€ Using plain English in the command line, developers were able to modify their code, with Claude Code describing its changes, testing for errors and even pushing updates to GitHub.\\nClaude 3.7 Sonnet Use Cases\\nLike all of Anthropicâ€™s models, Claude 3.7 can be used in all sorts of ways. The company has highlighted a few in its documentation:\\n\\nSoftware engineering: Claude 3.7 Sonnet specifically achieves â€œstate-of-the-artâ€ performance on software engineering benchmarks, according to Anthropic, so it is good at solving complex software-related issues. This makes it a strong tool for tasks like code generation, debugging and automating development workflows.\\nTicket routing: The modelâ€™s advanced natural language processing capabilities can be used to automatically sort and route customer support tickets based on factors like urgency, customer intent, priority, customer profile and more.\\nCustomer support agent: The modelâ€™s advanced conversational abilities can be used to build automated customer support agents that can handle inquiries in real time, providing around-the-clock support and managing high request volumes with accurate responses and â€œpositiveâ€ interactions.\\nContent moderation: Specifically trained to be as â€œhonest, helpful and harmlessâ€ as possible, the model can be used to moderate digital applications, helping to maintain a safe, respectful and productive environment.\\nLegal summarization: With its advanced natural language processing capabilities, the model can efficiently summarize legal documents, extracting key information to expedite the legal research process. It can be used to review contracts, assist with litigation preparation and support regulatory work, saving users time while maintaining accuracy.\\n\\nMore AI ContentAI Basics: Artificial Intelligence 101\\nHow Does Claude 3.7 Sonnet Compare to Other Models?\\nAnthropic compared Claude 3.7 Sonnet to other models of similar sizes and capabilities, including OpenAIâ€™s o1 and o3-mini, DeepSeekâ€™s R1 and xAIâ€™s Grok 3, as well as its own Claude 3.5 Sonnet. These comparisons evaluated capabilities like software engineering, agentic tool use, instruction following, general reasoning, multimodal capabilities and agentic coding.\\nIn short: Claude 3.7 Sonnet outperformed most of its competitors across the majority of these tests when it was in extended thinking mode. However, it scored lower than Grok 3 in graduate-level reasoning (GPQA Diamond); o1 in multilingual Q&A (MMMLU); both Grok 3 and o1 in visual reasoning (MMMU); o1, o3-mini and R1 in math problem-solving (MATH 500); and Grok 3, o1, o3-mini and R1 in high school math competition (AIME 2024). Claude 3.7 Sonnet also performed well in standard mode, but it did not beat out its competitors as consistently as when it was in extended thinking mode.\\nBeyond these more traditional benchmarks, Claude 3.7 Sonnet outperformed all of Anthropicâ€™s previous models in its PokÃ©mon gameplay tests when it was in extended thinking mode.\\nClaude 3.7 Sonnet Limitations\\nLike any other AI model, Claude 3.7 Sonnet is capable of producing inaccurate responses, and may reflect the biases present in its training data in its outputs. It also does not perform as well as other models in math-related tasks when it is in standard mode. However, it seems to experience a notable boost in this area when it is in extended thinking mode.\\nRelated Reading3 Things We Need to Fix Before AI Agents Go Mainstream\\nHow to Access Claude 3.7 Sonnet\\nClaude 3.7 Sonnet can be accessed in several locations, including:\\n\\nClaude chatbot: Claude 3.7 Sonnetâ€™s standard mode is available on all subscription tiers (Free, Pro, Team and Enterprise). However, its extended thinking mode is exclusive only to Pro, Team and Enterprise subscribers.\\nAnthropicâ€™s API: Users can integrate Claude 3.7 Sonnet into their own applications by accessing it through Anthropicâ€™s API. You can learn how to use the API to build with Claude in this step-by-step guide.\\nThird-party platforms: Claude 3.7 Sonnet is available on the Amazon Bedrock and Google Cloudâ€™s Vertex AI platforms, where users can integrate and deploy the model into their own applications without having to manage the underlying infrastructure themselves.\\n\\nFrequently Asked Questions\\nIs Claude 3.7 Sonnet available?\\nYes â€” Claude 3.7 Sonnet is available through the Claude chatbot across all subscription tiers (including Free), but its extended thinking mode is exclusive to Pro, Team and Enterprise subscribers. Claude 3.7 Sonnet is also available through the Anthropic API, Amazon Bedrock and Google Cloudâ€™s Vertex AI platforms.\\nIs Claude 3.7 Sonnet free?\\nYes â€” a standard version of Claude 3.7 Sonnet can be accessed for free through the Claude chatbot. However, its extended thinking capabilities are only available in the Pro, Team and Enterprise subscription tiers, which vary in price. Otherwise, the model costs $3 per million input tokens and $15 per million output tokens on the Anthropic API, Amazon Bedrock and Google Cloudâ€™s Vertex AI platforms.\\nIs Claude 3.7 Sonnet multimodal?\\nYes â€” Claude 3.7 Sonnet accepts both text and image inputs, so it has multimodal capabilities. But it is only able to generate text responses.\\nIs Claude 3.7 Sonnet safe?\\nWhile no AI model is completely risk-free, Anthropic says it conducted extensive testing, training and evaluation of Claude 3.7 Sonnet, working with â€œexternal expertsâ€ to ensure it meets its security, safety and reliability standards. The company also claims Claude 3.7 Sonnet can make more nuanced distinctions between harmful and benign prompts, so it defers questions less frequently than prior models. Specifically, the model reduces unnecessary refusals by 45 percent in standard mode and 31 percent in extended thinking mode compared to Claude 3.5 Sonnet.\\nWhat is Claude Code?\\nClaude Code is an agentic coding tool developed by Anthropic that can autonomously perform advanced tasks like searching and reading code, editing files, writing and running tests, using command tools and even pushing updates to GitHub.\\nWhat is a reasoning model?\\nIn general, reasoning models are designed to analyze complex problems, break them down into management steps and refine their responses before delivering a final answer. The goal is to provide more accurate and reliable responses than standard language models, which generate quick, pattern-based outputs. In the case of Claude 3.7 Sonnet specifically, the model can switch between rapid responses and deep, reflective thinking within a single system.\\nRecent Artificial Intelligence Articles\\n 6 Top Facial Recognition Companies LightGBM: A Guide 20 AI in Education Examples to Know\\nExplore Job Matches.\\nJob Title or Keyword Clear search\\nLocation\\nNo Results Found\\nJob Type\\nClear Apply\\nSee Jobs\\n\\nJobs\\nCompanies\\nArticles\\nMy Items\\nMore\\n\\n\\nJoin\\nLog In\\n\\nTech Jobs\\nCompanies\\nArticles\\nRemote\\nSalaries\\nBest Places To Work\\nTech Hubs\\n\\nPost Job\\n\\n\\nBuilt In is the online community for startups and tech companies. Find startup jobs, tech news and events.\\n\\nAbout\\nOur Story\\nCareers\\nOur Staff Writers\\nContent Descriptions\\n\\nGet Involved\\nRecruit With Built In\\nBecome an Expert Contributor\\n\\nResources\\nCustomer Support\\nShare Feedback\\nReport a Bug\\nBrowse Jobs\\nTech A-Z\\n\\nTech Hubs\\nOur Sites\\n\\nLearning Lab User Agreement Accessibility Statement Copyright Policy Privacy Policy Terms of Use Your Privacy Choices/Cookie Settings CA Notice of Collection\\nÂ© Built In 2025\"}, {\"title\": \"Claude 3.7 Sonnet is Anthropic's AI Resurgence\", \"url\": \"https://www.unite.ai/claude-3-7-sonnet-is-anthropics-ai-resurgence/\", \"content\": \"Claude 3.7 Sonnet is Anthropicâ€™s AI Resurgence Claude 3.7 Sonnet is Anthropicâ€™s AI Resurgence Billed as the companyâ€™s â€œmost intelligent model to dateâ€ and the first hybrid reasoning AI on the market, Claude 3.7 Sonnet introduces some major enhancements over its predecessor (Claude 3.5 Sonnet) in speed, reasoning, and real-world task performance. For many regular AI users, Claude 3.5 Sonnet had already been a go-to tool. Rather than releasing a separate Claude reasoning edition, Anthropic has merged both quick and deep thinking into one AI. Power users can even fine-tune how much the AI thinks: through the API, developers can set a token budget for reasoning, telling Claude how long to ponder (from just a few steps up to a massive 128k-token thought process) before finalizing an answer.\", \"score\": 0.92442214, \"raw_content\": \"Published Time: 2025-02-25T17:24:00+00:00\\nClaude 3.7 Sonnet is Anthropicâ€™s AI Resurgence - Unite.AI\\n\\n\\nAI Tools\\nBusiness\\nChatbots\\nCode Generators\\nEducation\\nCrypto Trading\\nHeadshot Generators\\nImage Enhancers\\nImage Generators\\nMarketing Tools\\nMusic Generators\\nSEO\\nStock Trading\\nText to Speech\\nTranscription\\nTranslation\\nVideo Enhancers\\nVideo Generators\\nVoice Generators\\nWriting Tools\\n\\n\\nEvents\\nAI Conferences\\nAR, VR & XR Conferences\\nCybersecurity Conferences\\nRobotics Conferences\\nSEO Conferences\\n\\n\\nNews\\nArtificial Intelligence\\nArtificial General Intelligence\\nBrain Machine Interface\\nCybersecurity\\nEthics\\nHealthcare\\nInterviews\\nFunding\\nQuantum Computing\\nRegulation\\nRobotics\\nSurveillance\\nThought Leaders\\n\\n\\nCertifications\\nBlockchain\\nCloud\\nCybersecurity\\nData Science\\nMachine Learning\\nNatural Language Processing\\nPrompt Engineering\\nPython\\nRobotic Process Automation\\nTensorFlow\\n\\n\\nPython Libraries\\nData Science\\nDeep Learning\\nImage Processing\\nMachine Learning\\nNatural Language Processing\\n\\n\\nDomain Names\\nNewsletters\\nContact Us\\n\\nConnect with us\\n\\n\\n\\n\\n\\n\\n\\nUnite.AI\\nClaude 3.7 Sonnet is Anthropicâ€™s AI Resurgence\\n\\n\\nAI Tools\\n\\n\\n\\n\\n\\nGenerative AI\\n            *   Business Plans\\n            *   Cartoonize\\n            *   Code\\n            *   Emails\\n            *   Headshots\\n            *   Images\\n            *   Media Kits\\n            *   Music\\n            *   Pitch Deck\\n            *   Presentations\\n            *   Sketch\\n            *   Video\\n            *   Voice\\n            *   Writing\\n\\n\\n\\n\\nBusiness AI\\n        *   Business\\n        *   Chatbots\\n        *   Doc Management\\n        *   ETL\\n        *   Legal Assistants\\n        *   Marketing\\n        *   Public Speaking\\n        *   Recruiting\\n        *   Resume & CV\\n        *   SEO\\n        *   Social Media\\n        *   Text to Speech\\n        *   White Label\\n        *   Work Management\\n\\n\\n\\n\\nOptimization AI\\n        *   AI Assistants\\n        *   App Builders\\n        *   Audio Enhancers\\n        *   Chrome Extensions\\n        *   Data Cleaning\\n        *   Image Enhancers\\n        *   Image Extenders\\n        *   Image Resizer\\n        *   Photo Editing\\n        *   Transcription\\n        *   Translation\\n        *   Video Enhancers\\n        *   Website Builders\\n\\n\\n\\n\\nToolkit AI\\n        *   CRM Platforms\\n        *   Data Analysts\\n        *   Education\\n        *   Fashion Designers\\n        *   Medical Scribes\\n        *   Trading Crypto\\n        *   Trading Stocks\\n        *   Task Management\\n        *   Trend Analysis\\n\\n\\n\\n\\n\\n\\n\\nCertifications\\n\\nBlockchain Certifications\\nChatbots\\nCloud\\nCybersecurity\\nData Science\\nMachine Learning\\nNatural Language Processing\\nPrompt Engineering\\nPython\\nRobotic Process Automation\\nTensorFlow\\n\\n\\nEvents\\nAI Conferences\\nAR, VR & XR Conferences\\nCybersecurity Conferences\\nRobotics Conferences\\nSEO Conferences\\n\\n\\nNews\\nAll\\nArtificial Intelligence\\nArtificial General Intelligence\\nAugmented Reality\\nBrain Machine Interface\\nCybersecurity\\nEthics\\nFuturist Series\\nHealthcare\\nFunding\\nQuantum Computing\\nRegulation\\nRobotics\\nSurveillance\\n\\n\\nInterviews\\nThought Leaders\\n.AI Domains\\n\\nArtificial Intelligence\\nClaude 3.7 Sonnet is Anthropicâ€™s AI Resurgence\\n\\nPublished\\n1 hour agoon\\nFebruary 25, 2025 \\nBy\\nAlex McFarland\\nTable Of Contents\\n\\n(Alex McFarland/Unite AI)\\nAnthropic has released Claude 3.7 Sonnet, a highly-anticipated upgrade to its large language model (LLM) family. Billed as the companyâ€™s â€œmost intelligent model to dateâ€ and the first hybrid reasoning AI on the market, Claude 3.7 Sonnet introduces some major enhancements over its predecessor (Claude 3.5 Sonnet) in speed, reasoning, and real-world task performance.\\nThe rollout comes amid fast advances from competitors like OpenAI and xAIâ€™s recent Grok 3, leading many AI enthusiasts (including me) to view this launch as Anthropicâ€™s answer to recent innovations. The new model aims to blend quick conversational answers with deeper analytical thinking in one system â€“ a unified approach that could show us what future interaction with AI will look like.\\nLong-Awaited Upgrade to a Beloved AI Assistant\\nFor many regular AI users, Claude 3.5 Sonnet had already been a go-to tool. It was regarded as one of the best out there. However, in recent months Anthropic faced growing pressure. The AI industry has been going crazy with new features and models â€“ OpenAIâ€™s ChatGPT gained voice, multi-step reasoning abilities, and deep research. Grok 3 made its debut with real-time X data, and other platforms like Perplexity and Gemini kept the releases coming. Many observers started to note that Anthropic was starting to fall behind. The community had been eagerly awaiting Anthropicâ€™s response, with expectations that a new Claude model was due any day.\\nClaude 3.7 Sonnet arrived at last to meet those expectations. It is a significant leap forward from Claude 3.5, rather than a minor tweak. Anthropic touts it as a comprehensive upgrade: faster, smarter, and more versatile.\\nThe modelâ€™s speed and output quality are striking. In my own tests, I found it to be incredibly fast compared to the last version, processing lengthy text inputs almost instantaneously. Given Anthropicâ€™s slow update cycle, the 3.7 release feels like a long-awaited catch-up that reclaims Claudeâ€™s position in the AI race. Claude 3.7 doubles down on what made users love Claude 3.5 â€“ exceptional performance in practical tasks â€“ while adding innovative reasoning capabilities under the hood.\\nHybrid Reasoning: Quick Answers and Deep Thinking in One\\nThe headline feature of Claude 3.7 Sonnet is its hybrid reasoning capability. In simple terms, this model can operate in two modes: a standard mode for near-instant responses, and a new â€œextended thinkingâ€ mode where it works through problems step-by-step, showing its chain-of-thought to the user.\\nRather than releasing a separate Claude reasoning edition, Anthropic has merged both quick and deep thinking into one AI. â€œJust as humans use a single brain for both quick responses and deep reflection, we believe reasoning should be an integrated capabilityâ€¦ rather than a separate model entirely,â€ the company explained in its announcement, emphasizing a unified approach for a seamless user experience.\\nIn practice, this means users can decide when they want a fast answer and when to let Claude deliberate at length. A simple toggle lets you switch to extended mode if a question requires detailed analysis or multi-step logic. In standard mode, Claude 3.7 Sonnet functions like an improved version of 3.5 â€“ faster and more refined, but with the familiar quick conversational style. In extended mode, the AI â€œself-reflectsâ€ before answering, writing out its reasoning process internally (and making it visible) to arrive at more accurate or complex solutions.\\nThe chain-of-thought scrolls out step by step on screen, a feature that has become popular in other advanced AI systems and now finally comes to Claude.\\nAlex McFarland/Unite AI\\nAnthropicâ€™s philosophy here deliberately contrasts with some competitors. OpenAI, for instance, has offered separate models or modes, which some find confusing to juggle. Claude 3.7â€™s all-in-one approach is meant to simplify things for users. Switching between modes is straightforward, and prompt style remains the same. Power users can even fine-tune how much the AI thinks: through the API, developers can set a token budget for reasoning, telling Claude how long to ponder (from just a few steps up to a massive 128k-token thought process) before finalizing an answer. This granular control lets one trade off speed for thoroughness on demand.\\nKey Improvements in Claude 3.7 Sonnet:\\nHere are some of the main improvements that we see from Claude 3.7 Sonnet:\\n\\nHybrid Reasoning Modes â€“ Offers both instant answers and an Extended Thinking mode where the AI works through problems stepwise with visible reasoning. Users choose the mode per query, unifying fast chat and deep analysis in one system.\\nUnified Model Philosophy â€“ Integrates quick and reflective thinking in a single AI â€œbrainâ€ for ease of use. This contrasts with rivals requiring multiple models or plugins, reducing complexity for the end-user.\\nSpeed and Responsiveness â€“ Delivers answers faster than Claude 3.5. Early tests show noticeably snappier performance in standard mode.\\nExpanded Thinking Control â€“ Through the API, users can limit or extend the AIâ€™s reasoning length (up to 128,000 tokens) to balance speed vs. quality as needed. This ensures extended mode is used only as much as necessary.\\nReal-World Task Focus â€“ According to the company, Claude 3.7â€™s training was shifted toward practical business and creative tasks rather than tricky math Olympiad puzzles. The model excels at everyday problem-solving and tasks that reflect common use cases.\\nCoding and Tool Use â€“ Stronger performance in programming tasks, especially front-end web development. Anthropic even launched a companion tool, Claude Code, which allows developers to use Claude from the command line for writing and fixing code. Early benchmarks show Claude 3.7 topping charts in solving real software issues.\\n\\nLimitations and Whatâ€™s Next for AI Users\\nDespite all the excitement, Claude 3.7 Sonnet is not without limits, and it is not a magic bullet for all AI challenges. For one, Anthropic consciously de-emphasized certain domains in training this model. They â€œoptimized somewhat less for math and computer science competition problemsâ€ in favor of more everyday business tasks. This means that while Claude 3.7 can certainly solve math and coding questions (often better than 3.5 could), it might not top the leaderboard on every academic benchmark or puzzle. Users whose needs skew toward complex math proofs or specialized coding contests might still find areas where Claudeâ€™s answers require double-checking or where a competitorâ€™s model tuned for that niche does better. Anthropic seems to have accepted this trade-off, aiming the model at practical utility over theoretical prowess.\\nAdditionally, Extended Thinking mode, while powerful, introduces some complexity. It is inherently slower than the standard mode; when the AI is in deep thought, users will notice a brief pause as it works through its reasoning. This is expected â€“ trading speed for thoroughness â€“ but it means users must decide when they actually need that extra power. In many everyday chat queries, the standard mode will suffice and be more efficient. There is also the fact that extended reasoning can sometimes overdo it and provide a lot more than you actually need. In some cases, this could overwhelm or veer off track. Anthropic will need to ensure that the AIâ€™s willingness to â€œgo bigâ€ with ideas remains relevant and on-topic. Users may learn to prompt more precisely or set token limits to curb runaway tangents.\\nIn terms of knowledge and modalities, Claude 3.7 remains primarily a text-based model. Unlike ChatGPTâ€™s vision features or other models incorporating image or voice inputs, Claude does not yet natively â€œseeâ€ images or speak aloud. Its strength is in textual understanding and generation. For most, this is not necessarily a downside â€“ but those hoping for a Claude that can analyze a photo or handle voice commands will have to wait for future iterations. Anthropic has not announced any multimodal functionality in Sonnet at this time. The focus has clearly been on refining the core language abilities and reasoning process.\\nThe Bottom Line\\nClaude 3.7 Sonnetâ€™s release is a statement that Anthropic is very much in the game alongside OpenAI, Google/DeepMind, and new players like xAI. For AI enthusiasts and developers, it adds another top-tier model to experiment with, one that offers a unique twist with its hybrid reasoning.\\nIn the competitive AI industry, Anthropicâ€™s latest move may also influence how companies position their models. By choosing not to do a massive model size jump or a glitzy multi-modal demo, but instead refining the user experience (unification of modes, speed, practical use cases), Anthropic is carving a niche focused on usability and reliability.\\nOverall, Claude 3.7 Sonnet is a pivotal moment for Anthropic. It is an evolution of the Claude series that shows the company learning from the communityâ€™s needs â€“ doubling down on strengths while addressing weaknesses. There are still areas to watch (and future Claude iterations to anticipate), but this release has clearly re-energized Anthropicâ€™s user base.\\nRelated Topics:anthropicclaude\\nDon't MissImandraX: A Breakthrough in Neurosymbolic AI Reasoning and Automated Logical Verification\\n\\nAlex McFarland\\n\\nAlex McFarland is an AI journalist and writer exploring the latest developments in artificial intelligence. He has collaborated with numerous AI startups and publications worldwide.\\n\\nYou may like\\n*   Citations: Can Anthropicâ€™s New Feature Solve AIâ€™s Trust Problem? *   Breaking Data Barriers: Can Anthropicâ€™s Model Context Protocol Enhance AI Performance? *   Anthropic Just Became Americaâ€™s Most Intriguing AI Company *   Claudeâ€™s Model Context Protocol (MCP): A Developerâ€™s Guide *   Claude Feature Enables Customized Writing Styles *   Amazon Seeks to Deepen AI Partnership with Anthropic Through Strategic Chip-Focused Investment\\n\\nAbout Us\\nMeet the Team\\nOur Charter\\n.AI Domain Names\\nPress Tools\\nContact Us\\n\\nAdvertiser Disclosure: Unite.AI is committed to rigorous editorial standards to provide our readers with accurate information and news. We may receive compensation when you click on links to products we reviewed.\\n\\n\\n\\n\\n\\n\\n\\nCopyright Â© 2025 Unite.AI\\n\\nEditorial Policy\\nPrivacy Policy\\nTerms and Conditions\\n\\n\"}, {\"title\": \"Claude 3.7 Sonnet debuts with \\\"extended thinking\\\" to tackle complex ...\", \"url\": \"https://arstechnica.com/ai/2025/02/claude-3-7-sonnet-debuts-with-extended-thinking-to-tackle-complex-problems/\", \"content\": \"Claude 3.7 Sonnet debuts with â€œextended thinkingâ€ to tackle complex problems - Ars Technica On Monday, Anthropic announced Claude 3.7 Sonnet, a new AI language model with a simulated reasoning (SR) capability called \\\"extended thinking,\\\" allowing the system to work through problems step by step. 3.7's predecessor, Claude 3.5 Sonnet, was excellent at programming tasks compared to other AI models in our experience, and according to Anthropic, early testing indicates strong performance in that area. An example of Claude 3.7 Sonnet with extended thinking is asked, \\\"Compose 5 original dad jokes that are not found anywhere in the world.\\\" Credit: Benj Edwards\", \"score\": 0.8858788, \"raw_content\": \"Published Time: 2025-02-24T22:23:34+00:00\\nClaude 3.7 Sonnet debuts with â€œextended thinkingâ€ to tackle complex problems - Ars Technica\\nSkip to content\\nArs Technica home\\nSections\\nForum\\nSubscribe\\n\\n\\nAI\\nBiz & IT\\nCars\\nCulture\\nGaming\\nHealth\\nPolicy\\nScience\\nSecurity\\nSpace\\n\\nTech\\n\\n\\nFeature\\n\\nReviews\\n\\nStore\\n\\n\\nAI\\n\\nBiz & IT\\nCars\\nCulture\\nGaming\\nHealth\\nPolicy\\nScience\\nSecurity\\nSpace\\nTech\\n\\nForum\\nSubscribe\\nStory text\\nSize  Width *  Links \\n* Subscribers only\\nLearn more\\nPin to story\\nTheme\\n\\nHyperLight\\nDay & Night\\nDark\\nSystem\\n\\nSearch dialog...\\nSign In\\nSign in dialog...\\nSign in\\nponder me this\\nClaude 3.7 Sonnet debuts with â€œextended thinkingâ€ to tackle complex problems\\nAnthropic's first simulated reasoning model is a beast at coding tasks.\\nBenj Edwards â€“ Feb 24, 2025 10:23 PM | 22\\n \\nCredit: Anthropic\\nCredit: Anthropic\\nText settings\\nStory text\\nSize  Width *  Links \\n* Subscribers only\\nLearn more\\nMinimize to nav\\nOn Monday, Anthropic announced Claude 3.7 Sonnet, a new AI language model with a simulated reasoning (SR) capability called \\\"extended thinking,\\\" allowing the system to work through problems step by step. The company also revealed Claude Code, a command line AI agent for developers currently available as a limited research preview.\\nAnthropic calls Claude 3.7 the first \\\"hybrid reasoning model\\\" on the market, giving users the option to choose between quick responses or extended, visible chain-of-thought processing similar to OpenAI's o1 and o3 series models, Google's Gemini 2.0 Flash Thinking, and DeepSeek's R1. When using Claude 3.7's API, developers can specify exactly how many tokens the model should use for thinking, up to its 128,000 token output limit.\\nThe new model is available across all Claude subscription plans, and the extended thinking mode feature is available on all plans except the free tier. API pricing remains unchanged at $3 per million input tokens and $15 per million output tokens, with thinking tokens included in the output pricing since they are part of the context considered by the model.\\nIn another interesting developmentâ€”since Claude 3.5 Sonnet was known as something of a Goody Two-shoes in the AI worldâ€”Anthropic said that it had reduced unnecessary refusals in 3.7 Sonnet by 45 percent. In other words, 3.7 Sonnet is more likely to do what you ask without complaining about ethical boundaries, which can otherwise pop up in innocent situations when interpreted incorrectly by the neural network running under Claude's hood.\\nArs Video\\nIn benchmarks, Anthropic's latest model seems to hold its own, and even excels in at least one category in particular: coding. 3.7's predecessor, Claude 3.5 Sonnet, was excellent at programming tasks compared to other AI models in our experience, and according to Anthropic, early testing indicates strong performance in that area. The company claims Claude 3.7 Sonnet achieved top scores on SWE-bench Verified, which evaluates how AI models handle real-world software issues, and also in TAU-bench, which tests AI agents on complex tasks with user and tool interactions.\\n\\nA chart showing self-reported Claude 3.7 Sonnet benchmark results. Credit: Anthropic\\nAiming at software developers, Anthropic has also expanded its GitHub integration to all Claude plans, allowing devs to connect code repositories directly to Claude for bug fixes, feature development, and documentation work.\\nIn our personal experience creating hobby programs with Claude 3.5 Sonnet over the past six months, the tool proved valuable for quickly prototyping projects, but we often ran up against usage limits. So far, Anthropic has not announced a subscription plan beyond the existing \\\"Claude Pro\\\" ($20/month) that might extend them, though we suspect developers who come to rely on 3.7 are soon going to need a plan more along the lines of OpenAI's ChatGPT Pro that features vastly expanded usage options for $200 a month. As an aside, our subjective experience with o1 and o3 in coding aligns with the benchmarks in the chart above; they have not been as good as Sonnet at coding.\\nAnd speaking of upgrades, we might as well talk about the name. Claude 3.5 SonnetÂ launched in June 2024, but it received an update in October with a nearly identical name (sometimes referred to as \\\"Claude 3.5 Sonnet (new) or \\\"Claude 3.5 Sonnet (October 2024)\\\") that some users criticized as confusing. As a result, some users began unofficially calling that version \\\"Claude 3.6 Sonnet\\\" instead. Apparently, Anthropic got the message on the desire for clearer naming practices, writing \\\"Lesson learned on naming\\\" in a footnote on the Claude 3.7 release page.\\nTaking â€œextended reasoningâ€ for a spin\\nLike other SR models, Claude 3.7, with extended thinking, tries to work through more complex problems by throwing more tokens at them through an ingrained simulated reasoning process. Just like o1, o3, and DeepSeek R1, you can see the \\\"thinking\\\" process going through Claude 3.7's simulated mind while it works out an ideal answer.\\nTo test it out briefly, we gave it a couple of simple tasks, including our time-honored (and now likely compromised as part of training datasets scraped from the web) test of asking it about the origin of the \\\"magenta\\\" color name.\\n\\nAn example of Claude 3.7 Sonnet with extended thinking is asked, \\\"Would the color be called 'magenta' if the town of Magenta didn't exist?\\\" Credit: Benj Edwards\\nInterestingly, xAI's Grok 3 with \\\"thinking\\\" (its SR mode) enabled was the first model that definitively gave us a \\\"no\\\" and not an \\\"it's not likely\\\" to the magenta question. Claude 3.7 Sonnet with extended thinking also impressed us with our second-ever firm \\\"no,\\\" then an explanation.\\nIn another informal test, we asked 3.7 Sonnet with extended thinking to compose five original dad jokes. We've found in the past that our old prompt, \\\"write 5 original dad jokes,\\\" was not specific enough and always resulted in canned dad jokes pulled directly from training data, so we asked, \\\"Compose 5 original dad jokes that are not found anywhere in the world.\\\"\\n\\nAn example of Claude 3.7 Sonnet with extended thinking is asked, \\\"Compose 5 original dad jokes that are not found anywhere in the world.\\\" Credit: Benj Edwards\\nClaude made some attempts at crafting original jokes, although we'll let you judge whether they are funny or not. We will likely put 3.7 Sonnet's SR capabilities to the test more exhaustively in a future article.\\nAnthropicâ€™s first agent: Claude Code\\nSo far, 2025 has been the year of both SR models (like R1 and o3) and agentic AI tools (like OpenAI's Operator and Deep Research). Not to be left out, Anthropic has announced its first agentic tool, Claude Code.\\nClaude Code operates directly from a console terminal and is an autonomous coding assistant. It allows Claude to search through codebases, read and edit files, write and run tests, commit and push code to GitHub repositories, and execute command line tools while keeping developers informed throughout the process.\\nIntroducing Claude Code.\\nAnthropic also aims for Claude Code to be used as an assistant for debugging and refactoring tasks. The company claims that during internal testing, Claude Code completed tasks in a single session that would typically require 45-plus minutes of manual work.\\nClaude Code is currently available only as a \\\"limited research preview,\\\" with Anthropic stating it plans to improve the tool based on user feedback over time. Meanwhile, Claude 3.7 Sonnet is now available through the Claude website, the Claude app, Anthropic API, Amazon Bedrock, and Google Cloud's Vertex AI.\\n\\nBenj Edwards Senior AI Reporter\\nBenj Edwards Senior AI Reporter\\nBenj Edwards is Ars Technica's Senior AI Reporter and founder of the site's dedicated AI beat in 2022. He's also a tech historian with almost two decades of experience. In his free time, he writes and records music, collects vintage computers, and enjoys nature. He lives in Raleigh, NC.\\n22 Comments\\nComments\\nForum view\\n Loading comments...\\nPrev story\\nNext story\\nMost Read\\n\\n\\n\\n\\nAsteroid 2024 YR4 is going to miss Earth, but the story doesnâ€™t end there\\n\\n\\n\\n\\nISPs fear wave of state laws after New Yorkâ€™s $15 broadband mandate\\n\\n\\n\\n\\nJudges block DOGE access to personal data in loss for Trump administration\\n\\n\\n\\n\\nRobot with 1,000 muscles twitches like human while dangling from ceiling\\n\\n\\n\\n\\nIn war against DEI in science, researchers see collateral damage\\n\\n\\n\\nCustomize\\nby Taboolaby Taboola\\nSponsored LinksSponsored Links\\nPromoted LinksPromoted Links\\nLearn More: Rheumatoid ArthritisGoodRx Learn More\\nUndo\\nFor Mountain View Residents Only: Claim Your Massive Welcome Bonus!McLuck | Play Now Install Now\\nUndo\\n3 Dividend Stocks to Buy NowZacks Investment Research Learn More\\nUndo\\nCalifornia : Govâ€™t Gives Homeowners A Massive Tax Break To Go SolarEnergy Bill Program Learn More\\nUndo\\nThis Mascara is Made With Older Women in MindPrime Prometics Learn More\\nUndo\\nInvest in Property With Historic Average 17% Yearly ReturnInvest With Roots Learn More\\nUndo\\nArs Technica has been separating the signal from the noise for over 25 years. With our unique combination of technical savvy and wide-ranging interest in the technological arts and sciences, Ars is the trusted source in a sea of information. After all, you donâ€™t need to know everything, only whatâ€™s important.\\n\\nMore from Ars\\n\\nAbout Us\\nStaff Directory\\nNewsletters\\nArs Videos\\nGeneral FAQ\\nRSS Feeds\\n\\nContact\\n\\nContact us\\nAdvertise with us\\nReprints\\n\\nPrivacy Configurations\\nÂ© 2025 CondÃ© Nast. All rights reserved. Use of and/or registration on any portion of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Ars Technica Addendum and Your California Privacy Rights. Ars Technica may earn compensation on sales from links on this site. Read our affiliate link policy. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of CondÃ© Nast. Ad Choices\\nSearch dialog...\\nSign in dialog...\\nSign in\\n\"}, {\"title\": \"Anthropic's Claude 3.7 Sonnet is here and results are insane\", \"url\": \"https://www.bleepingcomputer.com/news/artificial-intelligence/anthropics-claude-37-sonnet-is-here-and-results-are-insane/\", \"content\": \"Remove Security Tool and SecurityTool (Uninstall Guide) Anthropic has started rolling out Claude 3.7 Sonnet, the company's most advanced model and the first hybrid reasoning model it has shipped. Early tests show that Claude 3.7 Sonnet is outperforming rivals, including OpenAI's ChatGPT models and China's DeepSeek. SWE-bench Verified shows Claude 3.7 Sonnet is the best model for coding According to a benchmark test called â€œSoftware engineering (SWE-bench verified),â€ Claude 3.7 Sonnet is at the top with roughly 62% accuracy, which goes up to 70% when using extra test-time â€œscaffolding.â€ Competing models, including Claude 3.5 Sonnet and OpenAIâ€™s variants, sit closer to the 50% range. At BleepingComputer, he covers technology news with a strong focus on Microsoft and Windows-related stories.\", \"score\": 0.8684817, \"raw_content\": \"Published Time: 2025-02-25T08:07:57-05:00\\nAnthropic's Claude 3.7 Sonnet is here and results are insane\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNews\\n\\nFeatured\\n\\nLatest\\n\\n\\nNorth Korean hackers linked to $1.5 billion ByBit crypto heist\\n\\nAustralia bans all Kaspersky products on government systems\\nBeware: PayPal \\\"New Address\\\" feature abused to send phishing emails\\n\\nMicrosoft tests ad-supported Office apps for Windows users\\n\\n\\nAnthropic's Claude 3.7 Sonnet is here and results are insane\\n\\nSave an extra $40 on a lifetime of Koofr 1TB cloud storage in this deal\\nOrange Group confirms breach after hacker leaks company documents\\nOpenAI bans ChatGPT accounts used by North Korean hackers\\n\\n\\n\\nTutorials\\n\\nLatest\\n\\nPopular\\n\\n\\nHow to access the Dark Web using the Tor Browser\\n\\nHow to enable Kernel-mode Hardware-enforced Stack Protection in Windows 11\\nHow to use the Windows Registry Editor\\n\\nHow to backup and restore the Windows Registry\\n\\n\\nHow to start Windows in Safe Mode\\n\\nHow to remove a Trojan, Virus, Worm, or other Malware\\nHow to show hidden files in Windows 7\\nHow to see hidden files in Windows\\n\\n\\n\\nVirus Removal Guides\\n\\nLatest\\nMost Viewed\\n\\nRansomware\\n\\n\\nRemove the Theonlinesearch.com Search Redirect\\n\\nRemove the Smartwebfinder.com Search Redirect\\nHow to remove the PBlock+ adware browser extension\\n\\nRemove the Toksearches.xyz Search Redirect\\n\\n\\nRemove Security Tool and SecurityTool (Uninstall Guide)\\n\\nHow to Remove WinFixer / Virtumonde / Msevents / Trojan.vundo\\nHow to remove Antivirus 2009 (Uninstall Instructions)\\n\\nHow to remove Google Redirects or the TDSS, TDL3, or Alureon rootkit using TDSSKiller\\n\\n\\nLocky Ransomware Information, Help Guide, and FAQ\\n\\nCryptoLocker Ransomware Information Guide and FAQ\\nCryptorBit and HowDecrypt Information Guide and FAQ\\nCryptoDefense and How_Decrypt Ransomware Information Guide and FAQ\\n\\n\\n\\nDownloads\\n\\nLatest\\n\\nMost Downloaded\\n\\n\\nQualys BrowserCheck\\n\\nSTOPDecrypter\\nAuroraDecrypter\\n\\nFilesLockerDecrypter\\n\\n\\nAdwCleaner\\n\\nComboFix\\nRKill\\nJunkware Removal Tool\\n\\n\\n\\nDeals\\n\\n\\nCategories\\n\\n\\neLearning\\n\\nIT Certification Courses\\nGear + Gadgets\\nSecurity\\n\\n\\n\\nVPNs\\n\\n\\nPopular\\n\\n\\nBest VPNs\\n\\nHow to change IP address\\nAccess the dark web safely\\nBest VPN for YouTube\\n\\n\\n\\nForums\\n\\n\\nMore\\n\\nStartup Database\\nUninstall Database\\nGlossary\\nChat on Discord\\nSend us a Tip!\\nWelcome Guide\\n\\n\\n\\nHome\\n\\nNews\\nArtificial Intelligence\\n\\nAnthropic's Claude 3.7 Sonnet is here and results are insane\\n\\n\\n\\n\\n\\nAnthropic's Claude 3.7 Sonnet is here and results are insane\\nBy\\nMayank Parmar\\n\\nFebruary 25, 2025\\n08:07 AM\\n0\\n\\n\\nAnthropic has started rolling out Claude 3.7 Sonnet, the company's most advanced model and the first hybrid reasoning model it has shipped.\\nEarly tests show that Claude 3.7 Sonnet is outperforming rivals, including OpenAI's ChatGPT models and China's DeepSeek.\\nIn a blog post, Anthropic noted that its newest model combines fast, straightforward answers with the ability to â€œthinkâ€ step-by-step for complex tasks. This makes the Claude 3.7 model the best for programming, and these claims are backed by benchmarks.\\n\\nSWE-bench Verified shows Claude 3.7 Sonnet is the best model for coding\\nAccording to a benchmark test called â€œSoftware engineering (SWE-bench verified),â€ Claude 3.7 Sonnet is at the top with roughly 62% accuracy, which goes up to 70% when using extra test-time â€œscaffolding.â€\\nCompeting models, including Claude 3.5 Sonnet and OpenAIâ€™s variants, sit closer to the 50% range.\\n\\\"Software engineering (SWE-bench verified)\\\" is a benchmarking standard to see how well an AI model does when asked to code a program.\\nThese results show that Claude 3.7 Sonnet is significantly ahead of its competitors in terms of coding.\\nAGI moment for some users\\nUsers are also claiming that the results are insane.\\nFor example, in a thread, Reddit users noted that the modelÂ delivered outstanding results when they used it to create apps or even games.\\nâ€œClaude Code was my â€˜Feel the AGI moment.â€™ Iâ€™ve thrown bugs at this thing that no other models could fix, but Claude Code blasted through them,\\\" one user wrote in a Reddit thread.\\nAnother user added: â€œ3.7 just slapped out an entire project I had been working on for monthsâ€”5000 lines of code, front-end, debugging example, all from scratch. It didnâ€™t stop until the job was done.â€\\n\\nClaude 3.7 Sonnet benchmarks\\nAdditionally, Claude 3.7 Sonnet appears to excel in most categories, with its â€œextended thinkingâ€ mode boosting accuracy on tasks like math and science.\\nOther models, such as OpenAIâ€™s 0.1 and DeepSeek R1, trail behind on many of these tests.\\nRelated Articles:\\nOpenAI bans ChatGPT accounts used by North Korean hackers\\nGoogle Chrome's AI-powered security feature rolls out to everyone\\nMicrosoft raises rewards for Copilot AI bug bounty program\\nChinese cyberspies use new SSH backdoor in network device hacks\\nGoogle says hackers abuse Gemini AI to empower their attacks\\n\\nAI\\nAnthropic\\nClaude\\nClaude 3.7\\n\\nOpenAI\\n\\n\\n\\n\\n\\n\\n\\n\\nMayank Parmar\\nMayank Parmar is an technology entrepreneur who is currently pursuing an MBA. At BleepingComputer, he covers technology news with a strong focus on Microsoft and Windows-related stories. He is always poking under the hood of Windows, looking for the latest secrets to reveal.\\n\\nPrevious Article\\n\\nPost a Comment Community Rules\\nYou need to login in order to post a comment\\nNot a member yet? Register Now\\nYou may also like:\\nPopular Stories\\n\\n Beware: PayPal \\\"New Address\\\" feature abused to send phishing emails\\n Russia warns financial sector of major IT service provider hack\\n Fake CS2 tournament streams used to steal crypto, Steam accounts\\n\\nSponsor Posts\\n\\n Discover full attack chains and identify their root cause. Learn more about Automated Security Validation.\\n Get the GOAT Guide to learn how to start validating, start defending, and start winning.\\n RDP Security Simplified - No VPN, No Firewall Exposure. Get a free TruGrid business trial.\\n Integrating LLMs into security operations using Wazuh. Learn how to get started.\\n 5 Browser Security Threats Overlooked by Security Tools. Get the Free Report\\n\\nFollow us:\\n\\n\\n\\n\\n\\n\\n\\nMain Sections\\n\\nNews\\nVPN Buyer Guides\\nSysAdmin Software Guides\\nDownloads\\nVirus Removal Guides\\nTutorials\\nStartup Database\\nUninstall Database\\nGlossary\\n\\nCommunity\\n\\nForums\\nForum Rules\\nChat\\n\\nUseful Resources\\n\\nWelcome Guide\\nSitemap\\n\\nCompany\\n\\nAbout BleepingComputer\\nContact Us\\nSend us a Tip!\\nAdvertising\\nWrite for BleepingComputer\\nSocial & Feeds\\nChangelog\\n\\nTerms of Use - Privacy Policy - Ethics Statement - Affiliate Disclosure\\nCopyright @ 2003 - 2025 Bleeping ComputerÂ® LLC - All Rights Reserved\\n\\n\\nLogin\\nUsername \\nPassword \\nRemember Me\\nSign in anonymously\\n Sign in with Twitter\\n\\nNot a member yet? Register Now\\n\\nReporter\\nHelp us understand the problem. What is going on with this comment?\\n\\nSpam\\nAbusive or Harmful\\nInappropriate content\\nStrong language\\nOther\\n\\nRead our posting guidelinese to learn what content is prohibited.\\nSubmitting...\\nSUBMIT\\n\"}, {\"title\": \"Anthropic's Claude 3.7 Sonnet is now available in Amazon Bedrock\", \"url\": \"https://aws.amazon.com/about-aws/whats-new/2025/02/anthropics-claude-3-7-sonnet-amazon-bedrock/\", \"content\": \"Anthropic's Claude 3.7 Sonnet is now available in Amazon Bedrock - AWS About AWS Contact Us Support Â  English Â  My Account Â  AWS Management Console AWS Support Overview AWS re:Post AWS re:Post Anthropic's Claude 3.7 Sonnet is now available in Amazon Bedrock Anthropic's Claude 3.7 Sonnet hybrid reasoning model, their most intelligent model to date, is now available in Amazon Bedrock. Claude 3.7 Sonnet is now available in Amazon Bedrock in the US East (N. For more information and to learn more read the AWS News Blog and Claude in Bedrock product detail page. Learn About AWS What Is AWS? AWS Accessibility AWS Cloud Security AWS Partners Developers on AWS AWS re:Post AWS Support Overview AWS support for Internet Explorer ends on 07/31/2022.\", \"score\": 0.7646988, \"raw_content\": \"Anthropic's Claude 3.7 Sonnet is now available in Amazon Bedrock - AWS\\nSkip to main content\\nClick here to return to Amazon Web Services homepage\\nAbout AWS Contact Us Support Â  English Â  My Account Â \\nSign In\\nCreate an AWS Account\\n\\n\\nClose\\nProfile\\nYour profile helps improve your interactions with select AWS experiences.\\nLogin\\nClose\\nProfile\\nYour profile helps improve your interactions with select AWS experiences.\\nView profile\\nLog out\\n\\nAmazon Q\\nProducts\\nSolutions\\nPricing\\nDocumentation\\nLearn\\nPartner Network\\nAWS Marketplace\\nCustomer Enablement\\nEvents\\nExplore More\\n\\nClose\\n\\nØ¹Ø±Ø¨ÙŠ\\nBahasa Indonesia\\nDeutsch\\nEnglish\\nEspaÃ±ol\\nFranÃ§ais\\nItaliano\\n\\nPortuguÃªs\\n\\n\\nTiáº¿ng Viá»‡t\\n\\nTÃ¼rkÃ§e\\nÎ¡ÑƒÑÑĞºĞ¸Ğ¹\\nà¹„à¸—à¸¢\\næ—¥æœ¬èª\\ní•œêµ­ì–´\\nä¸­æ–‡ (ç®€ä½“)\\nä¸­æ–‡ (ç¹é«”)\\n\\nClose\\n\\nMy Profile\\nSign out of AWS Builder ID\\nAWS Management Console\\nAccount Settings\\nBilling & Cost Management\\nSecurity Credentials\\nAWS Personal Health Dashboard\\n\\nClose\\n\\nSupport Center\\nExpert Help\\nKnowledge Center\\nAWS Support Overview\\nAWS re:Post\\n\\nClick here to return to Amazon Web Services homepage\\n\\n\\nClose\\nProfile\\nYour profile helps improve your interactions with select AWS experiences.\\nLogin\\nClose\\nProfile\\nYour profile helps improve your interactions with select AWS experiences.\\nView profile\\nLog out\\nClose\\nProfile\\nYour profile helps improve your interactions with select AWS experiences.\\nView profile\\nLog out\\nGet Started for Free\\nContact Us\\n\\nProducts\\nSolutions\\nPricing\\nIntroduction to AWS\\nGetting Started\\nDocumentation\\nTraining and Certification\\nDeveloper Center\\nCustomer Success\\nPartner Network\\nAWS Marketplace\\nSupport\\nAWS re:Post\\nLog into Console\\nDownload the Mobile App\\n\\nAnthropic's Claude 3.7 Sonnet is now available in Amazon Bedrock\\nPosted on: Feb 24, 2025\\nAnthropic's Claude 3.7 Sonnet hybrid reasoning model, their most intelligent model to date, is now available in Amazon Bedrock. Claude 3.7 Sonnet represents a significant advancement in AI capabilities, offering both quick responses and extended, step-by-step thinking made visible to the user. This new model includes strong improvements in coding and brings enhanced performance across various tasks, like instruction following, math, and physics.  \\nClaude 3.7 Sonnet introduces a unique approach to AI reasoning by integrating it seamlessly with other capabilities. Unlike traditional models that separate quick responses from those requiring deeper thought, Claude 3.7 Sonnet allows users to toggle between standard and extended thinking modes. In standard mode, it functions as an upgraded version of Claude 3.5 Sonnet. While in extended thinking mode, it employs self-reflection to achieve improved results across a wide range of tasks. Amazon Bedrock users can adjust how long the model thinks, offering a flexible trade-off between speed and answer quality. Additionally, users can control the reasoning budget by specifying a token limit, enabling more precise management of cost.  \\nAnthropic has optimized Claude 3.7 Sonnet for real-world applications that align closely with typical language model use cases, rather than focusing solely on math and computer science competition problems. This approach ensures that the model is well-suited to address the diverse needs of customers across various industries and use cases.  \\nClaude 3.7 Sonnet is now available in Amazon Bedrock in the US East (N. Virginia), US East (Ohio), and US West (Oregon) regions. To get started, visit the Amazon Bedrock console. Integrate it into your applications using the Amazon Bedrock API or SDK. For more information and to learn more read the AWS News Blog and Claude in Bedrock product detail page.\\nSign In to the Console\\nLearn About AWS\\n\\nWhat Is AWS?\\nWhat Is Cloud Computing?\\nAWS Accessibility\\nWhat Is DevOps?\\nWhat Is a Container?\\nWhat Is a Data Lake?\\nWhat is Artificial Intelligence (AI)?\\nWhat is Generative AI?\\nWhat is Machine Learning (ML)?\\nAWS Cloud Security\\nWhat's New\\nBlogs\\nPress Releases\\n\\nResources for AWS\\n\\nGetting Started\\nTraining and Certification\\nAWS Trust Center\\nAWS Solutions Library\\nArchitecture Center\\nProduct and Technical FAQs\\nAnalyst Reports\\nAWS Partners\\n\\nDevelopers on AWS\\n\\nDeveloper Center\\nSDKs & Tools\\n.NET on AWS\\nPython on AWS\\nJava on AWS\\nPHP on AWS\\nJavaScript on AWS\\n\\nHelp\\n\\nContact Us\\nGet Expert Help\\nFile a Support Ticket\\nAWS re:Post\\nKnowledge Center\\nAWS Support Overview\\nLegal\\nAWS Careers\\n\\nCreate an AWS Account\\n\\n\\n\\n\\n\\n\\n\\n\\nAmazon is an Equal Opportunity Employer: Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.\\n\\nLanguage\\nØ¹Ø±Ø¨ÙŠ\\nBahasa Indonesia\\nDeutsch\\nEnglish\\nEspaÃ±ol\\nFranÃ§ais\\nItaliano\\nPortuguÃªs\\nTiáº¿ng Viá»‡t\\nTÃ¼rkÃ§e\\nÎ¡ÑƒÑÑĞºĞ¸Ğ¹\\nà¹„à¸—à¸¢\\næ—¥æœ¬èª\\ní•œêµ­ì–´\\nä¸­æ–‡ (ç®€ä½“)\\n\\nä¸­æ–‡ (ç¹é«”)\\n\\n\\nPrivacy\\n\\n|\\nAccessibility\\n|\\nSite Terms\\n|\\nCookie Preferences\\n|\\nÂ© 2024, Amazon Web Services, Inc. or its affiliates. All rights reserved.\\n\\nEnding Support for Internet Explorer\\nGot it\\nAWS support for Internet Explorer ends on 07/31/2022. Supported browsers are Chrome, Firefox, Edge, and Safari. Learn more Â»\\nGot it\"}, {\"title\": \"Anthropic's Claude 3.7 Sonnet is available on Vertex AI - Google Cloud\", \"url\": \"https://cloud.google.com/blog/products/ai-machine-learning/anthropics-claude-3-7-sonnet-is-available-on-vertex-ai\", \"content\": \"Anthropicâ€™s Claude 3.7 Sonnet is available on Vertex AI | Google Cloud Blog \\\"By making Claude 3.7 Sonnet available through Vertex AI, Google Cloud customers can now apply this transformative technology across their organizations. I love the accuracy of Anthropicâ€™s Claude models and the security and advanced AI tools that Google Cloud provides to utilize these models for our auditing process.â€ â€” Sean Otto, Senior Director of Data Science & Analytics at AES Running Claude on Google Cloudâ€™s Vertex AI not only accelerates development projects, it enables us to hardwire security into code before it ships.â€ â€” Gunjan Patel, Director of Engineering, Office of the CPO at Palo Alto Networks\", \"score\": 0.76300776, \"raw_content\": \"Published Time: 2025-02-25\\nAnthropicâ€™s Claude 3.7 Sonnet is available on Vertex AI | Google Cloud Blog\\nJump to Content\\nCloud\\nBlog\\nContact sales Get started for free\\nCloud\\nBlog\\n\\nSolutions & technology\\nAI & Machine Learning\\nAPI Management\\nApplication Development\\nApplication Modernization\\nChrome Enterprise\\nCompute\\nContainers & Kubernetes\\nData Analytics\\nDatabases\\nDevOps & SRE\\nMaps & Geospatial\\nSecurity\\nSecurity & Identity\\nThreat Intelligence\\n\\n\\nInfrastructure\\nInfrastructure Modernization\\nNetworking\\nProductivity & Collaboration\\nSAP on Google Cloud\\nStorage & Data Transfer\\nSustainability\\n\\n\\nEcosystem\\nIT Leaders\\nIndustries\\nFinancial Services\\nHealthcare & Life Sciences\\nManufacturing\\nMedia & Entertainment\\nPublic Sector\\nRetail\\nSupply Chain\\nTelecommunications\\n\\n\\nPartners\\nStartups & SMB\\nTraining & Certifications\\nInside Google Cloud\\nGoogle Cloud Next & Events\\nGoogle Cloud Consulting\\nGoogle Maps Platform\\nGoogle Workspace\\n\\n\\nDevelopers & Practitioners\\nTransform with Google Cloud\\n\\nContact sales Get started for free\\nAI & Machine Learning\\nAnnouncing Claude 3.7 Sonnet, Anthropicâ€™s first hybrid reasoning model, is available on Vertex AI\\n=====================================================================================================\\nFebruary 24, 2025\\n\\n\\n\\n\\n\\n\\n\\nNenshad Bardoliwalla\\nDirector, Product Management, Vertex AI\\nJoin us at Google Cloud Next\\nApril 9-11 in Las Vegas\\nRegister\\nToday, weâ€™re announcing Claude 3.7 Sonnet, Anthropicâ€™s most intelligent model to date and the first hybrid reasoning model on the market, is available in preview on Vertex AI Model Garden. Claude 3.7 Sonnet can produce quick responses or extended, step-by-step thinking that is made visible to the user. Claude 3.7 Sonnet includes improvements in coding, and is optimized for real-world, practical use cases to reflect customersâ€™ needs.\\n\\\"Claude 3.7 Sonnet represents an exciting breakthrough as the first hybrid reasoning model, combining rapid responses and reasoning in a single model,\\\" said Kate Jensen, Head of Revenue at Anthropic. \\\"By making Claude 3.7 Sonnet available through Vertex AI, Google Cloud customers can now apply this transformative technology across their organizations. Whether developing complex software solutions, delivering customer experiences, or conducting strategic analysis, Claude on Vertex AI helps teams to tackle their most challenging business problems with enterprise-grade reliability.\\\"\\nWeâ€™re also announcing Vertex AI support for Anthropicâ€™s new agentic coding tool, Claude Code. Claude Code lets developers delegate coding tasks to Claude directly from their terminal and is available through Anthropic's limited research preview. For more information on Claude 3.7 Sonnet and Claude Code, including how to access Claude Code, check out Anthropicâ€™s blog here.\\nBuild on a unified AI platform with Vertex AI\\nTo explore the full potential of foundational models like Claude, youâ€™ll need advanced development tools and enterprise-grade reliability to use them in your applications. Thatâ€™s what you get with Vertex AI, which is built on Googleâ€™s AI-optimized infrastructure, stringent security, and learnings from serving 300+ real-world use cases.\\nVertex AI empowers you to take your Claude-powered applications from concept to production on a unified platform. With Vertex AIâ€™s Model-as-a-Service (MaaS) offering, you benefit from simplified procurement, fully managed infrastructure, enterprise-grade security, and advanced developer tools.\\n\\nConfidently deploy agents in production: Power production-grade AI agents with Claude 3.7 Sonnet, using Vertex AIâ€™s full suite of agentic tools and services, including RAG Engine and Agent Engine (coming soon).Â \\nOptimize performance with fully managed infrastructure: Simplify how you deploy and scale Claude 3.7 Sonnet with Vertex AIâ€™s fully managed infrastructure thatâ€™s tailored for AI workloads.\\nAccelerate development with powerful MLOps tools: Explore and evaluate Claude 3.7 Sonnet with fully integrated platform tools like Vertex AI Evaluation for model testing and evaluation and the LangChain integration for custom application building.\\nBuild with enterprise-grade security, compliance, and data governance: Leverage Google Cloud's robust built-in security, privacy, and compliance measures to securely scale your applications. Enterprise controls, such as Vertex AI Model Gardenâ€™s organization policy, provide the right access controls to make sure only approved models can be accessed.\\n\\nAdditional features to make the most of Claude on Vertex AI\\nTo enhance your interaction and deployment of Claude models on Vertex AI, including Claude 3.7 Sonnet, we also offer advanced features designed to reduce latency and costs, increase throughput, and optimize Claude model utilization:\\n\\n\\nCount tokens (generally available): Make more informed decisions about your prompts and usage by determining the number of tokens in a message before sending it to Claude. Learn more on how to use count tokens with Claude models and which models are supported here.\\n\\n\\nCitations (generally available): Verify sources with detailed references to the exact sentences and passages it uses to generate responses, leading to more verifiable, trustworthy outputs. Claude 3.7 Sonnet, upgraded Claude 3.5 Sonnet, and Claude 3.5 Haiku support Citations.\\n\\n\\nBatch predictions (preview): Process large volumes of requests asynchronously for cost savings. Popular applications include analyzing large datasetsâ€”such as customer databasesâ€”for risk assessment or fraud detection, and applications that require periodic updatesâ€”such as generating daily reports. Each batch job is processed in less than 24 hours and costs 50% less than standard Anthropic API calls. Learn more on how to use batch predictions with Claude models and which models are supported here.\\n\\n\\nPrompt caching (preview): Provide Claude with more background knowledge and example outputs to improve response accuracyâ€”all while reducing costs. You can cache all or specific parts of your frequently used inputs, so that subsequent queries can use the cached results. Learn more on how to use prompt caching with Claude models and which models are supported here.\\n\\n\\nWeâ€™re also excited to share that Claude 3.5 Haiku, which is already available on Vertex AI Model Garden, now supports multi-modal image input. Claude 3.5 Haiku is Anthropicâ€™s fastest and most cost-effective model.\\nCustomers are driving business results with Anthropic on Google Cloud\\nAES, a global energy company, uses Claude on Vertex AI to significantly increase the accuracy and speed of the companyâ€™s health and safety audits:\\nâ€œOur auditors previously spent 14 days completing each audit process. Now, with our Claude-powered agents on Vertex AI, the same work is completed in just one hour. I love the accuracy of Anthropicâ€™s Claude models and the security and advanced AI tools that Google Cloud provides to utilize these models for our auditing process.â€ â€” Sean Otto, Senior Director of Data Science & Analytics at AES\\nPalo Alto Networks, a global cybersecurity company, is accelerating software development and security by deploying Anthropicâ€™s Claude models on Vertex AI:\\nâ€œWith Claude running on Vertex AI, we saw a 20% to 30% increase in feature development and code implementation. Running Claude on Google Cloudâ€™s Vertex AI not only accelerates development projects, it enables us to hardwire security into code before it ships.â€ â€” Gunjan Patel, Director of Engineering, Office of the CPO at Palo Alto Networks\\nQuora, the global knowledge-sharing platform, is harnessing Claude's capabilities on Vertex AI to facilitate millions of daily interactions through Quoraâ€™s own AI-powered chat platform, Poe:\\n\\\"We consistently hear from our users about how much they enjoy the intelligence, adaptability, and natural conversational abilities of Anthropic's Claude models. They're relying on these qualities for a wide variety of tasks, from the complex to the creative. By leveraging Claude with Vertex AI's secure and scalable platform, we're able to facilitate millions of daily interactions, ensuring both speed and reliability.\\\" â€” Spencer Chan, Product Lead at Poe by Quora\\nReplit, a platform for software development and deployment, leverages Claude on Vertex AI to power Replit Agent, which empowers people across the world to use natural language prompts to turn their ideas into applications, regardless of coding experience.\\nâ€œOur AI agent is made more powerful through Anthropicâ€™s Claude models running on Vertex AI. This integration allows us to easily connect with other Google Cloud services, like Cloud Run, to work together behind the scenes to help customers turn their ideas into apps.â€ â€” Amjad Masad, Founder and CEO of Replit\\nGet started\\n\\n\\nSelect the Claude 3.7 Sonnet model card in Vertex AI Model Garden. You can also find and easily procure Claude 3.7 Sonnet on Google Cloud Marketplace and take advantage of the ability to draw down on your Google Cloud spend commitments.\\n\\n\\nSelect â€œEnableâ€ and follow the proceeding instructions.\\n\\n\\nExplore our sample notebook and documentation to start building.\\n\\n\\nPosted in\\n\\nAI & Machine Learning\\n\\nRelated articles\\n AI & Machine Learning ### Optimizing image generation pipelines on Google Cloud: A practical guide By Gopala Dhar â€¢ 5-minute read\\n DevOps & SRE ### An SREâ€™s guide to optimizing ML systems with MLOps pipelines By Max Saltonstall â€¢ 5-minute read\\n AI & Machine Learning ### Unlock Inference-as-a-Service with Cloud Run and Vertex AI By Jason (Jay) Smith â€¢ 4-minute read\\n Telecommunications ### Rethinking 5G: The cloud imperative By Eric Parsons â€¢ 4-minute read\\nFooter Links\\nFollow us\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGoogle Cloud\\nGoogle Cloud Products\\nPrivacy\\nTerms\\n\\nCookies management controls\\n\\n\\nHelp\\n*\\n\\n\"}]12\n",
      "2023ë…„ ì›”í˜¸\n",
      "\n",
      "SPRi AI Brief |\n",
      "2023-12ì›”í˜¸\n",
      "EU AI ë²• 3ì í˜‘ìƒ, ê¸°ë°˜ëª¨ë¸ ê·œì œ ê´€ë ¨ ê²¬í•´ì°¨ë¡œ ë‚œí•­\n",
      "KEY Contents\n",
      "n ìœ ëŸ½ì˜íšŒ, EU ì§‘í–‰ìœ„ì›íšŒ, EU ì´ì‚¬íšŒê°€ ì§„í–‰ ì¤‘ì¸ AI ë²• ìµœì¢…í˜‘ìƒì—ì„œ í”„ë‘ìŠ¤, ì´íƒˆë¦¬ì•„,\n",
      "ë…ì¼ì´ ê¸°ë°˜ëª¨ë¸ì— ëŒ€í•œ ê·œì œì— ë°˜ëŒ€í•˜ë©° í˜‘ìƒì´ ë‚œê´€ì— ë´‰ì°©\n",
      "n í”„ë‘ìŠ¤, ì´íƒˆë¦¬ì•„, ë…ì¼ 3ê°œêµ­ì€ ê¸°ë°˜ëª¨ë¸ ê°œë°œê¸°ì—…ì— ëŒ€í•˜ì—¬ ììœ¨ì  í–‰ë™ê°•ë ¹ì„ ë„ì…í•˜ê³ \n",
      "ì¤€ìˆ˜ë¥¼ ì˜ë¬´í™”í•˜ëŠ” ë°©ì•ˆì„ ì œì•ˆ\n",
      "Â£AI ë²• 3ì í˜‘ìƒ, ì´ì‚¬íšŒ ì¼ë¶€ êµ­ê°€ê°€ ê¸°ë°˜ëª¨ë¸ ê·œì œì— ë°˜ëŒ€í•˜ë©° ì°¨ì§ˆ\n",
      "\n",
      "n 3ê°œì˜ ì‘ì—… ìœ í˜• í‰ê°€ ì „ì²´ì—ì„œ ì˜¤í”ˆAIì˜ GPT-4ê°€ ìµœê³ ì˜ ì„±ëŠ¥ì„ ê¸°ë¡í–ˆìœ¼ë©°, GPT-3.5 í„°ë³´ë„\n",
      "GPT-4ì™€ ê±°ì˜ ë™ë“±í•œ ì„±ëŠ¥ì„ ë°œíœ˜\n",
      "âˆ™ ë©”íƒ€ì˜ ë¼ë§ˆ2(Llama-2-70b)ëŠ” RAG ì—†ëŠ” ì§ˆë¬¸ê³¼ ë‹µë³€ ìœ í˜•ì—ì„œ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ ê°€ìš´ë° ê°€ì¥ ìš°ìˆ˜í–ˆê³  ê¸´\n",
      "í˜•ì‹ì˜ í…ìŠ¤íŠ¸ ìƒì„±ì—ì„œë„ GPT-4ì— ì¤€í•˜ëŠ” ì„±ëŠ¥ì„ ê¸°ë¡í–ˆìœ¼ë‚˜, RAG í¬í•¨ ì§ˆë¬¸ê³¼ ë‹µë³€ì—ì„œëŠ” í—ˆê¹…\n",
      "í˜ì´ìŠ¤ì˜ ì œí¼(Zephyr-7b)ê°€ ë¼ë§ˆ2ë¥¼ ëŠ¥ê°€\n",
      "<ê°ˆë¦´ë ˆì˜¤ì˜ LLM í™˜ê° ì§€ìˆ˜(RAG í¬í•¨ ì§ˆë¬¸ê³¼ ë‹µë³€ ê¸°ì¤€)>\n",
      "\n",
      "<ê°ˆë¦´ë ˆì˜¤ì˜ LLM í™˜ê° ì§€ìˆ˜(RAG í¬í•¨ ì§ˆë¬¸ê³¼ ë‹µë³€ ê¸°ì¤€)>\n",
      "â˜ ì¶œì²˜: Galileo, LLM Hallucination Index, 2023.11.15.\n",
      "\n",
      "í”„ë‘ìŠ¤ ì •ë¶€ì™€ëŠ” 1ë…„ í›„ ëŒ€ë©´ ì •ìƒíšŒì˜ë¥¼ ê°œìµœí•  ì˜ˆì •\n",
      "â˜ ì¶œì²˜: Gov.uk, The Bletchley Declaration by Countries Attending the AI Safety Summit, 1-2 November 2023, 2023.11.01.\n",
      "Gov.uk, World leaders, top AI companies set out plan for safety testing of frontier as first global AI Safety Summit\n",
      "concludes, 2023.11.02.\n",
      "\n",
      "Â£ì˜ˆìˆ ê°€ë“¤ì˜ AI ì €ì‘ê¶Œ ì¹¨í•´ ì†Œì†¡, ì €ì‘ê¶Œ ë¯¸ë“±ë¡ê³¼ ì¦ê±°ë¶ˆì¶©ë¶„ìœ¼ë¡œ ê¸°ê°\n",
      "n ë¯¸êµ­ ìº˜ë¦¬í¬ë‹ˆì•„ ë¶ë¶€ì§€ë°©ë²•ì›ì˜ ìœŒë¦¬ì—„ ì˜¤ë¦­(William Orrick) íŒì‚¬ëŠ” 2023ë…„ 10ì›” 30ì¼ ë¯¸ë“œì €ë‹ˆ\n",
      "(Midjourney), ìŠ¤íƒœë¹Œë¦¬í‹°AI(Stability AI), ë””ë¹„ì–¸íŠ¸ì•„íŠ¸(DeviantArt)ì— ì œê¸°ëœ ì €ì‘ê¶Œ ì¹¨í•´ ì†Œì†¡ì„ ê¸°ê°\n",
      "âˆ™ 2023ë…„ 1ì›” ì˜ˆìˆ ê°€ ì‚¬ë¼ ì•¤ë”ìŠ¨(Sarah Anderson), ìº˜ë¦¬ ë§¥ì»¤ë„Œ(Kelly McKernan), ì¹¼ë¼\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Claude 3.7 Sonnetì€ Anthropicì—ì„œ ê°œë°œí•œ AI ëª¨ë¸ë¡œ, ìµœì´ˆì˜ í•˜ì´ë¸Œë¦¬ë“œ ì¶”ë¡  ëª¨ë¸ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ë¹ ë¥¸ ì‘ë‹µê³¼ ê¹Šì´ ìˆëŠ” ë¶„ì„ì  ì‚¬ê³ ë¥¼ í•˜ë‚˜ì˜ ì‹œìŠ¤í…œì—ì„œ ê²°í•©í•˜ì—¬ ì œê³µí•©ë‹ˆë‹¤. Claude 3.7 Sonnetì€ ì´ì „ ëª¨ë¸ì¸ Claude 3.5 Sonnetì— ë¹„í•´ ì†ë„, ì¶”ë¡  ëŠ¥ë ¥ ë° ì‹¤ì œ ì—…ë¬´ ìˆ˜í–‰ ëŠ¥ë ¥ì—ì„œ í° í–¥ìƒì„ ë³´ì—¬ì£¼ë©°, íŠ¹íˆ ì½”ë”© ë° ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ë§ ì‘ì—…ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤.\n",
      "\n",
      "Claude 3.7 Sonnetì˜ ì£¼ìš” íŠ¹ì§•ì€ \"í™•ì¥ëœ ì‚¬ê³ \" ëª¨ë“œë¡œ, ì‚¬ìš©ìê°€ ë¹ ë¥¸ ì‘ë‹µê³¼ ìì„¸í•œ ë¶„ì„ì  ì‚¬ê³  ì¤‘ì—ì„œ ì„ íƒí•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ íŠ¹íˆ ì½”ë”©, ë¬¸ì œ í•´ê²°, ê³ ê° ì§€ì›, ì½˜í…ì¸  ê²€í† , ë²•ë¥  ë¬¸ì„œ ìš”ì•½ ë“± ë‹¤ì–‘í•œ ì‹¤ì œ ì—…ë¬´ì— ìµœì í™”ë˜ì–´ ìˆìœ¼ë©°, Amazon Bedrockê³¼ Google Cloudì˜ Vertex AI í”Œë«í¼ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "Claude 3.7 Sonnetì€ APIë¥¼ í†µí•´ ì‚¬ìš©ìê°€ í•„ìš”í•œ ë§Œí¼ì˜ ì¶”ë¡  ê³¼ì •ì„ ì„¤ì •í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ì œê³µí•˜ì—¬, ì†ë„ì™€ ì‘ë‹µì˜ í’ˆì§ˆì„ ì‚¬ìš©ìê°€ ì¡°ì ˆí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ íŠ¹íˆ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ, ê³ ê° ì§€ì› ìë™í™”, ë²•ë¥  ìš”ì•½ ë“±ì˜ ë¶„ì•¼ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ë©°, ë‹¤ì–‘í•œ ì‚°ì—…ê³¼ ì‚¬ìš© ì‚¬ë¡€ì— ì í•©í•œ ëª¨ë¸ë¡œ í‰ê°€ë°›ê³  ìˆìŠµë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "# ê·¸ë˜í”„ ìŠ¤íŠ¸ë¦¼\n",
    "stream_graph(\n",
    "    simple_react_agent,\n",
    "    {\"messages\": [(\"human\", \"claude 3.7 sonnet ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•´ì¤˜\")]},\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì¶”ì **: https://smith.langchain.com/public/e28f6b6c-463c-4211-8a75-3c88dfdcc41c/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•œ ë‹¨ê¸° ë©”ëª¨ë¦¬: `checkpointer`\n",
    "\n",
    "ë‹¨ê¸° ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì´ ì—†ëŠ” ê·¸ë˜í”„ëŠ” ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ì§€ ëª»í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì¦‰, ë©€í‹°í„´ ëŒ€í™”ë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ë§ì´ê¸°ë„ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ë‹¤ìŒê³¼ ê°™ì´ ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ì§€ ëª»í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì•ˆë…•í•˜ì„¸ìš”, í…Œë””ë‹˜! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?"
     ]
    }
   ],
   "source": [
    "# ê·¸ë˜í”„ ìŠ¤íŠ¸ë¦¼\n",
    "stream_graph(\n",
    "    simple_react_agent,\n",
    "    {\"messages\": [(\"human\", \"ì•ˆë…•, ë°˜ê°€ì›Œ! ë‚´ ì´ë¦„ì€ í…Œë””ì•¼!\")]},\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì£„ì†¡í•˜ì§€ë§Œ, ì €ëŠ” ì‚¬ìš©ìë‹˜ì˜ ì´ë¦„ì„ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "# ê·¸ë˜í”„ ìŠ¤íŠ¸ë¦¼\n",
    "stream_graph(simple_react_agent, {\"messages\": [(\"human\", \"ë‚´ ì´ë¦„ì´ ë­ë¼ê³ ?\")]}, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `MemorySaver`\n",
    "\n",
    "LangGraph ëŠ” `Checkpointer` ë¥¼ ì‚¬ìš©í•´ ê° ë‹¨ê³„ê°€ ëë‚œ í›„ ê·¸ë˜í”„ ìƒíƒœë¥¼ ìë™ìœ¼ë¡œ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ ë‚´ì¥ëœ ì§€ì†ì„± ê³„ì¸µì€ ë©”ëª¨ë¦¬ë¥¼ ì œê³µí•˜ì—¬ LangGraphê°€ ë§ˆì§€ë§‰ ìƒíƒœ ì—…ë°ì´íŠ¸ì—ì„œ ì„ íƒí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. \n",
    "\n",
    "ê°€ì¥ ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ ì²´í¬í¬ì¸í„° ì¤‘ í•˜ë‚˜ëŠ” ê·¸ë˜í”„ ìƒíƒœë¥¼ ìœ„í•œ ì¸ë©”ëª¨ë¦¬ í‚¤-ê°’ ì €ì¥ì†Œì¸ `MemorySaver`ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì„¤ì •\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# ReAct Agent ìƒì„±(checkpointer ì„¤ì •)\n",
    "simple_react_agent = create_react_agent(\n",
    "    model,\n",
    "    tools,\n",
    "    checkpointer=memory,\n",
    "    prompt=\"You are a helpful assistant. Answer in Korean.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì•ˆë…•í•˜ì„¸ìš”, í…Œë””! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?"
     ]
    }
   ],
   "source": [
    "# Config ì„¤ì •\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "# ê·¸ë˜í”„ ìŠ¤íŠ¸ë¦¼\n",
    "stream_graph(\n",
    "    simple_react_agent,\n",
    "    {\"messages\": [(\"human\", \"ì•ˆë…•, ë°˜ê°€ì›Œ! ë‚´ ì´ë¦„ì€ í…Œë””ì•¼!\")]},\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë²ˆì—ëŠ” ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì˜ ê¸°ì–µí•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ë‹¹ì‹ ì˜ ì´ë¦„ì€ í…Œë””ì…ë‹ˆë‹¤!"
     ]
    }
   ],
   "source": [
    "# ê·¸ë˜í”„ ìŠ¤íŠ¸ë¦¼\n",
    "stream_graph(simple_react_agent, {\"messages\": [(\"human\", \"ë‚´ ì´ë¦„ì´ ë­ë¼ê³ ?\")]}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36magent\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì£„ì†¡í•˜ì§€ë§Œ, ë‹¹ì‹ ì˜ ì´ë¦„ì€ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëŒ€í™” ì¤‘ì— ì´ë¦„ì„ ì œê³µí•´ ì£¼ì‹œë©´ ê¸°ì–µí•˜ê² ìŠµë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "# llm ë©”ëª¨ë¦¬ëŠ” thread_id ë³„ë¡œ ì €ì¥í•˜ê¸°ë•Œë¬¸ì— thread_id ë¥¼ abc123 ->abc124ë¡œ ë°”ê¾¸ë©´ ê¸°ì–µëª»í•¨\n",
    "config = {\"configurable\": {\"thread_id\": \"abc124\"}}\n",
    "stream_graph(simple_react_agent, {\"messages\": [(\"human\", \"ë‚´ ì´ë¦„ì´ ë­ë¼ê³ ?\")]}, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì¶”ì **: https://smith.langchain.com/public/16105167-e6db-4e26-add8-96cbf53191f7/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. LangGraph ì›Œí¬í”Œë¡œìš° êµ¬í˜„\n",
    "\n",
    "ì´ì „ì˜ `create_react_agent` ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ë¥¼ êµ¬í˜„í•´ ë³´ì•˜ìŠµë‹ˆë‹¤.\n",
    "\n",
    "í•˜ì§€ë§Œ, ì´ì „ì˜ ì—ì´ì „íŠ¸ëŠ” ë‹¨ì¼ ì—ì´ì „íŠ¸ í˜•íƒœì´ê¸° ë•Œë¬¸ì— ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬í˜„í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œëŠ” ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps**\n",
    "1. State ì •ì˜(TypedDict í˜•ì‹ìœ¼ë¡œ ì •ì˜)\n",
    "2. ë…¸ë“œ ì •ì˜(í•¨ìˆ˜ë¡œ êµ¬í˜„)\n",
    "3. ê·¸ë˜í”„ ìƒì„±(StateGraph í´ë˜ìŠ¤ ì‚¬ìš©)\n",
    "4. ì»´íŒŒì¼(checkpointer ì„¤ì •)\n",
    "5. ì‹¤í–‰\n",
    "\n",
    "### State ì •ì˜\n",
    "\n",
    "`State`: Graph ì˜ ë…¸ë“œì™€ ë…¸ë“œ ê°„ ê³µìœ í•˜ëŠ” ìƒíƒœë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì¼ë°˜ì ìœ¼ë¡œ `TypedDict` í˜•ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# GraphState ìƒíƒœ ì •ì˜\n",
    "class GraphState(TypedDict):\n",
    "    question: Annotated[str, \"User's Question\"]  # ì§ˆë¬¸\n",
    "    documents: Annotated[str, \"Retrieved Documents\"]  # ë¬¸ì„œì˜ ê²€ìƒ‰ ê²°ê³¼\n",
    "    answer: Annotated[str, \"LLM generated answer\"]  # ë‹µë³€\n",
    "    messages: Annotated[list, add_messages]  # ë©”ì‹œì§€(ëˆ„ì ë˜ëŠ” list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë…¸ë“œ(Node) ì •ì˜\n",
    "\n",
    "- `Nodes`: ê° ë‹¨ê³„ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë…¸ë“œì…ë‹ˆë‹¤. ë³´í†µì€ Python í•¨ìˆ˜ë¡œ êµ¬í˜„í•©ë‹ˆë‹¤. ì…ë ¥ê³¼ ì¶œë ¥ì´ ìƒíƒœ(State) ê°’ì…ë‹ˆë‹¤.\n",
    "  \n",
    "**ì°¸ê³ **  \n",
    "\n",
    "- `State`ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì •ì˜ëœ ë¡œì§ì„ ìˆ˜í–‰í•œ í›„ ì—…ë°ì´íŠ¸ëœ `State`ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.utils import format_docs\n",
    "\n",
    "\n",
    "# ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ\n",
    "def retrieve_document(state: GraphState) -> GraphState:\n",
    "    # ì§ˆë¬¸ì„ ìƒíƒœì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "    latest_question = state[\"question\"]\n",
    "\n",
    "    # ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•˜ì—¬ ê´€ë ¨ì„± ìˆëŠ” ë¬¸ì„œë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n",
    "    retrieved_docs = pdf_retriever.invoke(latest_question)\n",
    "\n",
    "    # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í˜•ì‹í™”í•©ë‹ˆë‹¤.(í”„ë¡¬í”„íŠ¸ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì£¼ê¸° ìœ„í•¨)\n",
    "    retrieved_docs = format_docs(retrieved_docs)\n",
    "\n",
    "    # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ context í‚¤ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    return {\"documents\": retrieved_docs}\n",
    "\n",
    "\n",
    "# ë‹µë³€ ìƒì„± ë…¸ë“œ\n",
    "def llm_answer(state: GraphState) -> GraphState:\n",
    "    # ì§ˆë¬¸ì„ ìƒíƒœì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "    latest_question = state[\"question\"]\n",
    "\n",
    "    # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ìƒíƒœì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # ì²´ì¸ì„ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    response = pdf_chain.invoke({\"question\": latest_question, \"context\": documents})\n",
    "    # ìƒì„±ëœ ë‹µë³€, (ìœ ì €ì˜ ì§ˆë¬¸, ë‹µë³€) ë©”ì‹œì§€ë¥¼ ìƒíƒœì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    return {\n",
    "        \"answer\": response,\n",
    "        \"messages\": [(\"user\", latest_question), (\"assistant\", response)],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê·¸ë˜í”„ ìƒì„±\n",
    "\n",
    "- `StateGraph`: `State` ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ `Node` ë¥¼ ì‹¤í–‰í•˜ê³  `State` ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ê·¸ë˜í”„ ìƒì„± í´ë˜ìŠ¤.\n",
    "- `Edges`: í˜„ì¬ `State`ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒì— ì‹¤í–‰í•  `Node`ë¥¼ ê²°ì •.\n",
    "- `set_entry_point`: ê·¸ë˜í”„ ì§„ì…ì  ì„¤ì •.\n",
    "- `compile`: ê·¸ë˜í”„ ì»´íŒŒì¼."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„±\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# ë…¸ë“œ ì •ì˜\n",
    "workflow.add_node(\"retrieve\", retrieve_document)\n",
    "workflow.add_node(\"llm_answer\", llm_answer)\n",
    "\n",
    "# ì—£ì§€ ì •ì˜\n",
    "workflow.add_edge(\"retrieve\", \"llm_answer\")  # ê²€ìƒ‰ -> ë‹µë³€\n",
    "workflow.add_edge(\"llm_answer\", END)  # ë‹µë³€ -> ì¢…ë£Œ\n",
    "\n",
    "# ê·¸ë˜í”„ ì§„ì…ì (entry_point) ì„¤ì •\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "\n",
    "# ì²´í¬í¬ì¸í„° ì„¤ì •\n",
    "memory = MemorySaver()\n",
    "\n",
    "# ì»´íŒŒì¼\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì»´íŒŒì¼ì´ ì™„ë£Œëœ ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•˜ê³  ì‹œê°í™” í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ë˜í”„ ì‹¤í–‰\n",
    "\n",
    "- `config` íŒŒë¼ë¯¸í„°ëŠ” ê·¸ë˜í”„ ì‹¤í–‰ ì‹œ í•„ìš”í•œ ì„¤ì • ì •ë³´ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "- `recursion_limit`: ê·¸ë˜í”„ ì‹¤í–‰ ì‹œ ì¬ê·€ ìµœëŒ€ íšŸìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "- `inputs`: ê·¸ë˜í”„ ì‹¤í–‰ ì‹œ í•„ìš”í•œ ì…ë ¥ ì •ë³´ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì°¸ê³ **\n",
    "\n",
    "- ë©”ì‹œì§€ ì¶œë ¥ ìŠ¤íŠ¸ë¦¬ë°ì€ [LangGraph ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì˜ ëª¨ë“  ê²ƒ](https://wikidocs.net/265770) ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ì•„ë˜ì˜ `stream_graph` í•¨ìˆ˜ëŠ” íŠ¹ì • ë…¸ë“œë§Œ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì†ì‰½ê²Œ íŠ¹ì • ë…¸ë“œì˜ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mllm_answer\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "êµ¬ê¸€ì€ ì•¤ìŠ¤ë¡œí”½ì— ìµœëŒ€ 20ì–µ ë‹¬ëŸ¬ë¥¼ íˆ¬ìí•˜ê¸°ë¡œ í•©ì˜í•˜ì˜€ìœ¼ë©°, ì´ ì¤‘ 5ì–µ ë‹¬ëŸ¬ë¥¼ ìš°ì„  íˆ¬ìí–ˆìŠµë‹ˆë‹¤. ì•„ë§ˆì¡´ì€ ì•¤ìŠ¤ë¡œí”½ì— ìµœëŒ€ 40ì–µ ë‹¬ëŸ¬ì˜ íˆ¬ì ê³„íšì„ ë°œí‘œí–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "**Source**\n",
      "- data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf (page 14)"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.messages import stream_graph, random_uuid\n",
    "\n",
    "# config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)\n",
    "config = {\"configurable\": {\"resursion_limit\": 10, \"thread_id\": random_uuid()}}\n",
    "\n",
    "# ì§ˆë¬¸ ì…ë ¥\n",
    "inputs = {\"question\": \"ì•¤ìŠ¤ë¡œí”½ì— íˆ¬ìí•œ ê¸°ì—…ê³¼ íˆ¬ìê¸ˆì•¡ì„ ì•Œë ¤ì£¼ì„¸ìš”.\"}\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "stream_graph(app, inputs, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¶”ì : https://smith.langchain.com/public/1aa445e0-672e-4f15-9253-1c8efcdb1355/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4.Routing\n",
    "\n",
    "LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë¼ìš°íŒ…ì€ ì…ë ¥ ì¿¼ë¦¬ë‚˜ ìƒíƒœì— ë”°ë¼ ì ì ˆí•œ ì²˜ë¦¬ ê²½ë¡œë‚˜ êµ¬ì„± ìš”ì†Œë¡œ ìš”ì²­ì„ ì „ë‹¬í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤. \n",
    "\n",
    "LangChain/LangGraphì—ì„œ ë¼ìš°íŒ…ì€ íŠ¹ì • ì‘ì—…ì— ê°€ì¥ ì í•©í•œ ëª¨ë¸ì´ë‚˜ ë„êµ¬ë¥¼ ì„ íƒí•˜ê³ , ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ê´€ë¦¬í•˜ë©°, ë¹„ìš©ê³¼ ì„±ëŠ¥ ê· í˜•ì„ ìµœì í™”í•˜ëŠ” ë° í•„ìˆ˜ì ì…ë‹ˆë‹¤. \n",
    "\n",
    "**Agent**\n",
    "- ë„êµ¬ ì„ íƒì„ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë¼ìš°íŒ…\n",
    "- ë”°ë¼ì„œ, ë„ìš°ì— ëŒ€í•œ description ì´ ìƒì„¸í•˜ê²Œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "**LLM.with_structured_output**\n",
    "- Function Calling ì„ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë¼ìš°íŒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë°ì´í„° ì†ŒìŠ¤ë¡œ ë¼ìš°íŒ…í•˜ëŠ” ë°ì´í„° ëª¨ë¸\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    # ë°ì´í„° ì†ŒìŠ¤ ì„ íƒì„ ìœ„í•œ ë¦¬í„°ëŸ´ íƒ€ì… í•„ë“œ \n",
    "    ## ì¤‘ìš”! ëŒ€ì‹  datasourceë¥¼ ì„¤ì •í•˜ë©´ ë°˜ë“œì‹œ ë‘ ì—ì´ì „íŠ¸ë¥¼ ìˆ˜í–‰. \n",
    "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route it to `web_search` or a `vectorstore`.\",\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM ì„¤ì •\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# llm êµ¬ì¡°í™”ëœ ì¶œë ¥ ì„¤ì •\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ì„¤ì •  (ìƒí™©ì— ë”°ë¼ ì–´ë–¤ ê±¸ ìˆ˜í–‰í•˜ë„ë¡í•˜ëŠ”ì§€ í”„ë¡¬í”„íŠ¸)\n",
    "## !ë„êµ¬ì˜ í”„ë¡¬í”„íŠ¸ ë‹¤ë¥¸ì  \n",
    "### ë„êµ¬ í”„ë¡¬í”„íŠ¸ : ìê¸° ì¨ë‹¬ë¼ê³  ì–´í•„í•˜ëŠ” í”„ë¡¬í”„íŠ¸\n",
    "### ì•„ë˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ : ì „ì²´ ì•„í‚¤í…ì³ ê¸°ë°˜ìœ¼ë¡œ ì¡°ìœ¨í•˜ëŠ” í”„ë¡¬í”„íŠ¸ (ë„êµ¬ë¥¼ ì ì ˆíˆ ê³ ë¦„)\n",
    "system = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "The vectorstore contains documents related to AI Brief Report(SPRI) including Samsung Gause, Anthropic, etc.\n",
    "Use the vectorstore for questions on AI related topics. Otherwise, use `web_search`.\"\"\"\n",
    "\n",
    "# Routing ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ê³¼ êµ¬ì¡°í™”ëœ LLM ë¼ìš°í„°ë¥¼ ê²°í•©í•˜ì—¬ ì§ˆë¬¸ ë¼ìš°í„° ìƒì„±\n",
    "question_router = route_prompt | structured_llm_router"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•œ ë’¤ í˜¸ì¶œ ê²°ê³¼ì˜ ì°¨ì´ë¥¼ ë¹„êµí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(datasource='vectorstore')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì¿¼ë¦¬ ì‹¤í–‰\n",
    "question_router.invoke(\"ì‚¼ì„±ì „ìê°€ ë§Œë“  ìƒì„±í˜• AI ì´ë¦„ì„ ì°¾ì•„ì¤˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¿¼ë¦¬ ì‹¤í–‰\n",
    "question_router.invoke(\"LangCon2025 ì´ë²¤íŠ¸ì˜ ë‚ ì§œì™€ ì¥ì†ŒëŠ”?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ìŒì€ `retrieve`, `generate`, `web_search` ë…¸ë“œë¥¼ êµ¬í˜„í•œ ì½”ë“œì…ë‹ˆë‹¤.\n",
    "\n",
    "- `retrieve`: ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ\n",
    "- `generate`: ë‹µë³€ ìƒì„± ë…¸ë“œ\n",
    "- `web_search`: ì›¹ ê²€ìƒ‰ ë…¸ë“œ\n",
    "\n",
    "ì´ ë…¸ë“œë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ\n",
    "def retrieve(state):\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # ë¬¸ì„œ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "    documents = pdf_retriever.invoke(question)\n",
    "\n",
    "    # ê²€ìƒ‰ëœ ë¬¸ì„œ ë°˜í™˜\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "\n",
    "# ë‹µë³€ ìƒì„± ë…¸ë“œ\n",
    "def generate(state):\n",
    "    # ì§ˆë¬¸ê³¼ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG ë‹µë³€ ìƒì„±\n",
    "    generation = pdf_chain.invoke({\"context\": documents, \"question\": question})\n",
    "\n",
    "    # ìƒì„±ëœ ë‹µë³€ ë°˜í™˜\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "\n",
    "# ì›¹ ê²€ìƒ‰ ë…¸ë“œ\n",
    "def web_search(state):\n",
    "    # print(\"==== [WEB SEARCH] ====\")\n",
    "    # ì§ˆë¬¸ê³¼ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "    web_results = web_search_tool.invoke({\"query\": question})\n",
    "\n",
    "    # ê²€ìƒ‰ëœ ë¬¸ì„œ ë°˜í™˜\n",
    "    web_results_docs = [\n",
    "        Document(\n",
    "            page_content=web_result[\"content\"],\n",
    "            metadata={\"source\": web_result[\"url\"]},\n",
    "        )\n",
    "        for web_result in web_results\n",
    "    ]\n",
    "    return {\"documents\": web_results_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì§ˆë¬¸ ë¼ìš°íŒ… ë…¸ë“œì˜ êµ¬í˜„ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ `question_router` ë¥¼ í˜¸ì¶œí•˜ì—¬ ì ì ˆí•œ ë°ì´í„° ì†ŒìŠ¤ë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `web_search`: ì›¹ ê²€ìƒ‰\n",
    "- `vectorstore`: ë²¡í„° ìŠ¤í† ì–´\n",
    "\n",
    "ë¼ìš°íŒ… ê²°ê³¼ì— ë”°ë¼ ì ì ˆí•œ ë…¸ë“œë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§ˆë¬¸ ë¼ìš°íŒ… ë…¸ë“œ\n",
    "def route_question(state):\n",
    "    print(\"==== [ROUTE QUESTION] ====\")\n",
    "    # ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°\n",
    "    question = state[\"question\"]\n",
    "    # ì§ˆë¬¸ ë¼ìš°íŒ…\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    # ì§ˆë¬¸ ë¼ìš°íŒ… ê²°ê³¼ì— ë”°ë¥¸ ë…¸ë“œ ë¼ìš°íŒ…\n",
    "    if source.datasource == \"web_search\":\n",
    "        print(\"\\n==== [GO TO WEB SEARCH] ====\")\n",
    "        return \"need to search web\"\n",
    "    elif source.datasource == \"vectorstore\":\n",
    "        print(\"\\n==== [GO TO VECTORSTORE] ====\")\n",
    "        return \"search on DB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê·¸ë˜í”„ ìƒì„±\n",
    "\n",
    "ì´ ë‹¨ê³„ì—ì„œëŠ” `web_search`, `retrieve`, `generate` ë…¸ë“œë¥¼ ìƒì„±í•˜ê³ , ì´ë“¤ì„ ì—°ê²°í•˜ëŠ” ì¡°ê±´ë¶€ ì—£ì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `web_search`: ì›¹ ê²€ìƒ‰ ë…¸ë“œ\n",
    "- `retrieve`: ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ\n",
    "- `generate`: ë‹µë³€ ìƒì„± ë…¸ë“œ\n",
    "\n",
    "ì¡°ê±´ë¶€ ì—£ì§€: ì§ˆë¬¸ ë¼ìš°íŒ… ë…¸ë“œì—ì„œ ë°˜í™˜ëœ ê²°ê³¼ì— ë”°ë¼ ì ì ˆí•œ ë…¸ë“œë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `need to search web`: ì›¹ ê²€ìƒ‰ ë…¸ë“œë¡œ ë¼ìš°íŒ…\n",
    "- `search on DB`: ë²¡í„° ìŠ¤í† ì–´ ë…¸ë“œë¡œ ë¼ìš°íŒ…\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒíƒœ ì´ˆê¸°í™”\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# ë…¸ë“œ ì •ì˜\n",
    "workflow.add_node(\"web_search\", web_search)  # ì›¹ ê²€ìƒ‰\n",
    "workflow.add_node(\"retrieve\", retrieve)  # ë¬¸ì„œ ê²€ìƒ‰\n",
    "workflow.add_node(\"generate\", generate)  # ë‹µë³€ ìƒì„±\n",
    "\n",
    "# ê·¸ë˜í”„ ë¹Œë“œ\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    route_question,\n",
    "    {\n",
    "        \"need to search web\": \"web_search\",  # ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ë¼ìš°íŒ…\n",
    "        \"search on DB\": \"retrieve\",  # ë²¡í„°ìŠ¤í† ì–´ë¡œ ë¼ìš°íŒ…\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"retrieve\", \"generate\")  # ë¬¸ì„œ ê²€ìƒ‰ í›„ ë‹µë³€ ìƒì„±\n",
    "workflow.add_edge(\"web_search\", \"generate\")  # ì›¹ ê²€ìƒ‰ í›„ ë‹µë³€ ìƒì„±\n",
    "workflow.add_edge(\"generate\", END)  # ë‹µë³€ ìƒì„± í›„ ì¢…ë£Œ\n",
    "\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "app = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ë˜í”„ë¥¼ ì‹œê°í™” í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"resursion_limit\": 10, \"thread_id\": \"123\"}}\n",
    "\n",
    "stream_graph(\n",
    "    app, {\"question\": \"ì•¤ìŠ¤ë¡œí”½ì— íˆ¬ìí•œ ê¸°ì—…ê³¼ íˆ¬ìê¸ˆì•¡ì„ ì•Œë ¤ì£¼ì„¸ìš”.\"}, config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"resursion_limit\": 10, \"thread_id\": \"123\"}}\n",
    "\n",
    "stream_graph(\n",
    "    app,\n",
    "    {\"question\": \"Claude 3.7 sonnet ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•´ì¤˜. í•œê¸€ë¡œ ë‹µë³€í•´ì¤˜\"},\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5.Fan-out / Fan-in\n",
    "\n",
    "LangGraphì—ì„œ Fan-out/Fan-inì€ ë³µì¡í•œ LLM ì›Œí¬í”Œë¡œìš° ê´€ë¦¬ë¥¼ ìœ„í•œ ì¤‘ìš”í•œ íŒ¨í„´ì…ë‹ˆë‹¤.\n",
    "\n",
    "Fan-outì€ ë‹¨ì¼ ì…ë ¥ì„ ì—¬ëŸ¬ ë³‘ë ¬ ì‘ì—…ìœ¼ë¡œ ë¶„ë°°í•˜ëŠ” íŒ¨í„´ìœ¼ë¡œ, í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ë‚˜ ì¿¼ë¦¬ë¥¼ ì—¬ëŸ¬ LLM, ë„êµ¬, ë˜ëŠ” ì²˜ë¦¬ ë‹¨ê³„ë¡œ ë™ì‹œì— ì „ì†¡í•˜ì—¬ ë‹¤ì–‘í•œ ê´€ì ì´ë‚˜ ì ‘ê·¼ ë°©ì‹ì„ ì–»ì„ ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ì´ëŠ” ë³µì¡í•œ ë¬¸ì œë¥¼ ë” ì‘ê³  ì „ë¬¸í™”ëœ í•˜ìœ„ ì‘ì—…ìœ¼ë¡œ ë¶„í• í•˜ê±°ë‚˜ ë™ì¼í•œ ì‘ì—…ì— ëŒ€í•´ ì—¬ëŸ¬ ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ë¹„êµí•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "Fan-inì€ Fan-outì˜ ì—­ê³¼ì •ìœ¼ë¡œ, ì—¬ëŸ¬ ë³‘ë ¬ ì‘ì—…ì˜ ê²°ê³¼ë¥¼ ë‹¨ì¼ ì¶œë ¥ì´ë‚˜ ë‹¤ìŒ ë‹¨ê³„ë¡œ í†µí•©í•©ë‹ˆë‹¤. ì´ëŠ” ë‹¤ì–‘í•œ ëª¨ë¸ì´ë‚˜ ë„êµ¬ì—ì„œ ìƒì„±ëœ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ë” ì™„ì „í•˜ê³  ì •í™•í•œ ìµœì¢… ì‘ë‹µì„ ë§Œë“¤ê±°ë‚˜ ì—¬ëŸ¬ ì—ì´ì „íŠ¸ì˜ ì‘ì—…ì„ ì¡°ì •í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Any\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# ìƒíƒœ ì •ì˜(add_messages ë¦¬ë“€ì„œ ì‚¬ìš©)\n",
    "class State(TypedDict):\n",
    "    aggregate: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "# ë…¸ë“œ ê°’ ë°˜í™˜ í´ë˜ìŠ¤\n",
    "class ReturnNodeValue:\n",
    "    # ì´ˆê¸°í™”\n",
    "    def __init__(self, node_secret: str):\n",
    "        self._value = node_secret\n",
    "\n",
    "    # í˜¸ì¶œì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸\n",
    "    def __call__(self, state: State) -> Any:\n",
    "        print(f\"Adding {self._value} to {state['aggregate']}\")\n",
    "        return {\"aggregate\": [self._value]}\n",
    "\n",
    "\n",
    "# ìƒíƒœ ê·¸ë˜í”„ ì´ˆê¸°í™”\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# ë…¸ë“œ Aë¶€í„° Dê¹Œì§€ ìƒì„± ë° ê°’ í• ë‹¹\n",
    "builder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n",
    "\n",
    "# ë…¸ë“œ ì—°ê²°\n",
    "builder.add_edge(\"a\", \"b\")\n",
    "builder.add_edge(\"a\", \"c\")\n",
    "builder.add_edge(\"b\", \"d\")\n",
    "builder.add_edge(\"c\", \"d\")\n",
    "builder.add_edge(\"d\", END)\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ë˜í”„ë¥¼ ì‹œê°í™” í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ë˜í”„ë¥¼ ì‹œê°í™” í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "result = graph.invoke({\"aggregate\": []}, {\"configurable\": {\"thread_id\": \"foo\"}})\n",
    "print(\"===\" * 30)\n",
    "print(result[\"aggregate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì¼ë¶€ë§Œ Fan-out í•˜ëŠ” ë°©ë²•\n",
    "\n",
    "(Fan-out ì˜ ìˆœì„œ ì¡°ì •)\n",
    "\n",
    "ì¡°ê±´ë¶€ ì—£ì§€ë¥¼ ë‘ì–´ ì¼ë¶€ë§Œ Fan-out í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "\n",
    "# ìƒíƒœ ì •ì˜(add_messages ë¦¬ë“€ì„œ ì‚¬ìš©)\n",
    "class State(TypedDict):\n",
    "    aggregate: Annotated[list, add_messages]\n",
    "    which: str\n",
    "\n",
    "\n",
    "# ë…¸ë“œë³„ ê³ ìœ  ê°’ì„ ë°˜í™˜í•˜ëŠ” í´ë˜ìŠ¤\n",
    "class ReturnNodeValue:\n",
    "    def __init__(self, node_secret: str):\n",
    "        self._value = node_secret\n",
    "\n",
    "    def __call__(self, state: State) -> Any:\n",
    "        print(f\"Adding {self._value} to {state['aggregate']}\")\n",
    "        return {\"aggregate\": [self._value]}\n",
    "\n",
    "\n",
    "# ìƒíƒœ ê·¸ë˜í”„ ì´ˆê¸°í™”\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n",
    "builder.add_node(\"e\", ReturnNodeValue(\"I'm E\"))\n",
    "\n",
    "\n",
    "# ìƒíƒœì˜ 'which' ê°’ì— ë”°ë¥¸ ì¡°ê±´ë¶€ ë¼ìš°íŒ… ê²½ë¡œ ê²°ì • í•¨ìˆ˜\n",
    "def route_bc_or_cd(state: State) -> Sequence[str]:\n",
    "    if state[\"which\"] == \"cd\":\n",
    "        return [\"c\", \"d\"]\n",
    "    elif state[\"which\"] == \"bc\":\n",
    "        return [\"b\", \"c\"]\n",
    "    else:\n",
    "        return [\"b\", \"c\", \"d\"]\n",
    "\n",
    "\n",
    "# ì „ì²´ ë³‘ë ¬ ì²˜ë¦¬í•  ë…¸ë“œ ëª©ë¡\n",
    "intermediates = [\"b\", \"c\", \"d\"]\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"a\",\n",
    "    route_bc_or_cd,\n",
    "    intermediates,\n",
    ")\n",
    "for node in intermediates:\n",
    "    builder.add_edge(node, \"e\")\n",
    "\n",
    "\n",
    "# ìµœì¢… ë…¸ë“œ ì—°ê²° ë° ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "builder.add_edge(\"e\", END)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ë˜í”„ë¥¼ ì‹œê°í™” í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ì‹¤í–‰(which: bc ë¡œ ì§€ì •)\n",
    "result = graph.invoke({\"aggregate\": [], \"which\": \"bc\"})\n",
    "print(\"===\" * 30)\n",
    "print(result[\"aggregate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ì‹¤í–‰(which: cd ë¡œ ì§€ì •)\n",
    "result = graph.invoke({\"aggregate\": [], \"which\": \"cd\"})\n",
    "print(\"===\" * 30)\n",
    "print(result[\"aggregate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6.ëŒ€í™” ê¸°ë¡ ìš”ì•½ì„ ì¶”ê°€í•˜ëŠ” ë°©ë²•\n",
    "\n",
    "ëŒ€í™” ê¸°ë¡ì„ ìœ ì§€í•˜ëŠ” ê²ƒì€ **ì§€ì†ì„±**ì˜ ê°€ì¥ ì¼ë°˜ì ì¸ ì‚¬ìš© ì‚¬ë¡€ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì´ëŠ” ëŒ€í™”ë¥¼ ì§€ì†í•˜ê¸° ì‰½ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "í•˜ì§€ë§Œ ëŒ€í™”ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ëŒ€í™” ê¸°ë¡ì´ ëˆ„ì ë˜ì–´ `context window`ë¥¼ ë” ë§ì´ ì°¨ì§€í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ëŠ” `LLM` í˜¸ì¶œì´ ë” ë¹„ì‹¸ê³  ê¸¸ì–´ì§€ë©°, ì ì¬ì ìœ¼ë¡œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆì–´ ë°”ëŒì§í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ í•œ ê°€ì§€ ë°©ë²•ì€ í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™” ìš”ì•½ë³¸ì„ ìƒì„±í•˜ê³ , ì´ë¥¼ ìµœê·¼ `N` ê°œì˜ ë©”ì‹œì§€ì™€ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. \n",
    "\n",
    "ì´ ê°€ì´ë“œì—ì„œëŠ” ì´ë¥¼ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì˜ ì˜ˆì‹œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "- ëŒ€í™”ê°€ ë„ˆë¬´ ê¸´ì§€ í™•ì¸ (ë©”ì‹œì§€ ìˆ˜ë‚˜ ë©”ì‹œì§€ ê¸¸ì´ë¡œ í™•ì¸ ê°€ëŠ¥)\n",
    "- ë„ˆë¬´ ê¸¸ë‹¤ë©´ ìš”ì•½ë³¸ ìƒì„± (ì´ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í•„ìš”)\n",
    "- ë§ˆì§€ë§‰ `N` ê°œì˜ ë©”ì‹œì§€ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì‚­ì œ\n",
    "\n",
    "ì´ ê³¼ì •ì—ì„œ ì¤‘ìš”í•œ ë¶€ë¶„ì€ ì˜¤ë˜ëœ ë©”ì‹œì§€ë¥¼ ì‚­ì œ(`DeleteMessage`) í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, RemoveMessage, HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# ëŒ€í™” ë° ìš”ì•½ì„ ìœ„í•œ ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "# ë©”ì‹œì§€ ìƒíƒœì™€ ìš”ì•½ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” ìƒíƒœ í´ë˜ìŠ¤\n",
    "class State(MessagesState):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM ë‹µë³€ ìƒì„± ë…¸ë“œë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ì´ì „ì˜ ëŒ€í™”ìš”ì•½ ë‚´ìš©ì´ ìˆë‹¤ë©´ ì´ë¥¼ ì…ë ¥ì— í¬í•¨í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state: State):\n",
    "    # ì´ì „ ìš”ì•½ ì •ë³´ í™•ì¸\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # ì´ì „ ìš”ì•½ ì •ë³´ê°€ ìˆë‹¤ë©´ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¡œ ì¶”ê°€\n",
    "    if summary:\n",
    "        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ì´ì „ ë©”ì‹œì§€ ê²°í•©\n",
    "        messages = [\n",
    "            SystemMessage(content=f\"Summary of conversation earlier: {summary}\")\n",
    "        ] + state[\"messages\"]\n",
    "    else:\n",
    "        # ì´ì „ ë©”ì‹œì§€ë§Œ ì‚¬ìš©\n",
    "        messages = state[\"messages\"]\n",
    "\n",
    "    # ëª¨ë¸ í˜¸ì¶œ\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    # ì‘ë‹µ ë°˜í™˜\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìš”ì•½ì´ í•„ìš”í•œ ìƒí™©ì¸ì§€ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì—¬ê¸°ì„œëŠ” ë©”ì‹œì§€ ìˆ˜ê°€ 6ê°œ ì´ˆê³¼ë¼ë©´ ìš”ì•½ ë…¸ë“œë¡œ ì´ë™í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "\n",
    "\n",
    "# ëŒ€í™” ì¢…ë£Œ ë˜ëŠ” ìš”ì•½ ê²°ì • ë¡œì§\n",
    "def should_continue(state: State) -> Literal[\"summarize_conversation\", END]:\n",
    "    # ë©”ì‹œì§€ ëª©ë¡ í™•ì¸\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # ë©”ì‹œì§€ ìˆ˜ê°€ 6ê°œ ì´ˆê³¼ë¼ë©´ ìš”ì•½ ë…¸ë“œë¡œ ì´ë™\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìš”ì•½ ë…¸ë“œë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. ì´ì „ ìš”ì•½ ì •ë³´ê°€ ìˆë‹¤ë©´ ì´ë¥¼ ì…ë ¥ì— í¬í•¨í•˜ê³ , ì—†ë‹¤ë©´ ìƒˆë¡œìš´ ìš”ì•½ ë©”ì‹œì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëŒ€í™” ë‚´ìš© ìš”ì•½ ë° ë©”ì‹œì§€ ì •ë¦¬ ë¡œì§\n",
    "def summarize_conversation(state: State):\n",
    "    # ì´ì „ ìš”ì•½ ì •ë³´ í™•ì¸\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # ì´ì „ ìš”ì•½ ì •ë³´ê°€ ìˆë‹¤ë©´ ìš”ì•½ ë©”ì‹œì§€ ìƒì„±\n",
    "    if summary:\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above in Korean.\"\n",
    "        )\n",
    "    else:\n",
    "        # ìš”ì•½ ë©”ì‹œì§€ ìƒì„±\n",
    "        summary_message = \"Create a summary of the conversation above in Korean:\"\n",
    "\n",
    "    # ìš”ì•½ ë©”ì‹œì§€ì™€ ì´ì „ ë©”ì‹œì§€ ê²°í•©\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    # ëª¨ë¸ í˜¸ì¶œ\n",
    "    response = model.invoke(messages)\n",
    "    # ì˜¤ë˜ëœ ë©”ì‹œì§€ ì‚­ì œ\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    # ìš”ì•½ ì •ë³´ ë°˜í™˜\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ë˜í”„ ìƒì„± ë° ì»´íŒŒì¼\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ì´ˆê¸°í™”\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# ëŒ€í™” ë° ìš”ì•½ ë…¸ë“œ ì¶”ê°€\n",
    "workflow.add_node(\"conversation\", generate)\n",
    "workflow.add_node(summarize_conversation)\n",
    "\n",
    "# ì‹œì‘ì ì„ ëŒ€í™” ë…¸ë“œë¡œ ì„¤ì •\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "\n",
    "# ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€\n",
    "workflow.add_conditional_edges(\n",
    "    \"conversation\",\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# ìš”ì•½ ë…¸ë“œì—ì„œ ì¢…ë£Œ ë…¸ë“œë¡œì˜ ì—£ì§€ ì¶”ê°€\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼ ë° ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸í„° ì„¤ì •\n",
    "app = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ë˜í”„ë¥¼ ì‹œê°í™” í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‚¬ìš©ì ë©”ì‹œì§€ ì¶œë ¥ì„ ìœ„í•œ í•¨ìˆ˜ë¥¼ êµ¬í˜„(í—¬í¼ í•¨ìˆ˜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_user_message(message):\n",
    "    print(\"\\n==================================================\\n\\nğŸ˜\", message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ìš°ì„  6ê°œì˜ ëŒ€í™”ë¥¼ ì±„ì›Œë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ì‹œì§€ í•¸ë“¤ë§ì„ ìœ„í•œ HumanMessage í´ë˜ìŠ¤ ì„í¬íŠ¸\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# ìŠ¤ë ˆë“œ IDê°€ í¬í•¨ëœ ì„¤ì • ê°ì²´ ì´ˆê¸°í™”\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"resursion_limit\": 10}}\n",
    "\n",
    "# ì²« ë²ˆì§¸ ë©”ì‹œì§€\n",
    "print_user_message(\"ì•ˆë…•í•˜ì„¸ìš”? ë°˜ê°‘ìŠµë‹ˆë‹¤. ì œ ì´ë¦„ì€ í…Œë””ì…ë‹ˆë‹¤.\")\n",
    "stream_graph(\n",
    "    app,\n",
    "    {\"messages\": [(\"human\", \"ì•ˆë…•í•˜ì„¸ìš”? ë°˜ê°‘ìŠµë‹ˆë‹¤. ì œ ì´ë¦„ì€ í…Œë””ì…ë‹ˆë‹¤.\")]},\n",
    "    config,\n",
    ")\n",
    "\n",
    "# ë‘ ë²ˆì§¸ ë©”ì‹œì§€\n",
    "print_user_message(\"ì œ ì´ë¦„ì´ ë­”ì§€ ê¸°ì–µí•˜ì„¸ìš”?\")\n",
    "stream_graph(app, {\"messages\": [(\"human\", \"ì œ ì´ë¦„ì´ ë­”ì§€ ê¸°ì–µí•˜ì„¸ìš”?\")]}, config)\n",
    "\n",
    "# ì„¸ ë²ˆì§¸ ë©”ì‹œì§€\n",
    "print_user_message(\"ì œ ì·¨ë¯¸ëŠ” Netflix ì‹œë¦¬ì¦ˆë¥¼ ë³´ëŠ” ê²ƒì…ë‹ˆë‹¤.\")\n",
    "stream_graph(\n",
    "    app, {\"messages\": [(\"human\", \"ì œ ì·¨ë¯¸ëŠ” Netflix ì‹œë¦¬ì¦ˆë¥¼ ë³´ëŠ” ê²ƒì…ë‹ˆë‹¤.\")]}, config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. 6ê°œ ëŒ€í™”ë¥¼ í–ˆìœ¼ë¯€ë¡œ, ìš”ì•½ë³¸ì´ ë§Œë“¤ì–´ ì ¸ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒíƒœ êµ¬ì„± ê°’ ê²€ìƒ‰\n",
    "values = app.get_state(config).values\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ì¶”ê°€ë¡œ ëŒ€í™”ë¥¼ ì…ë ¥í•˜ì—¬ ìš”ì•½ë³¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì˜ ë‹µë³€í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë„¤ ë²ˆì§¸ ë©”ì‹œì§€\n",
    "print_user_message(\"ì œ ì·¨ë¯¸ê°€ ë­ë¼ê³  í–ˆë‚˜ìš”?\")\n",
    "stream_graph(app, {\"messages\": [(\"human\", \"ì œ ì·¨ë¯¸ê°€ ë­ë¼ê³  í–ˆë‚˜ìš”?\")]}, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¶”ì : https://smith.langchain.com/public/c0f62f0b-74b5-4dc3-bd1c-3e474a0dbef9/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7. Human in the Loop\n",
    "\n",
    "LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ Human-in-the-loop ì€ ìë™í™”ëœ AI ì‹œìŠ¤í…œê³¼ ì¸ê°„ì˜ ê°œì… ë° íŒë‹¨ì„ ê²°í•©í•˜ëŠ” ì ‘ê·¼ ë°©ì‹ì…ë‹ˆë‹¤. \n",
    "\n",
    "ì´ ë°©ì‹ì—ì„œëŠ” AI ì‹œìŠ¤í…œì´ ì´ˆê¸° ì²˜ë¦¬ì™€ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì§€ë§Œ, ë¶ˆí™•ì‹¤í•˜ê±°ë‚˜ ì¤‘ìš”í•œ ê²°ì •ì´ í•„ìš”í•œ ì‹œì ì—ì„œ ì¸ê°„ ì „ë¬¸ê°€ì˜ ê°œì…ì„ ìš”ì²­í•©ë‹ˆë‹¤. \n",
    "\n",
    "Human-in-the-loop ì€ ë†’ì€ ì •í™•ë„ê°€ í•„ìš”í•œ ë³µì¡í•œ ìƒí™©, ìœ¤ë¦¬ì  íŒë‹¨ì´ í•„ìš”í•œ ê²½ìš°, ë˜ëŠ” AIì˜ ì‹ ë¢°ë„ê°€ ë‚®ì€ ê²°ê³¼ì— ëŒ€í•´ ê²€ì¦ì´ í•„ìš”í•  ë•Œ íŠ¹íˆ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "`from langgraph.types import interrupt`\n",
    "\n",
    "`interrupt` í•¨ìˆ˜ëŠ” ì¸ê°„ì˜ ê°œì…ì„ ìš”ì²­í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ì¸ê°„ì˜ ì…ë ¥ì„ ë°›ì•„ ì²˜ë¦¬í•˜ê³ , ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "import uuid\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    evaluation: Annotated[str, \"Evaluation\"]\n",
    "\n",
    "\n",
    "def llm_node(state: State):\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def human_node(state: State):\n",
    "    value = interrupt(\n",
    "        # ì‚¬ëŒì˜ í”¼ë“œë°± ìš”ì²­\n",
    "        {\"text_to_revise\": state[\"messages\"][-1]}\n",
    "    )\n",
    "    return {\n",
    "        # ì‚¬ëŒì˜ í”¼ë“œë°±ìœ¼ë¡œ ì—…ë°ì´íŠ¸\n",
    "        \"messages\": [HumanMessage(content=value)]\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluation_node(state: State):\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "    prompt = f\"\"\"\n",
    "Here is the user's question and the model's response\n",
    "The user's question: {state[\"messages\"][0]}\n",
    "Model's response: {state[\"messages\"][-1]}\n",
    "\n",
    "Please rate whether the model's response accurately answered your question.\"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"evaluation\": response}\n",
    "\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„±\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# ë…¸ë“œ ì¶”ê°€\n",
    "workflow.add_node(\"llm\", llm_node)\n",
    "workflow.add_node(\"human\", human_node)\n",
    "workflow.add_node(\"eval\", evaluation_node)\n",
    "\n",
    "# ì—£ì§€ ì¶”ê°€\n",
    "workflow.add_edge(START, \"llm\")\n",
    "workflow.add_edge(\"llm\", \"human\")\n",
    "workflow.add_edge(\"human\", \"eval\")\n",
    "workflow.add_edge(\"eval\", END)\n",
    "\n",
    "# ì²´í¬í¬ì¸í„° ì„¤ì •\n",
    "checkpointer = MemorySaver()\n",
    "app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ˆê¸° ì§ˆë¬¸ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config ì„¤ì •\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "\n",
    "stream_graph(app, {\"messages\": [(\"human\", \"2+6=?\")]}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.get_state(config).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Command` ê°ì²´ëŠ” ì¸ê°„ì˜ ê°œì…ì„ ìš”ì²­í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ê°ì²´ëŠ” ì¸ê°„ì˜ ì…ë ¥ì„ ë°›ì•„ ì²˜ë¦¬í•˜ê³ , ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `resume`: í”¼ë“œë°±ì„ ì…ë ¥í•˜ê³  ë‚¨ì€ ë‹¨ê³„ë¥¼ ì¬ê²Œ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_graph(app, Command(resume=\"2 + 6 = 9\"), config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (ì¸í„°ëŸ½íŠ¸) ì¶”ì : https://smith.langchain.com/public/c251955b-6999-4983-9a92-6905700333d3/r\n",
    "- (ì´í›„) ì¶”ì : https://smith.langchain.com/public/c251955b-6999-4983-9a92-6905700333d3/r    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "import uuid\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.graph import END\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    evaluation: Annotated[str, \"Evaluation\"]\n",
    "\n",
    "\n",
    "def llm_node(state: State):\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def human_approval(state: State) -> Command[Literal[\"llm\", END]]:\n",
    "    is_approved = interrupt(\n",
    "        {\n",
    "            \"question\": \"Is this correct?\",\n",
    "            # ë‹µë³€ì„ ì‚¬ëŒì—ê²Œ ë³´ì—¬ì£¼ê³  ìˆ˜ì • ìš”ì²­\n",
    "            \"need_to_revise\": state[\"messages\"][-1],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if is_approved:\n",
    "        return Command(\n",
    "            goto=END, update={\"messages\": [HumanMessage(content=\"You are great!\")]}\n",
    "        )\n",
    "    else:\n",
    "        return Command(\n",
    "            goto=\"llm\",\n",
    "            update={\n",
    "                \"messages\": [HumanMessage(content=\"You are wrong.. Please try again\")]\n",
    "            },\n",
    "        )\n",
    "\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„±\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# ë…¸ë“œ ì¶”ê°€\n",
    "workflow.add_node(\"llm\", llm_node)\n",
    "workflow.add_node(\"human\", human_approval)\n",
    "\n",
    "# ì—£ì§€ ì¶”ê°€\n",
    "workflow.add_edge(START, \"llm\")\n",
    "workflow.add_edge(\"llm\", \"human\")\n",
    "\n",
    "# ì²´í¬í¬ì¸í„° ì„¤ì •\n",
    "checkpointer = MemorySaver()\n",
    "app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config ì„¤ì •\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "\n",
    "stream_graph(app, {\"messages\": [(\"human\", \"1, 2, 4, 10, ?\")]}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.get_state(config).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_graph(app, Command(resume=False), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.get_state(config).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8. Long-Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ë©”ëª¨ë¦¬**ëŠ” ì‚¬ëŒë“¤ì´ í˜„ì¬ì™€ ë¯¸ë˜ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ì •ë³´ë¥¼ ì €ì¥í•˜ê³ , ê²€ìƒ‰í•˜ë©° ì‚¬ìš©í•˜ëŠ” ì¸ì§€ ê¸°ëŠ¥ì…ë‹ˆë‹¤.\n",
    "\n",
    "AI ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” [ë‹¤ì–‘í•œ **ì¥ê¸° ë©”ëª¨ë¦¬ ìœ í˜•**](https://langchain-ai.github.io/langgraph/concepts/memory/#memory)ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì—¬ê¸°ì—ì„œëŠ” **ì¥ê¸° ê¸°ì–µ**ì„ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ [LangGraph Memory Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore)ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤.\n",
    "\n",
    "`short-term (within-thread)` ë° `long-term (across-thread)` ë©”ëª¨ë¦¬ë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ì±—ë´‡ì„ êµ¬ì¶•í•  ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ì‚¬ìš©ìì— ëŒ€í•œ ì‚¬ì‹¤ì¸ ì¥ê¸° [**semantic memory**](https://langchain-ai.github.io/langgraph/concepts/memory/#semantic-memory)ì— ì¤‘ì ì„ ë‘˜ ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangGraph Store**\n",
    "\n",
    "[LangGraph Memory Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) ëŠ” LangGraphì—ì„œ *thread ê°„* ì •ë³´ë¥¼ ì €ì¥í•˜ê³  ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤. \n",
    "\n",
    "ì´ ì €ì¥ì†ŒëŠ” ì§€ì†ì ì¸ `key-value` ë°ì´í„°ë¥¼ ê´€ë¦¬í•˜ê¸° ìœ„í•œ [ì˜¤í”ˆ ì†ŒìŠ¤ ê¸°ë³¸ í´ë˜ìŠ¤](https://blog.langchain.dev/launching-long-term-memory-support-in-langgraph/)ë¡œ, ê°œë°œìê°€ ìì‹ ë§Œì˜ ì €ì¥ì†Œë¥¼ ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# LangGraphì˜ InMemoryStore ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "persistant_memory = InMemoryStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìŠ¤í† ì–´ì— ê°ì²´(ì˜ˆ: ë©”ëª¨ë¦¬)ë¥¼ ì €ì¥í•  ë•ŒëŠ” [Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore)ì— ë‹¤ìŒ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "- **namespace** (`namespace`): ê°ì²´ë¥¼ êµ¬ë¶„í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” íŠœí”Œ í˜•íƒœì˜ ì‹ë³„ìì…ë‹ˆë‹¤ (ë””ë ‰í† ë¦¬ì™€ ìœ ì‚¬).\n",
    "- **key** (`key`): ê°ì²´ì˜ ê³ ìœ  ì‹ë³„ìì…ë‹ˆë‹¤ (íŒŒì¼ ì´ë¦„ê³¼ ìœ ì‚¬).\n",
    "- **value** (`value`): ê°ì²´ì˜ ì‹¤ì œ ë‚´ìš©ì…ë‹ˆë‹¤ (íŒŒì¼ ë‚´ìš©ê³¼ ìœ ì‚¬).\n",
    "\n",
    "`namespace`ì™€ `key`ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì²´ë¥¼ ìŠ¤í† ì–´ì— ì €ì¥í•˜ê¸° ìœ„í•´ [put](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.put) ë©”ì„œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì €ì¥í•  ë©”ëª¨ë¦¬ì˜ ì‚¬ìš©ì ID ì„¤ì •\n",
    "user_id = \"teddy\"\n",
    "\n",
    "# ì‚¬ìš©ì IDì™€ ë©”ëª¨ë¦¬ êµ¬ë¶„ì„ ìœ„í•œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜\n",
    "namespace_for_memory = (\"memories\", user_id)\n",
    "\n",
    "# ê³ ìœ  í‚¤ ìƒì„±ì„ ìœ„í•œ UUID ìƒì„±\n",
    "key = \"user_memory\"\n",
    "\n",
    "# ì €ì¥í•  ë©”ëª¨ë¦¬ ê°’ìœ¼ë¡œ ë”•ì…”ë„ˆë¦¬ ì •ì˜\n",
    "value = {\n",
    "    \"job\": \"AI Engineer\",\n",
    "    \"location\": \"Seoul, Korea\",\n",
    "    \"hobbies\": [\"Watching Netflix\", \"Coding\"],\n",
    "}\n",
    "\n",
    "# ì§€ì •ëœ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ë©”ëª¨ë¦¬ ì €ì¥\n",
    "persistant_memory.put(namespace_for_memory, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`search`](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.search)ì„ ì´ìš©í•˜ì—¬ `namespace` ê¸°ì¤€ìœ¼ë¡œ `store`ì—ì„œ ê°ì²´ë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë©”ì„œë“œëŠ” ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§€ì •ëœ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¡œë¶€í„° ë©”ëª¨ë¦¬ ê°ì²´ ê²€ìƒ‰\n",
    "memories = persistant_memory.search(namespace_for_memory)\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë©”ëª¨ë¦¬ ê°ì²´ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "memories[0].dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì¥ê¸° ë©”ëª¨ë¦¬ë¥¼ ê°–ì¶˜ ì±—ë´‡**\n",
    "\n",
    "ì±—ë´‡ì€ [ë‘ ê°€ì§€ ìœ í˜•ì˜ ë©”ëª¨ë¦¬](https://docs.google.com/presentation/d/181mvjlgsnxudQI6S3ritg9sooNyu4AcLLFH1UK0kIuk/edit#slide=id.g30eb3c8cf10_0_156)ë¥¼ ê°–ì¶”ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "1. `Short-term (within-thread) memory`: ì±—ë´‡ì´ ëŒ€í™” ë‚´ì—­ì„ ì§€ì†ì ìœ¼ë¡œ ì €ì¥í•˜ê±°ë‚˜, ëŒ€í™” ì„¸ì…˜ ì¤‘ì— ì¤‘ë‹¨ì„ í—ˆìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "2. `Long-term (cross-thread) memory`: ì±—ë´‡ì´ íŠ¹ì • ì‚¬ìš©ìì— ëŒ€í•œ ì •ë³´ë¥¼ *ëª¨ë“  ëŒ€í™” ì„¸ì…˜ì— ê±¸ì³* ê¸°ì–µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ ë‘ ê°€ì§€ ë©”ëª¨ë¦¬ ìœ í˜•ì„ í†µí•´ ì±—ë´‡ì€ ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ë¥¼ ë”ìš± ì›í™œí•˜ê³  ê°œì¸í™”ëœ ë°©ì‹ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `Short-term memory`ëŠ” í˜„ì¬ ëŒ€í™” ì„¸ì…˜ ë‚´ì—ì„œì˜ ë§¥ë½ì„ ìœ ì§€í•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, `Long-term memory`ëŠ” ì‚¬ìš©ìì— ëŒ€í•œ ì§€ì†ì ì¸ ì •ë³´ë¥¼ ì €ì¥í•˜ì—¬ ì—¬ëŸ¬ ì„¸ì…˜ì— ê±¸ì³ ì¼ê´€ëœ ìƒí˜¸ì‘ìš©ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "from langchain_teddynote.graphs import visualize_graph\n",
    "from langchain_teddynote.messages import stream_graph\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# ëª¨ë¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì •ì˜\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful assistant with memory that provides information about the user. \n",
    "If you have memory for this user, use it to personalize your responses.\n",
    "Here is the memory (it may be empty): {memory}\"\"\"\n",
    "\n",
    "# ìƒˆë¡œìš´ ë©”ëª¨ë¦¬ ìƒì„± ì§€ì¹¨ ì •ì˜\n",
    "CREATE_MEMORY_INSTRUCTION = \"\"\"\"You are collecting information about the user to personalize your responses.\n",
    "\n",
    "CURRENT USER INFORMATION:\n",
    "{memory}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Review the chat history below carefully\n",
    "2. Identify new information about the user, such as:\n",
    "   - Personal details (name, job, location)\n",
    "   - Preferences (likes, dislikes)\n",
    "   - Interests and hobbies\n",
    "   - Past experiences\n",
    "   - Goals or future plans\n",
    "3. Merge any new information with existing memory\n",
    "4. Format the memory as a clear, bulleted list\n",
    "5. If new information conflicts with existing memory, keep the most recent version\n",
    "\n",
    "Remember: Only include factual information directly stated by the user. Do not make assumptions or inferences.\n",
    "\n",
    "Based on the chat history below, please update the user information:\"\"\"\n",
    "\n",
    "\n",
    "# call_model í•¨ìˆ˜ ì •ì˜\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"Load memory from the store and use it to personalize the chatbot's response.\"\"\"\n",
    "\n",
    "    # ì„¤ì •ì—ì„œ ì‚¬ìš©ì ID ê°€ì ¸ì˜¤ê¸°\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # ìŠ¤í† ì–´ì—ì„œ ë©”ëª¨ë¦¬ ê²€ìƒ‰\n",
    "    namespace = (\"memories\", user_id)\n",
    "    key = \"user_memory\"\n",
    "    existing_memory = store.get(namespace, key)\n",
    "\n",
    "    # ê¸°ì¡´ ë©”ëª¨ë¦¬ ë‚´ìš© ì¶”ì¶œ ë° í”„ë¦¬í”½ìŠ¤ ì¶”ê°€\n",
    "    if existing_memory:\n",
    "        # ê°’ì€ ë©”ëª¨ë¦¬ í‚¤ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬\n",
    "        existing_memory_content = existing_memory.value.get(\"memories\")\n",
    "    else:\n",
    "        existing_memory_content = \"No existing memory found.\"\n",
    "\n",
    "    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ë©”ëª¨ë¦¬ í¬ë§·\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=existing_memory_content)\n",
    "\n",
    "    # ë©”ëª¨ë¦¬ì™€ ëŒ€í™” ê¸°ë¡ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±\n",
    "    response = model.invoke([SystemMessage(content=system_msg)] + state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# write_memory í•¨ìˆ˜ ì •ì˜\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"Reflect on the chat history and save a memory to the store.\"\"\"\n",
    "\n",
    "    # ì„¤ì •ì—ì„œ ì‚¬ìš©ì ID ê°€ì ¸ì˜¤ê¸°\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # ìŠ¤í† ì–´ì—ì„œ ê¸°ì¡´ ë©”ëª¨ë¦¬ ê²€ìƒ‰\n",
    "    namespace = (\"memories\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "\n",
    "    # ë©”ëª¨ë¦¬ ì¶”ì¶œ\n",
    "    if existing_memory:\n",
    "        existing_memory_content = existing_memory.value.get(\"memories\")\n",
    "    else:\n",
    "        existing_memory_content = \"No existing memory found.\"\n",
    "\n",
    "    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ë©”ëª¨ë¦¬ í¬ë§·\n",
    "    system_msg = CREATE_MEMORY_INSTRUCTION.format(memory=existing_memory_content)\n",
    "    new_memory = model.invoke([SystemMessage(content=system_msg)] + state[\"messages\"])\n",
    "\n",
    "    # ìŠ¤í† ì–´ì— ê¸°ì¡´ ë©”ëª¨ë¦¬ ë®ì–´ì“°ê¸°\n",
    "    key = \"user_memory\"\n",
    "\n",
    "    # ë©”ëª¨ë¦¬ í‚¤ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¡œ ê°’ ì‘ì„±\n",
    "    store.put(namespace, key, {\"memories\": new_memory.content})\n",
    "\n",
    "\n",
    "# ê·¸ë˜í”„ ì •ì˜\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"call_model\", call_model)\n",
    "workflow.add_node(\"write_memory\", write_memory)\n",
    "workflow.add_edge(START, \"call_model\")\n",
    "workflow.add_edge(\"call_model\", \"write_memory\")\n",
    "workflow.add_edge(\"write_memory\", END)\n",
    "\n",
    "# ì¥ê¸° ë©”ëª¨ë¦¬ ì €ì¥ì†Œ ì„¤ì •\n",
    "across_thread_memory = InMemoryStore()\n",
    "\n",
    "# ë‹¨ê¸° ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸í„° ì„¤ì •\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# ì²´í¬í¬ì¸í„° ë° ìŠ¤í† ì–´ë¥¼ í¬í•¨í•˜ì—¬ ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "app = workflow.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹œê°í™”\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¨ê¸° ë©”ëª¨ë¦¬ë¥¼ ìœ„í•œ ì“°ë ˆë“œ ID ì œê³µ(thread_id)\n",
    "# ì¥ê¸° ë©”ëª¨ë¦¬ë¥¼ ìœ„í•œ ì‚¬ìš©ì ID ì œê³µ(user_id)\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"teddy\"}}\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥\n",
    "input_messages = [\n",
    "    HumanMessage(\n",
    "        content=\"ì•ˆë…• ë°˜ê°€ì›Œ! ë‚´ ì´ë¦„ì€ í…Œë”” ì…ë‹ˆë‹¤. ì €ì˜ ì·¨ë¯¸ëŠ” ì½”ë”© í•˜ê³  ì˜í™” ë³´ëŠ” ê²ƒì…ë‹ˆë‹¤.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "stream_graph(app, {\"messages\": input_messages}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¨ê¸° ë©”ëª¨ë¦¬ë¥¼ ìœ„í•œ ì“°ë ˆë“œ ID ì œê³µ(thread_id)\n",
    "# ì¥ê¸° ë©”ëª¨ë¦¬ë¥¼ ìœ„í•œ ì‚¬ìš©ì ID ì œê³µ(user_id)\n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"teddy2\"}}\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥\n",
    "input_messages = [HumanMessage(content=\"ë‚´ ì·¨ë¯¸ê°€ ë­ì˜€ë”ë¼...\")]\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "stream_graph(app, {\"messages\": input_messages}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¨ê¸° ë©”ëª¨ë¦¬ë¥¼ ìœ„í•œ ì“°ë ˆë“œ ID ì œê³µ(thread_id)\n",
    "# ì¥ê¸° ë©”ëª¨ë¦¬ë¥¼ ìœ„í•œ ì‚¬ìš©ì ID ì œê³µ(user_id)\n",
    "config = {\"configurable\": {\"thread_id\": \"3\", \"user_id\": \"teddy\"}}\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥\n",
    "input_messages = [HumanMessage(content=\"ë‚´ ì·¨ë¯¸ê°€ ë­ì˜€ë”ë¼...\")]\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "stream_graph(app, {\"messages\": input_messages}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistant_memory.get((\"memories\", \"teddy\"), \"user_memory\").value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¨ê¸° ë©”ëª¨ë¦¬ë¥¼ ìœ„í•œ ì“°ë ˆë“œ ID ì œê³µ(thread_id)\n",
    "# ì¥ê¸° ë©”ëª¨ë¦¬ë¥¼ ìœ„í•œ ì‚¬ìš©ì ID ì œê³µ(user_id)\n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"john\"}}\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥\n",
    "input_messages = [HumanMessage(content=\"ì•ˆë…• ë°˜ê°€ì›Œ! í˜¹ì‹œ ë‚´ ì·¨ë¯¸ ê¸°ì–µí•´?\")]\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "stream_graph(app, {\"messages\": input_messages}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¨ê¸° ë©”ëª¨ë¦¬ë¥¼ ìœ„í•œ ì“°ë ˆë“œ ID ì œê³µ(thread_id)\n",
    "# ì¥ê¸° ë©”ëª¨ë¦¬ë¥¼ ìœ„í•œ ì‚¬ìš©ì ID ì œê³µ(user_id)\n",
    "config = {\"configurable\": {\"thread_id\": \"3\", \"user_id\": \"teddy\"}}\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥\n",
    "input_messages = [HumanMessage(content=\"ë‚˜ì— ëŒ€í•´ ì•„ëŠ” ì •ë³´ ëª¨ë‘ ë§í•´ì¤˜\")]\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "stream_graph(app, {\"messages\": input_messages}, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¶”ì : https://smith.langchain.com/public/618341ed-4bc2-443f-8ad0-f96fc464fac8/r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
