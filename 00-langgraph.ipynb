{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph í•¸ì¦ˆì˜¨\n",
    "\n",
    "**ì°¸ê³ í•˜ë©´ ì¢‹ì€ ìë£Œ**\n",
    "\n",
    "- [LangChain í•œêµ­ì–´ íŠœí† ë¦¬ì–¼ğŸ‡°ğŸ‡·](https://wikidocs.net/book/14314)\n",
    "- [LangChain í•œêµ­ì–´ íŠœí† ë¦¬ì–¼ Github ì†ŒìŠ¤ì½”ë“œ](https://github.com/teddylee777/langchain-kr)\n",
    "- [í…Œë””ë…¸íŠ¸ YouTube](https://www.youtube.com/c/@teddynote)\n",
    "- [í…Œë””ë…¸íŠ¸ ë¸”ë¡œê·¸](https://teddylee777.github.io/)\n",
    "- [í…Œë””ë…¸íŠ¸ YouTube ë¡œ RAG ë°°ìš°ê¸°!](https://teddylee777.notion.site/YouTube-RAG-10a24f35d12980dc8478c750faa752a2?pvs=74)\n",
    "- [RAG ë¹„ë²•ë…¸íŠ¸](https://fastcampus.co.kr/data_online_teddy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. í™˜ê²½ ì„¤ì •\n",
    "\n",
    "**OpenAI API Key ì„¤ì •**\n",
    "- https://wikidocs.net/233342\n",
    "\n",
    "**ì›¹ ê²€ìƒ‰ì„ ìœ„í•œ API í‚¤ ë°œê¸‰ ì£¼ì†Œ**\n",
    "- https://app.tavily.com/\n",
    "\n",
    "íšŒì› ê°€ì… í›„ API Keyë¥¼ ë°œê¸‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì„¤ì¹˜ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langgraph langchain_openai langchain_teddynote faiss-cpu pdfplumber langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì‹¤ìŠµìë£Œë¥¼ ë‹¤ìš´ë¡œë“œ ë°›ìŠµë‹ˆë‹¤**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"  # ë°œê¸‰ ë°›ì€ OpenAI API Key ì…ë ¥\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"\"  # ë°œê¸‰ ë°›ì€ Tavily API Key ì…ë ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ì„ íƒ ì‚¬í•­)\n",
    "\n",
    "LangSmith ì¶”ì ì„ ì›í•˜ëŠ” ê²½ìš° ì•„ë˜ LangSmith API Key ë¥¼ ë°œê¸‰ ë°›ì•„ ì…ë ¥í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "- ë§í¬: https://smith.langchain.com\n",
    "- íšŒì› ê°€ì… í›„ - ì„¤ì • - ìƒë‹¨ API Keys ì—ì„œ ë°œê¸‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGSMITH_API_KEY\"] = \"\"  # ë°œê¸‰ ë°›ì€ LangSmith API Key ì…ë ¥\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"  # ì¶”ì  ì„¤ì •\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"  # ì¶”ì  ì—”ë“œí¬ì¸íŠ¸\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"LangGraph-Hands-On\"  # í”„ë¡œì íŠ¸ ì´ë¦„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. ê¸°ë³¸ ReAct Agent êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ëª¨ë¸ ì„¤ì •\n",
    "model = ChatOpenAI(model_name=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë„êµ¬ (Tools) ì„¤ì •\n",
    "\n",
    "ë„êµ¬(Tool)ëŠ” ì—ì´ì „íŠ¸, ì²´ì¸ ë˜ëŠ” LLMì´ ì™¸ë¶€ ì„¸ê³„ì™€ ìƒí˜¸ì‘ìš©í•˜ê¸° ìœ„í•œ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.\n",
    "\n",
    "LangChain ì—ì„œ ê¸°ë³¸ ì œê³µí•˜ëŠ” ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‰½ê²Œ ë„êµ¬ë¥¼ í™œìš©í•  ìˆ˜ ìˆìœ¼ë©°, ì‚¬ìš©ì ì •ì˜ ë„êµ¬(Custom Tool) ë¥¼ ì‰½ê²Œ êµ¬ì¶•í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "**LangChain ì— í†µí•©ëœ ë„êµ¬ ë¦¬ìŠ¤íŠ¸ëŠ” ì•„ë˜ ë§í¬ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.**\n",
    "\n",
    "ë­ì²´ì¸ì—ì„œ ì œê³µí•˜ëŠ” ì‚¬ì „ì— ì •ì˜ëœ ë„êµ¬(tool) ì™€ íˆ´í‚·(toolkit) ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "tool ì€ ë‹¨ì¼ ë„êµ¬ë¥¼ ì˜ë¯¸í•˜ë©°, toolkit ì€ ì—¬ëŸ¬ ë„êµ¬ë¥¼ ë¬¶ì–´ì„œ í•˜ë‚˜ì˜ ë„êµ¬ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ê´€ë ¨ ë„êµ¬ëŠ” ì•„ë˜ì˜ ë§í¬ì—ì„œ ì°¸ê³ í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "**ì°¸ê³ **\n",
    "- [LangChain Tools/Toolkits](https://python.langchain.com/docs/integrations/tools/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ê²€ìƒ‰ API ë„êµ¬**\n",
    "\n",
    "Tavily ê²€ìƒ‰ APIë¥¼ í™œìš©í•˜ì—¬ ê²€ìƒ‰ ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤. \n",
    "\n",
    "**API í‚¤ ë°œê¸‰ ì£¼ì†Œ**\n",
    "- https://app.tavily.com/\n",
    "\n",
    "ë°œê¸‰í•œ API í‚¤ë¥¼ í™˜ê²½ë³€ìˆ˜ì— ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "`.env` íŒŒì¼ì— ì•„ë˜ì™€ ê°™ì´ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "```\n",
    "TAVILY_API_KEY=tvly-abcdefghijklmnopqrstuvwxyz\n",
    "```\n",
    "\n",
    "**TavilySearch**\n",
    "\n",
    "**ì„¤ëª…**\n",
    "- Tavily ê²€ìƒ‰ APIë¥¼ ì¿¼ë¦¬í•˜ê³  JSON í˜•ì‹ì˜ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "- í¬ê´„ì ì´ê³  ì •í™•í•˜ë©° ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²°ê³¼ì— ìµœì í™”ëœ ê²€ìƒ‰ ì—”ì§„ì…ë‹ˆë‹¤.\n",
    "- í˜„ì¬ ì´ë²¤íŠ¸ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” ë§¤ê°œë³€ìˆ˜**\n",
    "- `max_results` (int): ë°˜í™˜í•  ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 5)\n",
    "- `search_depth` (str): ê²€ìƒ‰ ê¹Šì´ (\"basic\" ë˜ëŠ” \"advanced\")\n",
    "- `include_domains` (List[str]): ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨í•  ë„ë©”ì¸ ëª©ë¡\n",
    "- `exclude_domains` (List[str]): ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì œì™¸í•  ë„ë©”ì¸ ëª©ë¡\n",
    "- `include_answer` (bool): ì›ë³¸ ì¿¼ë¦¬ì— ëŒ€í•œ ì§§ì€ ë‹µë³€ í¬í•¨ ì—¬ë¶€\n",
    "- `include_raw_content` (bool): ê° ì‚¬ì´íŠ¸ì˜ ì •ì œëœ HTML ì½˜í…ì¸  í¬í•¨ ì—¬ë¶€\n",
    "- `include_images` (bool): ì¿¼ë¦¬ ê´€ë ¨ ì´ë¯¸ì§€ ëª©ë¡ í¬í•¨ ì—¬ë¶€\n",
    "\n",
    "**ë°˜í™˜ ê°’**\n",
    "- ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬í•¨í•˜ëŠ” JSON í˜•ì‹ì˜ ë¬¸ìì—´(url, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "\n",
    "# ì›¹ ê²€ìƒ‰ ë„êµ¬ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "web_search_tool = TavilySearch(\n",
    "    max_results=6,  # ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼\n",
    ")\n",
    "\n",
    "# ì›¹ ê²€ìƒ‰ ë„êµ¬ì˜ ì´ë¦„ê³¼ ì„¤ëª…ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "web_search_tool.name = \"web_search\"\n",
    "web_search_tool.description = \"Use this tool to search on the web\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PDFRetrievalChain`: PDF ë¬¸ì„œ ê¸°ë°˜ Naive RAG ì²´ì¸\n",
    "\n",
    "ë¬¸ì„œ ê¸°ë°˜ RAG ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ ì²´ì¸ì€ ì£¼ì–´ì§„ PDF ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì£¼ìš” ë§¤ê°œë³€ìˆ˜**\n",
    "- `source_uri` (List[str]): PDF ë¬¸ì„œì˜ ê²½ë¡œ\n",
    "- `model_name` (str): ì‚¬ìš©í•  ëª¨ë¸ì˜ ì´ë¦„\n",
    "- `k` (int): ë°˜í™˜í•  ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.pdf import PDFRetrievalChain\n",
    "\n",
    "# PDF ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "pdf = PDFRetrievalChain(\n",
    "    [\"data/SPRI_AI_Brief_2023ë…„12ì›”í˜¸_F.pdf\"], model_name=\"gpt-4o-mini\", k=6\n",
    ").create_chain()\n",
    "\n",
    "# retrieverì™€ chainì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "pdf_retriever = pdf.retriever\n",
    "pdf_chain = pdf.chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì…ë ¥í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "searched_docs = pdf_retriever.invoke(\"ì‚¼ì„±ì „ìê°€ ë§Œë“  ìƒì„±í˜• AI ì´ë¦„ì„ ì°¾ì•„ì¤˜\")\n",
    "searched_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì¶”ì **: https://smith.langchain.com/public/bdaa2410-0a6a-44c9-8e2b-c5d8628bf84e/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = pdf_chain.invoke(\n",
    "    {\"question\": \"ì‚¼ì„±ì „ìê°€ ë§Œë“  ìƒì„±í˜• AI ì´ë¦„ì„ ì°¾ì•„ì¤˜\", \"context\": searched_docs}\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "\n",
    "# PDF ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ ë„êµ¬ ìƒì„±\n",
    "retriever_tool = create_retriever_tool(\n",
    "    pdf_retriever,\n",
    "    \"pdf_retriever\",\n",
    "    \"Search and return information about SPRI AI Brief PDF file. It contains useful information on recent AI trends. The document is published on Dec 2023.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = retriever_tool.invoke(\"ì‚¼ì„±ì „ìê°€ ë§Œë“  ìƒì„±í˜• AI ì´ë¦„ì„ ì°¾ì•„ì¤˜\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë„êµ¬ ëª©ë¡ì„ ì •ì˜ í•©ë‹ˆë‹¤. ì´ëŠ” Agent ì—ê²Œ ì œê³µë  ë„êµ¬ ëª©ë¡ì…ë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë„êµ¬ ëª©ë¡ ì •ì˜\n",
    "tools = [web_search_tool, retriever_tool]\n",
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_react_agent`\n",
    "\n",
    "ReAct Agent ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ëŠ” ë„êµ¬ ëª©ë¡ì„ ì œê³µí•˜ê³ , ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `model`: ì‚¬ìš©í•  ëª¨ë¸\n",
    "- `tools`: ë„êµ¬ ëª©ë¡\n",
    "- `prompt`: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "simple_react_agent = create_react_agent(\n",
    "    model, tools, prompt=\"You are a helpful assistant. Answer in Korean.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ê·¸ë˜í”„ ì‹œê°í™”**\n",
    "\n",
    "`visualize_graph` í•¨ìˆ˜ëŠ” ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(simple_react_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê·¸ë˜í”„ ì‹¤í–‰\n",
    "\n",
    "- `config` íŒŒë¼ë¯¸í„°ëŠ” ê·¸ë˜í”„ ì‹¤í–‰ ì‹œ í•„ìš”í•œ ì„¤ì • ì •ë³´ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "    - `resursion_limit`: ê·¸ë˜í”„ ì‹¤í–‰ ì‹œ ì¬ê·€ ìµœëŒ€ íšŸìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "    - `thread_id`: ê·¸ë˜í”„ ì‹¤í–‰ ì‹œ ìŠ¤ë ˆë“œ ì•„ì´ë””ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "- `inputs`: ê·¸ë˜í”„ ì‹¤í–‰ ì‹œ í•„ìš”í•œ ì…ë ¥ ì •ë³´ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì°¸ê³ **\n",
    "\n",
    "- ë©”ì‹œì§€ ì¶œë ¥ ìŠ¤íŠ¸ë¦¬ë°ì€ [LangGraph ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì˜ ëª¨ë“  ê²ƒ](https://wikidocs.net/265770) ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_graph\n",
    "\n",
    "# Config ì„¤ì •\n",
    "config = {\"configurable\": {\"resursion_limit\": 10, \"thread_id\": \"abc123\"}}\n",
    "\n",
    "# ì…ë ¥ ì„¤ì •\n",
    "inputs = {\n",
    "    \"messages\": [(\"human\", \"AI Brief ë¬¸ì„œì—ì„œ ì‚¼ì„±ì „ìê°€ ë§Œë“  ìƒì„±í˜• AI ì´ë¦„ì„ ì°¾ì•„ì¤˜\")]\n",
    "}\n",
    "\n",
    "# ê·¸ë˜í”„ ìŠ¤íŠ¸ë¦¼\n",
    "stream_graph(simple_react_agent, inputs, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¶”ì : https://smith.langchain.com/public/145d8012-9791-4320-a17d-b2ec048d0110/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì°¸ê³ **: `config` ëŠ” ì´ì „ì˜ ê°’ì„ ì¬í™œìš© í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ìŠ¤íŠ¸ë¦¼\n",
    "stream_graph(\n",
    "    simple_react_agent,\n",
    "    {\"messages\": [(\"human\", \"claude 3.7 sonnet ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•´ì¤˜\")]},\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì¶”ì **: https://smith.langchain.com/public/e28f6b6c-463c-4211-8a75-3c88dfdcc41c/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•œ ë‹¨ê¸° ë©”ëª¨ë¦¬: `checkpointer`\n",
    "\n",
    "ë‹¨ê¸° ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì´ ì—†ëŠ” ê·¸ë˜í”„ëŠ” ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ì§€ ëª»í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì¦‰, ë©€í‹°í„´ ëŒ€í™”ë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ë§ì´ê¸°ë„ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ë‹¤ìŒê³¼ ê°™ì´ ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ì§€ ëª»í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ìŠ¤íŠ¸ë¦¼\n",
    "stream_graph(\n",
    "    simple_react_agent,\n",
    "    {\"messages\": [(\"human\", \"ì•ˆë…•, ë°˜ê°€ì›Œ! ë‚´ ì´ë¦„ì€ í…Œë””ì•¼!\")]},\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ìŠ¤íŠ¸ë¦¼\n",
    "stream_graph(simple_react_agent, {\"messages\": [(\"human\", \"ë‚´ ì´ë¦„ì´ ë­ë¼ê³ ?\")]}, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `MemorySaver`\n",
    "\n",
    "LangGraph ëŠ” `Checkpointer` ë¥¼ ì‚¬ìš©í•´ ê° ë‹¨ê³„ê°€ ëë‚œ í›„ ê·¸ë˜í”„ ìƒíƒœë¥¼ ìë™ìœ¼ë¡œ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ ë‚´ì¥ëœ ì§€ì†ì„± ê³„ì¸µì€ ë©”ëª¨ë¦¬ë¥¼ ì œê³µí•˜ì—¬ LangGraphê°€ ë§ˆì§€ë§‰ ìƒíƒœ ì—…ë°ì´íŠ¸ì—ì„œ ì„ íƒí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. \n",
    "\n",
    "ê°€ì¥ ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ ì²´í¬í¬ì¸í„° ì¤‘ í•˜ë‚˜ëŠ” ê·¸ë˜í”„ ìƒíƒœë¥¼ ìœ„í•œ ì¸ë©”ëª¨ë¦¬ í‚¤-ê°’ ì €ì¥ì†Œì¸ `MemorySaver`ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì„¤ì •\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# ReAct Agent ìƒì„±(checkpointer ì„¤ì •)\n",
    "simple_react_agent = create_react_agent(\n",
    "    model,\n",
    "    tools,\n",
    "    checkpointer=memory,\n",
    "    prompt=\"You are a helpful assistant. Answer in Korean.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config ì„¤ì •\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "# ê·¸ë˜í”„ ìŠ¤íŠ¸ë¦¼\n",
    "stream_graph(\n",
    "    simple_react_agent,\n",
    "    {\"messages\": [(\"human\", \"ì•ˆë…•, ë°˜ê°€ì›Œ! ë‚´ ì´ë¦„ì€ í…Œë””ì•¼!\")]},\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë²ˆì—ëŠ” ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì˜ ê¸°ì–µí•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ìŠ¤íŠ¸ë¦¼\n",
    "stream_graph(simple_react_agent, {\"messages\": [(\"human\", \"ë‚´ ì´ë¦„ì´ ë­ë¼ê³ ?\")]}, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì¶”ì **: https://smith.langchain.com/public/16105167-e6db-4e26-add8-96cbf53191f7/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. LangGraph ì›Œí¬í”Œë¡œìš° êµ¬í˜„\n",
    "\n",
    "ì´ì „ì˜ `create_react_agent` ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—ì´ì „íŠ¸ë¥¼ êµ¬í˜„í•´ ë³´ì•˜ìŠµë‹ˆë‹¤.\n",
    "\n",
    "í•˜ì§€ë§Œ, ì´ì „ì˜ ì—ì´ì „íŠ¸ëŠ” ë‹¨ì¼ ì—ì´ì „íŠ¸ í˜•íƒœì´ê¸° ë•Œë¬¸ì— ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬í˜„í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œëŠ” ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps**\n",
    "1. State ì •ì˜(TypedDict í˜•ì‹ìœ¼ë¡œ ì •ì˜)\n",
    "2. ë…¸ë“œ ì •ì˜(í•¨ìˆ˜ë¡œ êµ¬í˜„)\n",
    "3. ê·¸ë˜í”„ ìƒì„±(StateGraph í´ë˜ìŠ¤ ì‚¬ìš©)\n",
    "4. ì»´íŒŒì¼(checkpointer ì„¤ì •)\n",
    "5. ì‹¤í–‰\n",
    "\n",
    "### State ì •ì˜\n",
    "\n",
    "`State`: Graph ì˜ ë…¸ë“œì™€ ë…¸ë“œ ê°„ ê³µìœ í•˜ëŠ” ìƒíƒœë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì¼ë°˜ì ìœ¼ë¡œ `TypedDict` í˜•ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# GraphState ìƒíƒœ ì •ì˜\n",
    "class GraphState(TypedDict):\n",
    "    question: Annotated[str, \"User's Question\"]  # ì§ˆë¬¸\n",
    "    documents: Annotated[str, \"Retrieved Documents\"]  # ë¬¸ì„œì˜ ê²€ìƒ‰ ê²°ê³¼\n",
    "    answer: Annotated[str, \"LLM generated answer\"]  # ë‹µë³€\n",
    "    messages: Annotated[list, add_messages]  # ë©”ì‹œì§€(ëˆ„ì ë˜ëŠ” list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë…¸ë“œ(Node) ì •ì˜\n",
    "\n",
    "- `Nodes`: ê° ë‹¨ê³„ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë…¸ë“œì…ë‹ˆë‹¤. ë³´í†µì€ Python í•¨ìˆ˜ë¡œ êµ¬í˜„í•©ë‹ˆë‹¤. ì…ë ¥ê³¼ ì¶œë ¥ì´ ìƒíƒœ(State) ê°’ì…ë‹ˆë‹¤.\n",
    "  \n",
    "**ì°¸ê³ **  \n",
    "\n",
    "- `State`ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì •ì˜ëœ ë¡œì§ì„ ìˆ˜í–‰í•œ í›„ ì—…ë°ì´íŠ¸ëœ `State`ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.utils import format_docs\n",
    "\n",
    "\n",
    "# ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ\n",
    "def retrieve_document(state: GraphState) -> GraphState:\n",
    "    # ì§ˆë¬¸ì„ ìƒíƒœì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "    latest_question = state[\"question\"]\n",
    "\n",
    "    # ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•˜ì—¬ ê´€ë ¨ì„± ìˆëŠ” ë¬¸ì„œë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n",
    "    retrieved_docs = pdf_retriever.invoke(latest_question)\n",
    "\n",
    "    # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í˜•ì‹í™”í•©ë‹ˆë‹¤.(í”„ë¡¬í”„íŠ¸ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì£¼ê¸° ìœ„í•¨)\n",
    "    retrieved_docs = format_docs(retrieved_docs)\n",
    "\n",
    "    # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ context í‚¤ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    return {\"documents\": retrieved_docs}\n",
    "\n",
    "\n",
    "# ë‹µë³€ ìƒì„± ë…¸ë“œ\n",
    "def llm_answer(state: GraphState) -> GraphState:\n",
    "    # ì§ˆë¬¸ì„ ìƒíƒœì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "    latest_question = state[\"question\"]\n",
    "\n",
    "    # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ìƒíƒœì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # ì²´ì¸ì„ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    response = pdf_chain.invoke({\"question\": latest_question, \"context\": documents})\n",
    "    # ìƒì„±ëœ ë‹µë³€, (ìœ ì €ì˜ ì§ˆë¬¸, ë‹µë³€) ë©”ì‹œì§€ë¥¼ ìƒíƒœì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    return {\n",
    "        \"answer\": response,\n",
    "        \"messages\": [(\"user\", latest_question), (\"assistant\", response)],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê·¸ë˜í”„ ìƒì„±\n",
    "\n",
    "- `StateGraph`: `State` ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ `Node` ë¥¼ ì‹¤í–‰í•˜ê³  `State` ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ê·¸ë˜í”„ ìƒì„± í´ë˜ìŠ¤.\n",
    "- `Edges`: í˜„ì¬ `State`ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒì— ì‹¤í–‰í•  `Node`ë¥¼ ê²°ì •.\n",
    "- `set_entry_point`: ê·¸ë˜í”„ ì§„ì…ì  ì„¤ì •.\n",
    "- `compile`: ê·¸ë˜í”„ ì»´íŒŒì¼."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„±\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# ë…¸ë“œ ì •ì˜\n",
    "workflow.add_node(\"retrieve\", retrieve_document)\n",
    "workflow.add_node(\"llm_answer\", llm_answer)\n",
    "\n",
    "# ì—£ì§€ ì •ì˜\n",
    "workflow.add_edge(\"retrieve\", \"llm_answer\")  # ê²€ìƒ‰ -> ë‹µë³€\n",
    "workflow.add_edge(\"llm_answer\", END)  # ë‹µë³€ -> ì¢…ë£Œ\n",
    "\n",
    "# ê·¸ë˜í”„ ì§„ì…ì (entry_point) ì„¤ì •\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "\n",
    "# ì²´í¬í¬ì¸í„° ì„¤ì •\n",
    "memory = MemorySaver()\n",
    "\n",
    "# ì»´íŒŒì¼\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì»´íŒŒì¼ì´ ì™„ë£Œëœ ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•˜ê³  ì‹œê°í™” í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ë˜í”„ ì‹¤í–‰\n",
    "\n",
    "- `config` íŒŒë¼ë¯¸í„°ëŠ” ê·¸ë˜í”„ ì‹¤í–‰ ì‹œ í•„ìš”í•œ ì„¤ì • ì •ë³´ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "- `recursion_limit`: ê·¸ë˜í”„ ì‹¤í–‰ ì‹œ ì¬ê·€ ìµœëŒ€ íšŸìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "- `inputs`: ê·¸ë˜í”„ ì‹¤í–‰ ì‹œ í•„ìš”í•œ ì…ë ¥ ì •ë³´ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì°¸ê³ **\n",
    "\n",
    "- ë©”ì‹œì§€ ì¶œë ¥ ìŠ¤íŠ¸ë¦¬ë°ì€ [LangGraph ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œì˜ ëª¨ë“  ê²ƒ](https://wikidocs.net/265770) ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ì•„ë˜ì˜ `stream_graph` í•¨ìˆ˜ëŠ” íŠ¹ì • ë…¸ë“œë§Œ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì†ì‰½ê²Œ íŠ¹ì • ë…¸ë“œì˜ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_graph, random_uuid\n",
    "\n",
    "# config ì„¤ì •(ì¬ê·€ ìµœëŒ€ íšŸìˆ˜, thread_id)\n",
    "config = {\"configurable\": {\"resursion_limit\": 10, \"thread_id\": random_uuid()}}\n",
    "\n",
    "# ì§ˆë¬¸ ì…ë ¥\n",
    "inputs = {\"question\": \"ì•¤ìŠ¤ë¡œí”½ì— íˆ¬ìí•œ ê¸°ì—…ê³¼ íˆ¬ìê¸ˆì•¡ì„ ì•Œë ¤ì£¼ì„¸ìš”.\"}\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "stream_graph(app, inputs, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¶”ì : https://smith.langchain.com/public/1aa445e0-672e-4f15-9253-1c8efcdb1355/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4.Routing\n",
    "\n",
    "LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë¼ìš°íŒ…ì€ ì…ë ¥ ì¿¼ë¦¬ë‚˜ ìƒíƒœì— ë”°ë¼ ì ì ˆí•œ ì²˜ë¦¬ ê²½ë¡œë‚˜ êµ¬ì„± ìš”ì†Œë¡œ ìš”ì²­ì„ ì „ë‹¬í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤. \n",
    "\n",
    "LangChain/LangGraphì—ì„œ ë¼ìš°íŒ…ì€ íŠ¹ì • ì‘ì—…ì— ê°€ì¥ ì í•©í•œ ëª¨ë¸ì´ë‚˜ ë„êµ¬ë¥¼ ì„ íƒí•˜ê³ , ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ê´€ë¦¬í•˜ë©°, ë¹„ìš©ê³¼ ì„±ëŠ¥ ê· í˜•ì„ ìµœì í™”í•˜ëŠ” ë° í•„ìˆ˜ì ì…ë‹ˆë‹¤. \n",
    "\n",
    "**Agent**\n",
    "- ë„êµ¬ ì„ íƒì„ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë¼ìš°íŒ…\n",
    "- ë”°ë¼ì„œ, ë„ìš°ì— ëŒ€í•œ description ì´ ìƒì„¸í•˜ê²Œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "**LLM.with_structured_output**\n",
    "- Function Calling ì„ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë¼ìš°íŒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë°ì´í„° ì†ŒìŠ¤ë¡œ ë¼ìš°íŒ…í•˜ëŠ” ë°ì´í„° ëª¨ë¸\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    # ë°ì´í„° ì†ŒìŠ¤ ì„ íƒì„ ìœ„í•œ ë¦¬í„°ëŸ´ íƒ€ì… í•„ë“œ\n",
    "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route it to `web_search` or a `vectorstore`.\",\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM ì„¤ì •\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# llm êµ¬ì¡°í™”ëœ ì¶œë ¥ ì„¤ì •\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ì„¤ì •\n",
    "system = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "The vectorstore contains documents related to AI Brief Report(SPRI) including Samsung Gause, Anthropic, etc.\n",
    "Use the vectorstore for questions on AI related topics. Otherwise, use `web_search`.\"\"\"\n",
    "\n",
    "# Routing ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ê³¼ êµ¬ì¡°í™”ëœ LLM ë¼ìš°í„°ë¥¼ ê²°í•©í•˜ì—¬ ì§ˆë¬¸ ë¼ìš°í„° ìƒì„±\n",
    "question_router = route_prompt | structured_llm_router"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•œ ë’¤ í˜¸ì¶œ ê²°ê³¼ì˜ ì°¨ì´ë¥¼ ë¹„êµí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¿¼ë¦¬ ì‹¤í–‰\n",
    "question_router.invoke(\"ì‚¼ì„±ì „ìê°€ ë§Œë“  ìƒì„±í˜• AI ì´ë¦„ì„ ì°¾ì•„ì¤˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¿¼ë¦¬ ì‹¤í–‰\n",
    "question_router.invoke(\"LangCon2025 ì´ë²¤íŠ¸ì˜ ë‚ ì§œì™€ ì¥ì†ŒëŠ”?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ìŒì€ `retrieve`, `generate`, `web_search` ë…¸ë“œë¥¼ êµ¬í˜„í•œ ì½”ë“œì…ë‹ˆë‹¤.\n",
    "\n",
    "- `retrieve`: ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ\n",
    "- `generate`: ë‹µë³€ ìƒì„± ë…¸ë“œ\n",
    "- `web_search`: ì›¹ ê²€ìƒ‰ ë…¸ë“œ\n",
    "\n",
    "ì´ ë…¸ë“œë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ\n",
    "def retrieve(state):\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # ë¬¸ì„œ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "    documents = pdf_retriever.invoke(question)\n",
    "\n",
    "    # ê²€ìƒ‰ëœ ë¬¸ì„œ ë°˜í™˜\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "\n",
    "# ë‹µë³€ ìƒì„± ë…¸ë“œ\n",
    "def generate(state):\n",
    "    # ì§ˆë¬¸ê³¼ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG ë‹µë³€ ìƒì„±\n",
    "    generation = pdf_chain.invoke({\"context\": documents, \"question\": question})\n",
    "\n",
    "    # ìƒì„±ëœ ë‹µë³€ ë°˜í™˜\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "\n",
    "# ì›¹ ê²€ìƒ‰ ë…¸ë“œ\n",
    "def web_search(state):\n",
    "    # print(\"==== [WEB SEARCH] ====\")\n",
    "    # ì§ˆë¬¸ê³¼ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # ì›¹ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "    web_results = web_search_tool.invoke({\"query\": question})\n",
    "\n",
    "    # ê²€ìƒ‰ëœ ë¬¸ì„œ ë°˜í™˜\n",
    "    web_results_docs = [\n",
    "        Document(\n",
    "            page_content=web_result[\"content\"],\n",
    "            metadata={\"source\": web_result[\"url\"]},\n",
    "        )\n",
    "        for web_result in web_results\n",
    "    ]\n",
    "    return {\"documents\": web_results_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì§ˆë¬¸ ë¼ìš°íŒ… ë…¸ë“œì˜ êµ¬í˜„ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ `question_router` ë¥¼ í˜¸ì¶œí•˜ì—¬ ì ì ˆí•œ ë°ì´í„° ì†ŒìŠ¤ë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `web_search`: ì›¹ ê²€ìƒ‰\n",
    "- `vectorstore`: ë²¡í„° ìŠ¤í† ì–´\n",
    "\n",
    "ë¼ìš°íŒ… ê²°ê³¼ì— ë”°ë¼ ì ì ˆí•œ ë…¸ë“œë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§ˆë¬¸ ë¼ìš°íŒ… ë…¸ë“œ\n",
    "def route_question(state):\n",
    "    print(\"==== [ROUTE QUESTION] ====\")\n",
    "    # ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°\n",
    "    question = state[\"question\"]\n",
    "    # ì§ˆë¬¸ ë¼ìš°íŒ…\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    # ì§ˆë¬¸ ë¼ìš°íŒ… ê²°ê³¼ì— ë”°ë¥¸ ë…¸ë“œ ë¼ìš°íŒ…\n",
    "    if source.datasource == \"web_search\":\n",
    "        print(\"\\n==== [GO TO WEB SEARCH] ====\")\n",
    "        return \"need to search web\"\n",
    "    elif source.datasource == \"vectorstore\":\n",
    "        print(\"\\n==== [GO TO VECTORSTORE] ====\")\n",
    "        return \"search on DB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê·¸ë˜í”„ ìƒì„±\n",
    "\n",
    "ì´ ë‹¨ê³„ì—ì„œëŠ” `web_search`, `retrieve`, `generate` ë…¸ë“œë¥¼ ìƒì„±í•˜ê³ , ì´ë“¤ì„ ì—°ê²°í•˜ëŠ” ì¡°ê±´ë¶€ ì—£ì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `web_search`: ì›¹ ê²€ìƒ‰ ë…¸ë“œ\n",
    "- `retrieve`: ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ\n",
    "- `generate`: ë‹µë³€ ìƒì„± ë…¸ë“œ\n",
    "\n",
    "ì¡°ê±´ë¶€ ì—£ì§€: ì§ˆë¬¸ ë¼ìš°íŒ… ë…¸ë“œì—ì„œ ë°˜í™˜ëœ ê²°ê³¼ì— ë”°ë¼ ì ì ˆí•œ ë…¸ë“œë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `need to search web`: ì›¹ ê²€ìƒ‰ ë…¸ë“œë¡œ ë¼ìš°íŒ…\n",
    "- `search on DB`: ë²¡í„° ìŠ¤í† ì–´ ë…¸ë“œë¡œ ë¼ìš°íŒ…\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒíƒœ ì´ˆê¸°í™”\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# ë…¸ë“œ ì •ì˜\n",
    "workflow.add_node(\"web_search\", web_search)  # ì›¹ ê²€ìƒ‰\n",
    "workflow.add_node(\"retrieve\", retrieve)  # ë¬¸ì„œ ê²€ìƒ‰\n",
    "workflow.add_node(\"generate\", generate)  # ë‹µë³€ ìƒì„±\n",
    "\n",
    "# ê·¸ë˜í”„ ë¹Œë“œ\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    route_question,\n",
    "    {\n",
    "        \"need to search web\": \"web_search\",  # ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ë¼ìš°íŒ…\n",
    "        \"search on DB\": \"retrieve\",  # ë²¡í„°ìŠ¤í† ì–´ë¡œ ë¼ìš°íŒ…\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"retrieve\", \"generate\")  # ë¬¸ì„œ ê²€ìƒ‰ í›„ ë‹µë³€ ìƒì„±\n",
    "workflow.add_edge(\"web_search\", \"generate\")  # ì›¹ ê²€ìƒ‰ í›„ ë‹µë³€ ìƒì„±\n",
    "workflow.add_edge(\"generate\", END)  # ë‹µë³€ ìƒì„± í›„ ì¢…ë£Œ\n",
    "\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "app = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ë˜í”„ë¥¼ ì‹œê°í™” í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"resursion_limit\": 10, \"thread_id\": \"123\"}}\n",
    "\n",
    "stream_graph(\n",
    "    app, {\"question\": \"ì•¤ìŠ¤ë¡œí”½ì— íˆ¬ìí•œ ê¸°ì—…ê³¼ íˆ¬ìê¸ˆì•¡ì„ ì•Œë ¤ì£¼ì„¸ìš”.\"}, config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"resursion_limit\": 10, \"thread_id\": \"123\"}}\n",
    "\n",
    "stream_graph(\n",
    "    app,\n",
    "    {\"question\": \"Claude 3.7 sonnet ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•´ì¤˜. í•œê¸€ë¡œ ë‹µë³€í•´ì¤˜\"},\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5.Fan-out / Fan-in\n",
    "\n",
    "LangGraphì—ì„œ Fan-out/Fan-inì€ ë³µì¡í•œ LLM ì›Œí¬í”Œë¡œìš° ê´€ë¦¬ë¥¼ ìœ„í•œ ì¤‘ìš”í•œ íŒ¨í„´ì…ë‹ˆë‹¤.\n",
    "\n",
    "Fan-outì€ ë‹¨ì¼ ì…ë ¥ì„ ì—¬ëŸ¬ ë³‘ë ¬ ì‘ì—…ìœ¼ë¡œ ë¶„ë°°í•˜ëŠ” íŒ¨í„´ìœ¼ë¡œ, í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ë‚˜ ì¿¼ë¦¬ë¥¼ ì—¬ëŸ¬ LLM, ë„êµ¬, ë˜ëŠ” ì²˜ë¦¬ ë‹¨ê³„ë¡œ ë™ì‹œì— ì „ì†¡í•˜ì—¬ ë‹¤ì–‘í•œ ê´€ì ì´ë‚˜ ì ‘ê·¼ ë°©ì‹ì„ ì–»ì„ ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ì´ëŠ” ë³µì¡í•œ ë¬¸ì œë¥¼ ë” ì‘ê³  ì „ë¬¸í™”ëœ í•˜ìœ„ ì‘ì—…ìœ¼ë¡œ ë¶„í• í•˜ê±°ë‚˜ ë™ì¼í•œ ì‘ì—…ì— ëŒ€í•´ ì—¬ëŸ¬ ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ë¹„êµí•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "Fan-inì€ Fan-outì˜ ì—­ê³¼ì •ìœ¼ë¡œ, ì—¬ëŸ¬ ë³‘ë ¬ ì‘ì—…ì˜ ê²°ê³¼ë¥¼ ë‹¨ì¼ ì¶œë ¥ì´ë‚˜ ë‹¤ìŒ ë‹¨ê³„ë¡œ í†µí•©í•©ë‹ˆë‹¤. ì´ëŠ” ë‹¤ì–‘í•œ ëª¨ë¸ì´ë‚˜ ë„êµ¬ì—ì„œ ìƒì„±ëœ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ë” ì™„ì „í•˜ê³  ì •í™•í•œ ìµœì¢… ì‘ë‹µì„ ë§Œë“¤ê±°ë‚˜ ì—¬ëŸ¬ ì—ì´ì „íŠ¸ì˜ ì‘ì—…ì„ ì¡°ì •í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Any\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# ìƒíƒœ ì •ì˜(add_messages ë¦¬ë“€ì„œ ì‚¬ìš©)\n",
    "class State(TypedDict):\n",
    "    aggregate: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "# ë…¸ë“œ ê°’ ë°˜í™˜ í´ë˜ìŠ¤\n",
    "class ReturnNodeValue:\n",
    "    # ì´ˆê¸°í™”\n",
    "    def __init__(self, node_secret: str):\n",
    "        self._value = node_secret\n",
    "\n",
    "    # í˜¸ì¶œì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸\n",
    "    def __call__(self, state: State) -> Any:\n",
    "        print(f\"Adding {self._value} to {state['aggregate']}\")\n",
    "        return {\"aggregate\": [self._value]}\n",
    "\n",
    "\n",
    "# ìƒíƒœ ê·¸ë˜í”„ ì´ˆê¸°í™”\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# ë…¸ë“œ Aë¶€í„° Dê¹Œì§€ ìƒì„± ë° ê°’ í• ë‹¹\n",
    "builder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n",
    "\n",
    "# ë…¸ë“œ ì—°ê²°\n",
    "builder.add_edge(\"a\", \"b\")\n",
    "builder.add_edge(\"a\", \"c\")\n",
    "builder.add_edge(\"b\", \"d\")\n",
    "builder.add_edge(\"c\", \"d\")\n",
    "builder.add_edge(\"d\", END)\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ë˜í”„ë¥¼ ì‹œê°í™” í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ë˜í”„ë¥¼ ì‹œê°í™” í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "result = graph.invoke({\"aggregate\": []}, {\"configurable\": {\"thread_id\": \"foo\"}})\n",
    "print(\"===\" * 30)\n",
    "print(result[\"aggregate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì¼ë¶€ë§Œ Fan-out í•˜ëŠ” ë°©ë²•\n",
    "\n",
    "(Fan-out ì˜ ìˆœì„œ ì¡°ì •)\n",
    "\n",
    "ì¡°ê±´ë¶€ ì—£ì§€ë¥¼ ë‘ì–´ ì¼ë¶€ë§Œ Fan-out í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "\n",
    "# ìƒíƒœ ì •ì˜(add_messages ë¦¬ë“€ì„œ ì‚¬ìš©)\n",
    "class State(TypedDict):\n",
    "    aggregate: Annotated[list, add_messages]\n",
    "    which: str\n",
    "\n",
    "\n",
    "# ë…¸ë“œë³„ ê³ ìœ  ê°’ì„ ë°˜í™˜í•˜ëŠ” í´ë˜ìŠ¤\n",
    "class ReturnNodeValue:\n",
    "    def __init__(self, node_secret: str):\n",
    "        self._value = node_secret\n",
    "\n",
    "    def __call__(self, state: State) -> Any:\n",
    "        print(f\"Adding {self._value} to {state['aggregate']}\")\n",
    "        return {\"aggregate\": [self._value]}\n",
    "\n",
    "\n",
    "# ìƒíƒœ ê·¸ë˜í”„ ì´ˆê¸°í™”\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n",
    "builder.add_node(\"e\", ReturnNodeValue(\"I'm E\"))\n",
    "\n",
    "\n",
    "# ìƒíƒœì˜ 'which' ê°’ì— ë”°ë¥¸ ì¡°ê±´ë¶€ ë¼ìš°íŒ… ê²½ë¡œ ê²°ì • í•¨ìˆ˜\n",
    "def route_bc_or_cd(state: State) -> Sequence[str]:\n",
    "    if state[\"which\"] == \"cd\":\n",
    "        return [\"c\", \"d\"]\n",
    "    elif state[\"which\"] == \"bc\":\n",
    "        return [\"b\", \"c\"]\n",
    "    else:\n",
    "        return [\"b\", \"c\", \"d\"]\n",
    "\n",
    "\n",
    "# ì „ì²´ ë³‘ë ¬ ì²˜ë¦¬í•  ë…¸ë“œ ëª©ë¡\n",
    "intermediates = [\"b\", \"c\", \"d\"]\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"a\",\n",
    "    route_bc_or_cd,\n",
    "    intermediates,\n",
    ")\n",
    "for node in intermediates:\n",
    "    builder.add_edge(node, \"e\")\n",
    "\n",
    "\n",
    "# ìµœì¢… ë…¸ë“œ ì—°ê²° ë° ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "builder.add_edge(\"e\", END)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ë˜í”„ë¥¼ ì‹œê°í™” í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ì‹¤í–‰(which: bc ë¡œ ì§€ì •)\n",
    "result = graph.invoke({\"aggregate\": [], \"which\": \"bc\"})\n",
    "print(\"===\" * 30)\n",
    "print(result[\"aggregate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ì‹¤í–‰(which: cd ë¡œ ì§€ì •)\n",
    "result = graph.invoke({\"aggregate\": [], \"which\": \"cd\"})\n",
    "print(\"===\" * 30)\n",
    "print(result[\"aggregate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6.ëŒ€í™” ê¸°ë¡ ìš”ì•½ì„ ì¶”ê°€í•˜ëŠ” ë°©ë²•\n",
    "\n",
    "ëŒ€í™” ê¸°ë¡ì„ ìœ ì§€í•˜ëŠ” ê²ƒì€ **ì§€ì†ì„±**ì˜ ê°€ì¥ ì¼ë°˜ì ì¸ ì‚¬ìš© ì‚¬ë¡€ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì´ëŠ” ëŒ€í™”ë¥¼ ì§€ì†í•˜ê¸° ì‰½ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "í•˜ì§€ë§Œ ëŒ€í™”ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ëŒ€í™” ê¸°ë¡ì´ ëˆ„ì ë˜ì–´ `context window`ë¥¼ ë” ë§ì´ ì°¨ì§€í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ëŠ” `LLM` í˜¸ì¶œì´ ë” ë¹„ì‹¸ê³  ê¸¸ì–´ì§€ë©°, ì ì¬ì ìœ¼ë¡œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆì–´ ë°”ëŒì§í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ í•œ ê°€ì§€ ë°©ë²•ì€ í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™” ìš”ì•½ë³¸ì„ ìƒì„±í•˜ê³ , ì´ë¥¼ ìµœê·¼ `N` ê°œì˜ ë©”ì‹œì§€ì™€ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. \n",
    "\n",
    "ì´ ê°€ì´ë“œì—ì„œëŠ” ì´ë¥¼ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì˜ ì˜ˆì‹œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "- ëŒ€í™”ê°€ ë„ˆë¬´ ê¸´ì§€ í™•ì¸ (ë©”ì‹œì§€ ìˆ˜ë‚˜ ë©”ì‹œì§€ ê¸¸ì´ë¡œ í™•ì¸ ê°€ëŠ¥)\n",
    "- ë„ˆë¬´ ê¸¸ë‹¤ë©´ ìš”ì•½ë³¸ ìƒì„± (ì´ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í•„ìš”)\n",
    "- ë§ˆì§€ë§‰ `N` ê°œì˜ ë©”ì‹œì§€ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì‚­ì œ\n",
    "\n",
    "ì´ ê³¼ì •ì—ì„œ ì¤‘ìš”í•œ ë¶€ë¶„ì€ ì˜¤ë˜ëœ ë©”ì‹œì§€ë¥¼ ì‚­ì œ(`DeleteMessage`) í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, RemoveMessage, HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# ëŒ€í™” ë° ìš”ì•½ì„ ìœ„í•œ ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "# ë©”ì‹œì§€ ìƒíƒœì™€ ìš”ì•½ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” ìƒíƒœ í´ë˜ìŠ¤\n",
    "class State(MessagesState):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM ë‹µë³€ ìƒì„± ë…¸ë“œë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ì´ì „ì˜ ëŒ€í™”ìš”ì•½ ë‚´ìš©ì´ ìˆë‹¤ë©´ ì´ë¥¼ ì…ë ¥ì— í¬í•¨í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state: State):\n",
    "    # ì´ì „ ìš”ì•½ ì •ë³´ í™•ì¸\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # ì´ì „ ìš”ì•½ ì •ë³´ê°€ ìˆë‹¤ë©´ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¡œ ì¶”ê°€\n",
    "    if summary:\n",
    "        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ì´ì „ ë©”ì‹œì§€ ê²°í•©\n",
    "        messages = [\n",
    "            SystemMessage(content=f\"Summary of conversation earlier: {summary}\")\n",
    "        ] + state[\"messages\"]\n",
    "    else:\n",
    "        # ì´ì „ ë©”ì‹œì§€ë§Œ ì‚¬ìš©\n",
    "        messages = state[\"messages\"]\n",
    "\n",
    "    # ëª¨ë¸ í˜¸ì¶œ\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    # ì‘ë‹µ ë°˜í™˜\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìš”ì•½ì´ í•„ìš”í•œ ìƒí™©ì¸ì§€ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì—¬ê¸°ì„œëŠ” ë©”ì‹œì§€ ìˆ˜ê°€ 6ê°œ ì´ˆê³¼ë¼ë©´ ìš”ì•½ ë…¸ë“œë¡œ ì´ë™í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "\n",
    "\n",
    "# ëŒ€í™” ì¢…ë£Œ ë˜ëŠ” ìš”ì•½ ê²°ì • ë¡œì§\n",
    "def should_continue(state: State) -> Literal[\"summarize_conversation\", END]:\n",
    "    # ë©”ì‹œì§€ ëª©ë¡ í™•ì¸\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # ë©”ì‹œì§€ ìˆ˜ê°€ 6ê°œ ì´ˆê³¼ë¼ë©´ ìš”ì•½ ë…¸ë“œë¡œ ì´ë™\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìš”ì•½ ë…¸ë“œë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. ì´ì „ ìš”ì•½ ì •ë³´ê°€ ìˆë‹¤ë©´ ì´ë¥¼ ì…ë ¥ì— í¬í•¨í•˜ê³ , ì—†ë‹¤ë©´ ìƒˆë¡œìš´ ìš”ì•½ ë©”ì‹œì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëŒ€í™” ë‚´ìš© ìš”ì•½ ë° ë©”ì‹œì§€ ì •ë¦¬ ë¡œì§\n",
    "def summarize_conversation(state: State):\n",
    "    # ì´ì „ ìš”ì•½ ì •ë³´ í™•ì¸\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # ì´ì „ ìš”ì•½ ì •ë³´ê°€ ìˆë‹¤ë©´ ìš”ì•½ ë©”ì‹œì§€ ìƒì„±\n",
    "    if summary:\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above in Korean.\"\n",
    "        )\n",
    "    else:\n",
    "        # ìš”ì•½ ë©”ì‹œì§€ ìƒì„±\n",
    "        summary_message = \"Create a summary of the conversation above in Korean:\"\n",
    "\n",
    "    # ìš”ì•½ ë©”ì‹œì§€ì™€ ì´ì „ ë©”ì‹œì§€ ê²°í•©\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    # ëª¨ë¸ í˜¸ì¶œ\n",
    "    response = model.invoke(messages)\n",
    "    # ì˜¤ë˜ëœ ë©”ì‹œì§€ ì‚­ì œ\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    # ìš”ì•½ ì •ë³´ ë°˜í™˜\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ë˜í”„ ìƒì„± ë° ì»´íŒŒì¼\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ì´ˆê¸°í™”\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# ëŒ€í™” ë° ìš”ì•½ ë…¸ë“œ ì¶”ê°€\n",
    "workflow.add_node(\"conversation\", generate)\n",
    "workflow.add_node(summarize_conversation)\n",
    "\n",
    "# ì‹œì‘ì ì„ ëŒ€í™” ë…¸ë“œë¡œ ì„¤ì •\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "\n",
    "# ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€\n",
    "workflow.add_conditional_edges(\n",
    "    \"conversation\",\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# ìš”ì•½ ë…¸ë“œì—ì„œ ì¢…ë£Œ ë…¸ë“œë¡œì˜ ì—£ì§€ ì¶”ê°€\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼ ë° ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸í„° ì„¤ì •\n",
    "app = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ë˜í”„ë¥¼ ì‹œê°í™” í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‚¬ìš©ì ë©”ì‹œì§€ ì¶œë ¥ì„ ìœ„í•œ í•¨ìˆ˜ë¥¼ êµ¬í˜„(í—¬í¼ í•¨ìˆ˜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_user_message(message):\n",
    "    print(\"\\n==================================================\\n\\nğŸ˜\", message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ìš°ì„  6ê°œì˜ ëŒ€í™”ë¥¼ ì±„ì›Œë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ì‹œì§€ í•¸ë“¤ë§ì„ ìœ„í•œ HumanMessage í´ë˜ìŠ¤ ì„í¬íŠ¸\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# ìŠ¤ë ˆë“œ IDê°€ í¬í•¨ëœ ì„¤ì • ê°ì²´ ì´ˆê¸°í™”\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"resursion_limit\": 10}}\n",
    "\n",
    "# ì²« ë²ˆì§¸ ë©”ì‹œì§€\n",
    "print_user_message(\"ì•ˆë…•í•˜ì„¸ìš”? ë°˜ê°‘ìŠµë‹ˆë‹¤. ì œ ì´ë¦„ì€ í…Œë””ì…ë‹ˆë‹¤.\")\n",
    "stream_graph(\n",
    "    app,\n",
    "    {\"messages\": [(\"human\", \"ì•ˆë…•í•˜ì„¸ìš”? ë°˜ê°‘ìŠµë‹ˆë‹¤. ì œ ì´ë¦„ì€ í…Œë””ì…ë‹ˆë‹¤.\")]},\n",
    "    config,\n",
    ")\n",
    "\n",
    "# ë‘ ë²ˆì§¸ ë©”ì‹œì§€\n",
    "print_user_message(\"ì œ ì´ë¦„ì´ ë­”ì§€ ê¸°ì–µí•˜ì„¸ìš”?\")\n",
    "stream_graph(app, {\"messages\": [(\"human\", \"ì œ ì´ë¦„ì´ ë­”ì§€ ê¸°ì–µí•˜ì„¸ìš”?\")]}, config)\n",
    "\n",
    "# ì„¸ ë²ˆì§¸ ë©”ì‹œì§€\n",
    "print_user_message(\"ì œ ì·¨ë¯¸ëŠ” Netflix ì‹œë¦¬ì¦ˆë¥¼ ë³´ëŠ” ê²ƒì…ë‹ˆë‹¤.\")\n",
    "stream_graph(\n",
    "    app, {\"messages\": [(\"human\", \"ì œ ì·¨ë¯¸ëŠ” Netflix ì‹œë¦¬ì¦ˆë¥¼ ë³´ëŠ” ê²ƒì…ë‹ˆë‹¤.\")]}, config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. 6ê°œ ëŒ€í™”ë¥¼ í–ˆìœ¼ë¯€ë¡œ, ìš”ì•½ë³¸ì´ ë§Œë“¤ì–´ ì ¸ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒíƒœ êµ¬ì„± ê°’ ê²€ìƒ‰\n",
    "values = app.get_state(config).values\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì œ ì¶”ê°€ë¡œ ëŒ€í™”ë¥¼ ì…ë ¥í•˜ì—¬ ìš”ì•½ë³¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì˜ ë‹µë³€í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë„¤ ë²ˆì§¸ ë©”ì‹œì§€\n",
    "print_user_message(\"ì œ ì·¨ë¯¸ê°€ ë­ë¼ê³  í–ˆë‚˜ìš”?\")\n",
    "stream_graph(app, {\"messages\": [(\"human\", \"ì œ ì·¨ë¯¸ê°€ ë­ë¼ê³  í–ˆë‚˜ìš”?\")]}, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¶”ì : https://smith.langchain.com/public/c0f62f0b-74b5-4dc3-bd1c-3e474a0dbef9/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7. Human in the Loop\n",
    "\n",
    "LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ Human-in-the-loop ì€ ìë™í™”ëœ AI ì‹œìŠ¤í…œê³¼ ì¸ê°„ì˜ ê°œì… ë° íŒë‹¨ì„ ê²°í•©í•˜ëŠ” ì ‘ê·¼ ë°©ì‹ì…ë‹ˆë‹¤. \n",
    "\n",
    "ì´ ë°©ì‹ì—ì„œëŠ” AI ì‹œìŠ¤í…œì´ ì´ˆê¸° ì²˜ë¦¬ì™€ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì§€ë§Œ, ë¶ˆí™•ì‹¤í•˜ê±°ë‚˜ ì¤‘ìš”í•œ ê²°ì •ì´ í•„ìš”í•œ ì‹œì ì—ì„œ ì¸ê°„ ì „ë¬¸ê°€ì˜ ê°œì…ì„ ìš”ì²­í•©ë‹ˆë‹¤. \n",
    "\n",
    "Human-in-the-loop ì€ ë†’ì€ ì •í™•ë„ê°€ í•„ìš”í•œ ë³µì¡í•œ ìƒí™©, ìœ¤ë¦¬ì  íŒë‹¨ì´ í•„ìš”í•œ ê²½ìš°, ë˜ëŠ” AIì˜ ì‹ ë¢°ë„ê°€ ë‚®ì€ ê²°ê³¼ì— ëŒ€í•´ ê²€ì¦ì´ í•„ìš”í•  ë•Œ íŠ¹íˆ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "`from langgraph.types import interrupt`\n",
    "\n",
    "`interrupt` í•¨ìˆ˜ëŠ” ì¸ê°„ì˜ ê°œì…ì„ ìš”ì²­í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” ì¸ê°„ì˜ ì…ë ¥ì„ ë°›ì•„ ì²˜ë¦¬í•˜ê³ , ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "import uuid\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    evaluation: Annotated[str, \"Evaluation\"]\n",
    "\n",
    "\n",
    "def llm_node(state: State):\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def human_node(state: State):\n",
    "    value = interrupt(\n",
    "        # ì‚¬ëŒì˜ í”¼ë“œë°± ìš”ì²­\n",
    "        {\"text_to_revise\": state[\"messages\"][-1]}\n",
    "    )\n",
    "    return {\n",
    "        # ì‚¬ëŒì˜ í”¼ë“œë°±ìœ¼ë¡œ ì—…ë°ì´íŠ¸\n",
    "        \"messages\": [HumanMessage(content=value)]\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluation_node(state: State):\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "    prompt = f\"\"\"\n",
    "Here is the user's question and the model's response\n",
    "The user's question: {state[\"messages\"][0]}\n",
    "Model's response: {state[\"messages\"][-1]}\n",
    "\n",
    "Please rate whether the model's response accurately answered your question.\"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"evaluation\": response}\n",
    "\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„±\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# ë…¸ë“œ ì¶”ê°€\n",
    "workflow.add_node(\"llm\", llm_node)\n",
    "workflow.add_node(\"human\", human_node)\n",
    "workflow.add_node(\"eval\", evaluation_node)\n",
    "\n",
    "# ì—£ì§€ ì¶”ê°€\n",
    "workflow.add_edge(START, \"llm\")\n",
    "workflow.add_edge(\"llm\", \"human\")\n",
    "workflow.add_edge(\"human\", \"eval\")\n",
    "workflow.add_edge(\"eval\", END)\n",
    "\n",
    "# ì²´í¬í¬ì¸í„° ì„¤ì •\n",
    "checkpointer = MemorySaver()\n",
    "app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ˆê¸° ì§ˆë¬¸ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config ì„¤ì •\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "\n",
    "stream_graph(app, {\"messages\": [(\"human\", \"2+6=?\")]}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.get_state(config).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Command` ê°ì²´ëŠ” ì¸ê°„ì˜ ê°œì…ì„ ìš”ì²­í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ê°ì²´ëŠ” ì¸ê°„ì˜ ì…ë ¥ì„ ë°›ì•„ ì²˜ë¦¬í•˜ê³ , ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "- `resume`: í”¼ë“œë°±ì„ ì…ë ¥í•˜ê³  ë‚¨ì€ ë‹¨ê³„ë¥¼ ì¬ê²Œ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_graph(app, Command(resume=\"2 + 6 = 9\"), config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (ì¸í„°ëŸ½íŠ¸) ì¶”ì : https://smith.langchain.com/public/c251955b-6999-4983-9a92-6905700333d3/r\n",
    "- (ì´í›„) ì¶”ì : https://smith.langchain.com/public/c251955b-6999-4983-9a92-6905700333d3/r    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "import uuid\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.graph import END\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    evaluation: Annotated[str, \"Evaluation\"]\n",
    "\n",
    "\n",
    "def llm_node(state: State):\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def human_approval(state: State) -> Command[Literal[\"llm\", END]]:\n",
    "    is_approved = interrupt(\n",
    "        {\n",
    "            \"question\": \"Is this correct?\",\n",
    "            # ë‹µë³€ì„ ì‚¬ëŒì—ê²Œ ë³´ì—¬ì£¼ê³  ìˆ˜ì • ìš”ì²­\n",
    "            \"need_to_revise\": state[\"messages\"][-1],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if is_approved:\n",
    "        return Command(\n",
    "            goto=END, update={\"messages\": [HumanMessage(content=\"You are great!\")]}\n",
    "        )\n",
    "    else:\n",
    "        return Command(\n",
    "            goto=\"llm\",\n",
    "            update={\n",
    "                \"messages\": [HumanMessage(content=\"You are wrong.. Please try again\")]\n",
    "            },\n",
    "        )\n",
    "\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„±\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# ë…¸ë“œ ì¶”ê°€\n",
    "workflow.add_node(\"llm\", llm_node)\n",
    "workflow.add_node(\"human\", human_approval)\n",
    "\n",
    "# ì—£ì§€ ì¶”ê°€\n",
    "workflow.add_edge(START, \"llm\")\n",
    "workflow.add_edge(\"llm\", \"human\")\n",
    "\n",
    "# ì²´í¬í¬ì¸í„° ì„¤ì •\n",
    "checkpointer = MemorySaver()\n",
    "app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config ì„¤ì •\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "\n",
    "stream_graph(app, {\"messages\": [(\"human\", \"1, 2, 4, 10, ?\")]}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.get_state(config).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_graph(app, Command(resume=False), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.get_state(config).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8. Long-Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ë©”ëª¨ë¦¬**ëŠ” ì‚¬ëŒë“¤ì´ í˜„ì¬ì™€ ë¯¸ë˜ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ì •ë³´ë¥¼ ì €ì¥í•˜ê³ , ê²€ìƒ‰í•˜ë©° ì‚¬ìš©í•˜ëŠ” ì¸ì§€ ê¸°ëŠ¥ì…ë‹ˆë‹¤.\n",
    "\n",
    "AI ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” [ë‹¤ì–‘í•œ **ì¥ê¸° ë©”ëª¨ë¦¬ ìœ í˜•**](https://langchain-ai.github.io/langgraph/concepts/memory/#memory)ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì—¬ê¸°ì—ì„œëŠ” **ì¥ê¸° ê¸°ì–µ**ì„ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ [LangGraph Memory Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore)ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤.\n",
    "\n",
    "`short-term (within-thread)` ë° `long-term (across-thread)` ë©”ëª¨ë¦¬ë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ì±—ë´‡ì„ êµ¬ì¶•í•  ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "ì‚¬ìš©ìì— ëŒ€í•œ ì‚¬ì‹¤ì¸ ì¥ê¸° [**semantic memory**](https://langchain-ai.github.io/langgraph/concepts/memory/#semantic-memory)ì— ì¤‘ì ì„ ë‘˜ ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangGraph Store**\n",
    "\n",
    "[LangGraph Memory Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) ëŠ” LangGraphì—ì„œ *thread ê°„* ì •ë³´ë¥¼ ì €ì¥í•˜ê³  ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤. \n",
    "\n",
    "ì´ ì €ì¥ì†ŒëŠ” ì§€ì†ì ì¸ `key-value` ë°ì´í„°ë¥¼ ê´€ë¦¬í•˜ê¸° ìœ„í•œ [ì˜¤í”ˆ ì†ŒìŠ¤ ê¸°ë³¸ í´ë˜ìŠ¤](https://blog.langchain.dev/launching-long-term-memory-support-in-langgraph/)ë¡œ, ê°œë°œìê°€ ìì‹ ë§Œì˜ ì €ì¥ì†Œë¥¼ ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# LangGraphì˜ InMemoryStore ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "persistant_memory = InMemoryStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìŠ¤í† ì–´ì— ê°ì²´(ì˜ˆ: ë©”ëª¨ë¦¬)ë¥¼ ì €ì¥í•  ë•ŒëŠ” [Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore)ì— ë‹¤ìŒ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "- **namespace** (`namespace`): ê°ì²´ë¥¼ êµ¬ë¶„í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” íŠœí”Œ í˜•íƒœì˜ ì‹ë³„ìì…ë‹ˆë‹¤ (ë””ë ‰í† ë¦¬ì™€ ìœ ì‚¬).\n",
    "- **key** (`key`): ê°ì²´ì˜ ê³ ìœ  ì‹ë³„ìì…ë‹ˆë‹¤ (íŒŒì¼ ì´ë¦„ê³¼ ìœ ì‚¬).\n",
    "- **value** (`value`): ê°ì²´ì˜ ì‹¤ì œ ë‚´ìš©ì…ë‹ˆë‹¤ (íŒŒì¼ ë‚´ìš©ê³¼ ìœ ì‚¬).\n",
    "\n",
    "`namespace`ì™€ `key`ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì²´ë¥¼ ìŠ¤í† ì–´ì— ì €ì¥í•˜ê¸° ìœ„í•´ [put](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.put) ë©”ì„œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì €ì¥í•  ë©”ëª¨ë¦¬ì˜ ì‚¬ìš©ì ID ì„¤ì •\n",
    "user_id = \"teddy\"\n",
    "\n",
    "# ì‚¬ìš©ì IDì™€ ë©”ëª¨ë¦¬ êµ¬ë¶„ì„ ìœ„í•œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜\n",
    "namespace_for_memory = (\"memories\", user_id)\n",
    "\n",
    "# ê³ ìœ  í‚¤ ìƒì„±ì„ ìœ„í•œ UUID ìƒì„±\n",
    "key = \"user_memory\"\n",
    "\n",
    "# ì €ì¥í•  ë©”ëª¨ë¦¬ ê°’ìœ¼ë¡œ ë”•ì…”ë„ˆë¦¬ ì •ì˜\n",
    "value = {\n",
    "    \"job\": \"AI Engineer\",\n",
    "    \"location\": \"Seoul, Korea\",\n",
    "    \"hobbies\": [\"Watching Netflix\", \"Coding\"],\n",
    "}\n",
    "\n",
    "# ì§€ì •ëœ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ë©”ëª¨ë¦¬ ì €ì¥\n",
    "persistant_memory.put(namespace_for_memory, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`search`](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.search)ì„ ì´ìš©í•˜ì—¬ `namespace` ê¸°ì¤€ìœ¼ë¡œ `store`ì—ì„œ ê°ì²´ë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë©”ì„œë“œëŠ” ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§€ì •ëœ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¡œë¶€í„° ë©”ëª¨ë¦¬ ê°ì²´ ê²€ìƒ‰\n",
    "memories = persistant_memory.search(namespace_for_memory)\n",
    "\n",
    "# ê²€ìƒ‰ëœ ë©”ëª¨ë¦¬ ê°ì²´ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "memories[0].dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì¥ê¸° ë©”ëª¨ë¦¬ë¥¼ ê°–ì¶˜ ì±—ë´‡**\n",
    "\n",
    "ì±—ë´‡ì€ [ë‘ ê°€ì§€ ìœ í˜•ì˜ ë©”ëª¨ë¦¬](https://docs.google.com/presentation/d/181mvjlgsnxudQI6S3ritg9sooNyu4AcLLFH1UK0kIuk/edit#slide=id.g30eb3c8cf10_0_156)ë¥¼ ê°–ì¶”ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "1. `Short-term (within-thread) memory`: ì±—ë´‡ì´ ëŒ€í™” ë‚´ì—­ì„ ì§€ì†ì ìœ¼ë¡œ ì €ì¥í•˜ê±°ë‚˜, ëŒ€í™” ì„¸ì…˜ ì¤‘ì— ì¤‘ë‹¨ì„ í—ˆìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "2. `Long-term (cross-thread) memory`: ì±—ë´‡ì´ íŠ¹ì • ì‚¬ìš©ìì— ëŒ€í•œ ì •ë³´ë¥¼ *ëª¨ë“  ëŒ€í™” ì„¸ì…˜ì— ê±¸ì³* ê¸°ì–µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ ë‘ ê°€ì§€ ë©”ëª¨ë¦¬ ìœ í˜•ì„ í†µí•´ ì±—ë´‡ì€ ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ë¥¼ ë”ìš± ì›í™œí•˜ê³  ê°œì¸í™”ëœ ë°©ì‹ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `Short-term memory`ëŠ” í˜„ì¬ ëŒ€í™” ì„¸ì…˜ ë‚´ì—ì„œì˜ ë§¥ë½ì„ ìœ ì§€í•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, `Long-term memory`ëŠ” ì‚¬ìš©ìì— ëŒ€í•œ ì§€ì†ì ì¸ ì •ë³´ë¥¼ ì €ì¥í•˜ì—¬ ì—¬ëŸ¬ ì„¸ì…˜ì— ê±¸ì³ ì¼ê´€ëœ ìƒí˜¸ì‘ìš©ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "from langchain_teddynote.graphs import visualize_graph\n",
    "from langchain_teddynote.messages import stream_graph\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# ëª¨ë¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì •ì˜\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful assistant with memory that provides information about the user. \n",
    "If you have memory for this user, use it to personalize your responses.\n",
    "Here is the memory (it may be empty): {memory}\"\"\"\n",
    "\n",
    "# ìƒˆë¡œìš´ ë©”ëª¨ë¦¬ ìƒì„± ì§€ì¹¨ ì •ì˜\n",
    "CREATE_MEMORY_INSTRUCTION = \"\"\"\"You are collecting information about the user to personalize your responses.\n",
    "\n",
    "CURRENT USER INFORMATION:\n",
    "{memory}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Review the chat history below carefully\n",
    "2. Identify new information about the user, such as:\n",
    "   - Personal details (name, job, location)\n",
    "   - Preferences (likes, dislikes)\n",
    "   - Interests and hobbies\n",
    "   - Past experiences\n",
    "   - Goals or future plans\n",
    "3. Merge any new information with existing memory\n",
    "4. Format the memory as a clear, bulleted list\n",
    "5. If new information conflicts with existing memory, keep the most recent version\n",
    "\n",
    "Remember: Only include factual information directly stated by the user. Do not make assumptions or inferences.\n",
    "\n",
    "Based on the chat history below, please update the user information:\"\"\"\n",
    "\n",
    "\n",
    "# call_model í•¨ìˆ˜ ì •ì˜\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"Load memory from the store and use it to personalize the chatbot's response.\"\"\"\n",
    "\n",
    "    # ì„¤ì •ì—ì„œ ì‚¬ìš©ì ID ê°€ì ¸ì˜¤ê¸°\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # ìŠ¤í† ì–´ì—ì„œ ë©”ëª¨ë¦¬ ê²€ìƒ‰\n",
    "    namespace = (\"memories\", user_id)\n",
    "    key = \"user_memory\"\n",
    "    existing_memory = store.get(namespace, key)\n",
    "\n",
    "    # ê¸°ì¡´ ë©”ëª¨ë¦¬ ë‚´ìš© ì¶”ì¶œ ë° í”„ë¦¬í”½ìŠ¤ ì¶”ê°€\n",
    "    if existing_memory:\n",
    "        # ê°’ì€ ë©”ëª¨ë¦¬ í‚¤ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬\n",
    "        existing_memory_content = existing_memory.value.get(\"memories\")\n",
    "    else:\n",
    "        existing_memory_content = \"No existing memory found.\"\n",
    "\n",
    "    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ë©”ëª¨ë¦¬ í¬ë§·\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=existing_memory_content)\n",
    "\n",
    "    # ë©”ëª¨ë¦¬ì™€ ëŒ€í™” ê¸°ë¡ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±\n",
    "    response = model.invoke([SystemMessage(content=system_msg)] + state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# write_memory í•¨ìˆ˜ ì •ì˜\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"Reflect on the chat history and save a memory to the store.\"\"\"\n",
    "\n",
    "    # ì„¤ì •ì—ì„œ ì‚¬ìš©ì ID ê°€ì ¸ì˜¤ê¸°\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # ìŠ¤í† ì–´ì—ì„œ ê¸°ì¡´ ë©”ëª¨ë¦¬ ê²€ìƒ‰\n",
    "    namespace = (\"memories\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "\n",
    "    # ë©”ëª¨ë¦¬ ì¶”ì¶œ\n",
    "    if existing_memory:\n",
    "        existing_memory_content = existing_memory.value.get(\"memories\")\n",
    "    else:\n",
    "        existing_memory_content = \"No existing memory found.\"\n",
    "\n",
    "    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ë©”ëª¨ë¦¬ í¬ë§·\n",
    "    system_msg = CREATE_MEMORY_INSTRUCTION.format(memory=existing_memory_content)\n",
    "    new_memory = model.invoke([SystemMessage(content=system_msg)] + state[\"messages\"])\n",
    "\n",
    "    # ìŠ¤í† ì–´ì— ê¸°ì¡´ ë©”ëª¨ë¦¬ ë®ì–´ì“°ê¸°\n",
    "    key = \"user_memory\"\n",
    "\n",
    "    # ë©”ëª¨ë¦¬ í‚¤ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¡œ ê°’ ì‘ì„±\n",
    "    store.put(namespace, key, {\"memories\": new_memory.content})\n",
    "\n",
    "\n",
    "# ê·¸ë˜í”„ ì •ì˜\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"call_model\", call_model)\n",
    "workflow.add_node(\"write_memory\", write_memory)\n",
    "workflow.add_edge(START, \"call_model\")\n",
    "workflow.add_edge(\"call_model\", \"write_memory\")\n",
    "workflow.add_edge(\"write_memory\", END)\n",
    "\n",
    "# ì¥ê¸° ë©”ëª¨ë¦¬ ì €ì¥ì†Œ ì„¤ì •\n",
    "across_thread_memory = InMemoryStore()\n",
    "\n",
    "# ë‹¨ê¸° ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸í„° ì„¤ì •\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# ì²´í¬í¬ì¸í„° ë° ìŠ¤í† ì–´ë¥¼ í¬í•¨í•˜ì—¬ ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "app = workflow.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹œê°í™”\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¨ê¸° ë©”ëª¨ë¦¬ë¥¼ ìœ„í•œ ì“°ë ˆë“œ ID ì œê³µ(thread_id)\n",
    "# ì¥ê¸° ë©”ëª¨ë¦¬ë¥¼ ìœ„í•œ ì‚¬ìš©ì ID ì œê³µ(user_id)\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"teddy\"}}\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥\n",
    "input_messages = [\n",
    "    HumanMessage(\n",
    "        content=\"ì•ˆë…• ë°˜ê°€ì›Œ! ë‚´ ì´ë¦„ì€ í…Œë”” ì…ë‹ˆë‹¤. ì €ì˜ ì·¨ë¯¸ëŠ” ì½”ë”© í•˜ê³  ì˜í™” ë³´ëŠ” ê²ƒì…ë‹ˆë‹¤.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "stream_graph(app, {\"messages\": input_messages}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¨ê¸° ë©”ëª¨ë¦¬ë¥¼ ìœ„í•œ ì“°ë ˆë“œ ID ì œê³µ(thread_id)\n",
    "# ì¥ê¸° ë©”ëª¨ë¦¬ë¥¼ ìœ„í•œ ì‚¬ìš©ì ID ì œê³µ(user_id)\n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"teddy2\"}}\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥\n",
    "input_messages = [HumanMessage(content=\"ë‚´ ì·¨ë¯¸ê°€ ë­ì˜€ë”ë¼...\")]\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "stream_graph(app, {\"messages\": input_messages}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¨ê¸° ë©”ëª¨ë¦¬ë¥¼ ìœ„í•œ ì“°ë ˆë“œ ID ì œê³µ(thread_id)\n",
    "# ì¥ê¸° ë©”ëª¨ë¦¬ë¥¼ ìœ„í•œ ì‚¬ìš©ì ID ì œê³µ(user_id)\n",
    "config = {\"configurable\": {\"thread_id\": \"3\", \"user_id\": \"teddy\"}}\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥\n",
    "input_messages = [HumanMessage(content=\"ë‚´ ì·¨ë¯¸ê°€ ë­ì˜€ë”ë¼...\")]\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "stream_graph(app, {\"messages\": input_messages}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistant_memory.get((\"memories\", \"teddy\"), \"user_memory\").value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¨ê¸° ë©”ëª¨ë¦¬ë¥¼ ìœ„í•œ ì“°ë ˆë“œ ID ì œê³µ(thread_id)\n",
    "# ì¥ê¸° ë©”ëª¨ë¦¬ë¥¼ ìœ„í•œ ì‚¬ìš©ì ID ì œê³µ(user_id)\n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"john\"}}\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥\n",
    "input_messages = [HumanMessage(content=\"ì•ˆë…• ë°˜ê°€ì›Œ! í˜¹ì‹œ ë‚´ ì·¨ë¯¸ ê¸°ì–µí•´?\")]\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "stream_graph(app, {\"messages\": input_messages}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¨ê¸° ë©”ëª¨ë¦¬ë¥¼ ìœ„í•œ ì“°ë ˆë“œ ID ì œê³µ(thread_id)\n",
    "# ì¥ê¸° ë©”ëª¨ë¦¬ë¥¼ ìœ„í•œ ì‚¬ìš©ì ID ì œê³µ(user_id)\n",
    "config = {\"configurable\": {\"thread_id\": \"3\", \"user_id\": \"teddy\"}}\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥\n",
    "input_messages = [HumanMessage(content=\"ë‚˜ì— ëŒ€í•´ ì•„ëŠ” ì •ë³´ ëª¨ë‘ ë§í•´ì¤˜\")]\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹¤í–‰\n",
    "stream_graph(app, {\"messages\": input_messages}, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì¶”ì : https://smith.langchain.com/public/618341ed-4bc2-443f-8ad0-f96fc464fac8/r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
